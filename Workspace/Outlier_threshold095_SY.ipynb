{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970ee34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-9fz8wyt_\n",
      "  Running command git clone --filter=blob:none -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-9fz8wyt_\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 5c46b1c68e4755b54879431bd302db621f4d2f47\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3<=1.15.18 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.15.18)\n",
      "Requirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: mxnet<=1.7.0.post2,>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.7.0.post2)\n",
      "Requirement already satisfied: onnxruntime<=1.8.0,==1.8.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.8.0)\n",
      "Requirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.9.1+cu111)\n",
      "Requirement already satisfied: transformers<=4.8.1,>=4.8.1 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (4.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.4)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.1)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /opt/conda/lib/python3.9/site-packages (from boto3<=1.15.18->kobert==0.2.3) (1.18.18)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3<=1.15.18->kobert==0.2.3) (0.10.0)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.24)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.9/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.26.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.0.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.12)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2021.11.10)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /opt/conda/lib/python3.9/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (1.25.11)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070349da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69afe564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_train = os.getenv('HOME')+'/aiffel/dktc/data/train.csv'\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0c8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c919c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idx             0\n",
       "class           0\n",
       "conversation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f37d731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          협박 대화\n",
       "2      기타 괴롭힘 대화\n",
       "3          갈취 대화\n",
       "5    직장 내 괴롭힘 대화\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a287862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 클렌징 함수\n",
    "def clean_text(text):\n",
    "    # 불필요한 특수 문자, 숫자 제거 (한글, 영문, 공백 제외)\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "train['conversation'] = train['conversation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74a39c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty conversations: 0\n"
     ]
    }
   ],
   "source": [
    "# 클렌징 후 비어 있는 대화가 있는지 확인\n",
    "empty_conversations = train[train['conversation'].str.strip() == '']\n",
    "print(f\"Empty conversations: {len(empty_conversations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "725e7f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f0e7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/.cache/kobert_v1.zip\n"
     ]
    }
   ],
   "source": [
    "#bert 모델, vocab 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64346ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['class'] == \"협박 대화\"), 'label'] = 0  # 협박 대화 => 0\n",
    "train.loc[(train['class'] == \"갈취 대화\"), 'label'] = 1  # 갈취 대화 => 1\n",
    "train.loc[(train['class'] == \"직장 내 괴롭힘 대화\"), 'label'] = 2  # 직장 내 괴롭힘 대화 => 2\n",
    "train.loc[(train['class'] == \"기타 괴롭힘 대화\"), 'label'] = 3  # 기타 괴롭힘 대화 => 3\n",
    "train.loc[(train['class'] == \"일반 대화\"), 'label'] = 4  # 일반 대화 => 4\n",
    "\n",
    "data_list = []\n",
    "for content, label in zip(train['conversation'], train['label'])  :\n",
    "    temp = []\n",
    "    temp.append(content)\n",
    "    temp.append(str(int(label)))\n",
    "\n",
    "    data_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed72ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns = 'idx', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2deba3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e7880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        \n",
    "        # 레이블이 없는 경우를 처리\n",
    "        if label_idx is not None:\n",
    "            self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.labels is not None:\n",
    "            return self.sentences[i] + (self.labels[i], )\n",
    "        else:\n",
    "            return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8636872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 130 # 이것도 변경되는 것이 좋을 것 같다\n",
    "batch_size = 32 # 고정! 이걸 넘기면 모델이 안돌아감\n",
    "warmup_ratio = 0.2 #이걸 수정해보자\n",
    "num_epochs = 5 \n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "#BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bb1a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 및 데이터로더 설정 (num_workers 최적화)\n",
    "# num_workers는 시스템에 맞춰 적절한 값을 설정 (예: CPU 코어 수에 맞춰 4나 8 정도로 설정)\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        # attention mask는 token_ids에서 패딩(0)이 아닌 부분을 1로 설정\n",
    "        attention_mask = (token_ids != 0).float()\n",
    "        \n",
    "        # BERT 모델에서 pooler 출력 사용\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        # 드롭아웃 적용\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53832df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().item() / max_indices.size(0)\n",
    "    return train_acc\n",
    "\n",
    "def calc_metrics(preds, Y):\n",
    "    # preds는 detach하여 그래디언트를 추적하지 않도록 하고, CPU로 옮긴 후 numpy 배열로 변환\n",
    "    pred_labels = preds.detach().cpu().numpy()\n",
    "    true_labels = Y.cpu().numpy()\n",
    "    \n",
    "    # 정확도 계산: pred_labels == true_labels 결과를 명시적으로 int로 변환 후 합산\n",
    "    accuracy = (pred_labels == true_labels).astype(int).sum() / len(true_labels)\n",
    "    \n",
    "    # F1, Precision, Recall 계산, zero_division 옵션 추가로 경고 억제\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9419de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# 하이퍼파라미터 공간 정의\n",
    "space = [\n",
    "    Integer(60, 220, name='max_len'),  # max_len: 60~220, 10씩 증가\n",
    "    Real(0.0, 0.4, name='warmup_ratio'),  # warmup_ratio: 0~0.4\n",
    "    Integer(3, 10, name='num_epochs'),  # num_epochs: 3~10\n",
    "    Real(1e-5, 1e-4, prior='log-uniform', name='learning_rate'),  # learning_rate: 1e-5~1e-4\n",
    "    Real(0.1, 0.5, name='dr_rate')  # dr_rate: 0.1~0.5\n",
    "]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(max_len, warmup_ratio, num_epochs, learning_rate, dr_rate):\n",
    "    # 모델 설정\n",
    "    model = BERTClassifier(bertmodel, dr_rate=dr_rate).to(device)\n",
    "\n",
    "    # 옵티마이저와 스케줄러 설정\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "    t_total = len(train_dataloader) * num_epochs\n",
    "    warmup_step = int(t_total * warmup_ratio)\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "    # 학습 및 성능 평가\n",
    "    for e in range(num_epochs):\n",
    "        train_acc = 0.0\n",
    "        train_f1, train_precision, train_recall = 0.0, 0.0, 0.0\n",
    "        test_acc, val_f1, val_precision, val_recall = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        model.train()\n",
    "        for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            label = label.long().to(device)\n",
    "            \n",
    "            out = model(token_ids, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "            train_acc += batch_acc\n",
    "            train_f1 += batch_f1\n",
    "            train_precision += batch_precision\n",
    "            train_recall += batch_recall\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "                token_ids = token_ids.long().to(device)\n",
    "                segment_ids = segment_ids.long().to(device)\n",
    "                label = label.long().to(device)\n",
    "\n",
    "                out = model(token_ids, segment_ids)\n",
    "                batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "                test_acc += batch_acc\n",
    "                val_f1 += batch_f1\n",
    "                val_precision += batch_precision\n",
    "                val_recall += batch_recall\n",
    "\n",
    "    # 최종 검증 F1-score 반환\n",
    "    return -val_f1 / len(test_dataloader)  # 최적화 방향을 위해 음수로 반환\n",
    "\n",
    "# 베이지안 최적화 실행\n",
    "result = gp_minimize(objective, space, n_calls=50, random_state=42)\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best hyperparameters: \", result.x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a259efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확률 기반 이상치 탐지 함수\n",
    "def classify_with_outlier_detection(logits, threshold=0.95):\n",
    "    probs = F.softmax(logits, dim=1)  # 클래스별 확률 계산\n",
    "    max_probs, preds = torch.max(probs, dim=1)  # 가장 높은 확률과 그 클래스\n",
    "    \n",
    "    # 확률이 threshold 이하인 경우 '일반 대화'로 분류 (label: 4)\n",
    "    preds[max_probs < threshold] = 4\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea71ea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Accuracy: 0.0, F1: 0.0, Precision: 0.0, Recall: 0.0\n",
      "Epoch 3: Train Accuracy: 0.06902356902356901, F1: 0.09681959514620006, Precision: 0.20309869528619529, Recall: 0.06902356902356901\n",
      "Epoch 4: Train Accuracy: 0.31502525252525254, F1: 0.3884927243852999, Precision: 0.6665374547240077, Recall: 0.31502525252525254\n",
      "Epoch 5: Train Accuracy: 0.5913299663299663, F1: 0.6970007831120921, Precision: 0.9683058488929702, Recall: 0.5913299663299663\n",
      "Epoch 6: Train Accuracy: 0.8499579124579124, F1: 0.9012767662346678, Precision: 0.9867901485088986, Recall: 0.8499579124579124\n",
      "Epoch 7: Train Accuracy: 0.9452861952861953, F1: 0.965858582621741, Precision: 0.9922639317052199, Recall: 0.9452861952861953\n",
      "Epoch 8: Train Accuracy: 0.9747474747474747, F1: 0.9840011113600675, Precision: 0.9957799644589416, Recall: 0.9747474747474747\n",
      "Epoch 9: Train Accuracy: 0.9863215488215488, F1: 0.9909802409634184, Precision: 0.9969053244384684, Recall: 0.9863215488215488\n",
      "Epoch 10: Train Accuracy: 0.992739898989899, F1: 0.9955239988898477, Precision: 0.9988885632114798, Recall: 0.992739898989899\n",
      "Test Accuracy: 0.8059090909090909, F1: 0.8435636937193278, Precision: 0.8987429653679652, Recall: 0.8059090909090909\n"
     ]
    }
   ],
   "source": [
    "# 수동으로 최적의 하이퍼파라미터 설정\n",
    "max_len = 69\n",
    "warmup_ratio = 0.28879950890672995\n",
    "num_epochs = 10\n",
    "learning_rate = 1.0017947833154703e-05\n",
    "dr_rate = 0.4968846237164871\n",
    "\n",
    "# 데이터셋 및 데이터로더 생성\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "# 모델 생성\n",
    "model = BERTClassifier(bertmodel, dr_rate=dr_rate).to(device)\n",
    "\n",
    "# 옵티마이저와 스케줄러 설정\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "# 최종 모델 학습\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_acc, train_f1, train_precision, train_recall = 0.0, 0.0, 0.0, 0.0\n",
    "    for batch_id, (token_ids, _, segment_ids, label) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        out = model(token_ids, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 확률 기반 이상치 탐지 적용 (임계값 threshold = 0.95)\n",
    "        preds = classify_with_outlier_detection(out, threshold=0.95)\n",
    "\n",
    "        # 성능 지표 계산: preds 사용\n",
    "        batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(preds, label)\n",
    "        train_acc += batch_acc\n",
    "        train_f1 += batch_f1\n",
    "        train_precision += batch_precision\n",
    "        train_recall += batch_recall\n",
    "\n",
    "    print(f'Epoch {epoch + 1}: Train Accuracy: {train_acc/len(train_dataloader)}, F1: {train_f1/len(train_dataloader)}, Precision: {train_precision/len(train_dataloader)}, Recall: {train_recall/len(train_dataloader)}')\n",
    "\n",
    "# 테스트 데이터 예측 및 이상치 탐지 적용\n",
    "model.eval()\n",
    "test_acc, test_f1, test_precision, test_recall = 0.0, 0.0, 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for batch_id, (token_ids, _, segment_ids, label) in enumerate(test_dataloader):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        # 모델 예측\n",
    "        out = model(token_ids, segment_ids)\n",
    "\n",
    "        # 확률 기반 이상치 탐지 적용 (임계값 threshold = 0.95)\n",
    "        preds = classify_with_outlier_detection(out, threshold=0.95)\n",
    "\n",
    "        # 성능 지표 계산: preds 사용\n",
    "        batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(preds, label)\n",
    "        test_acc += batch_acc\n",
    "        test_f1 += batch_f1\n",
    "        test_precision += batch_precision\n",
    "        test_recall += batch_recall\n",
    "\n",
    "print(f'Test Accuracy: {test_acc/len(test_dataloader)}, F1: {test_f1/len(test_dataloader)}, Precision: {test_precision/len(test_dataloader)}, Recall: {test_recall/len(test_dataloader)}')\n",
    "\n",
    "\n",
    "# 확률 기반 이상치 탐지 함수\n",
    "def classify_with_outlier_detection(logits, threshold=0.95):\n",
    "    probs = F.softmax(logits, dim=1)  # 클래스별 확률 계산\n",
    "    max_probs, preds = torch.max(probs, dim=1)  # 가장 높은 확률과 그 클래스\n",
    "    \n",
    "    # 확률이 threshold 이하인 경우 '일반 대화'로 분류 (label: 4)\n",
    "    preds[max_probs < threshold] = 4\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3b95a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22])\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ab4bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd3addc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10426/4250175812.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2a4456b7fe4c1ebe68fe47ae70a674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to threshold095.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# 확률 기반 이상치 탐지 함수\n",
    "def classify_with_outlier_detection(logits, threshold=0.95):\n",
    "    probs = F.softmax(logits, dim=1)  # 클래스별 확률 계산\n",
    "    max_probs, preds = torch.max(probs, dim=1)  # 가장 높은 확률과 그 클래스\n",
    "    \n",
    "    # 확률이 threshold 이하인 경우 '일반 대화'로 분류 (label: 4)\n",
    "    preds[max_probs < threshold] = 4\n",
    "    return preds\n",
    "\n",
    "# test.json 파일 경로 설정\n",
    "filepath_test = os.getenv('HOME')+'/aiffel/dktc/data/test.json'\n",
    "\n",
    "# test.json 파일을 DataFrame으로 읽기 (pandas를 사용하는 경우)\n",
    "test_data = pd.read_json(filepath_test)\n",
    "\n",
    "# 각 열에 있는 'text' 데이터를 추출하여 하나의 리스트로 만듭니다.\n",
    "test_list = []\n",
    "\n",
    "for column in test_data.columns:\n",
    "    conversation = test_data[column]['text']\n",
    "    test_list.append([conversation])  # 각 대화를 리스트에 추가\n",
    "\n",
    "# 텍스트 클렌징 함수 (train에서 사용한 것과 동일하게 적용)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# 전처리 적용\n",
    "test_list = [[clean_text(conversation[0])] for conversation in test_list]\n",
    "\n",
    "# 토큰화 및 데이터셋 변환 (BERTDataset 클래스 및 토크나이저는 이미 정의되어 있다고 가정)\n",
    "data_test = BERTDataset(test_list, 0, None, tok, max_len, True, False)\n",
    "\n",
    "# 테스트 데이터 로더 생성\n",
    "test_dataloader = DataLoader(data_test, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "# 모델 예측\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        out = model(token_ids, segment_ids)\n",
    "        \n",
    "        # 확률 기반 이상치 탐지 적용 (임계값 threshold = 0.95)\n",
    "        preds = classify_with_outlier_detection(out, threshold=0.95)  # 확률 기반 분류 적용\n",
    "        predictions.extend(preds.cpu().numpy())  # 예측된 클래스 저장\n",
    "\n",
    "# 숫자 레이블을 클래스명으로 변환하는 매핑\n",
    "label_mapping = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\"  # '일반 대화' 추가\n",
    "}\n",
    "\n",
    "# 예측된 숫자 레이블을 클래스명으로 변환\n",
    "predicted_classes = [label_mapping[pred] for pred in predictions]\n",
    "\n",
    "# 예측 결과를 DataFrame으로 변환\n",
    "submission = pd.DataFrame({\n",
    "    'idx': test_data.columns,  # 각 대화에 해당하는 열 이름 (예: 't_000', 't_001' 등)\n",
    "    'target': predicted_classes  # 예측된 클래스명\n",
    "})\n",
    "\n",
    "# 제출 파일로 저장\n",
    "submission.to_csv('./threshold095.csv', index=False)\n",
    "print(\"Results saved to threshold095.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e03e8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.4968846237164871, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d44649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
