{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a28d835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 라이브러리 임포트\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be57043",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1010cc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 3950\n"
     ]
    }
   ],
   "source": [
    "# 사전에 다운로드 받은 경로를 통해 데이터 불러오기\n",
    "filepath = os.getenv('HOME')+'/aiffel/dktc/data/train.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a7eb0fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게.\\n 정말 잘못했습니다.\\n 너가 선택해. 너가 죽을래 네 가족을 죽여줄까.\\n 죄송합니다. 정말 잘못했습니다.\\n 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야.\\n 선택 못하겠습니다. 한번만 도와주세요.\\n 그냥 다 죽여버려야겠군. 이의 없지?\\n 제발 도와주세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 한번만 더 얘기한다.\\n장난전화 걸지 마시죠. \\n9시 40분 마트에 폭발물이 터지면 다 죽는거야. \\n장난전화는 업무방해죄에 해당됩니다.\\n판단은 너에게 달려있다. 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지.\\n선생님 진정하세요.\\n난 이야기했어. 경고했다는 말이야.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미없어.\\n지영아 너가 키 160이지? 그럼 재는 160도 안돼는거네?\\n너 군대도 안가고 좋겠다.\\n니들이 나 작은데 보태준거 있냐?\\n난쟁이들도 장가가고하던데. 너도 희망을 가져봐 \\n더이상 하지마라. \\n그 키크는 수술도 있대잖아? 니네 엄마는 그거 안해주디?\\n나람 해줬어. 저 키로 어찌살아.\\n제발 그만 괴롭히라고!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?\\n얘 돈 좀 있나봐\\n아니에요.돈 없어요\\n뒤져서 나오면 넌 죽는다\\n오늘 피시방 콜?\\n콜. 마지막 기회다. 있는거 다 내놔\\n정말 없어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요?\\n아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요!\\n여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요.\\n음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요.\\n이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요.\\n네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요.\\n아니 저기요 돈 안내요?\\n네? 발라보는것도 돈 내야 하나요?\\n그럼 이거 누구한테 팔아요? 당신이 바른거를?\\n아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요?\\n내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지\\n진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class  \\\n",
       "0    0      협박 대화   \n",
       "1    1      협박 대화   \n",
       "2    2  기타 괴롭힘 대화   \n",
       "3    3      갈취 대화   \n",
       "4    4      갈취 대화   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                              conversation  \n",
       "0                                                                                                                                                                                                                              지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해? 진짜 죽여버리고 싶게.\\n 정말 잘못했습니다.\\n 너가 선택해. 너가 죽을래 네 가족을 죽여줄까.\\n 죄송합니다. 정말 잘못했습니다.\\n 너에게는 선택권이 없어. 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야.\\n 선택 못하겠습니다. 한번만 도와주세요.\\n 그냥 다 죽여버려야겠군. 이의 없지?\\n 제발 도와주세요.  \n",
       "1                                                                                                                                                                                                                                                                                    길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 한번만 더 얘기한다.\\n장난전화 걸지 마시죠. \\n9시 40분 마트에 폭발물이 터지면 다 죽는거야. \\n장난전화는 업무방해죄에 해당됩니다.\\n판단은 너에게 달려있다. 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지.\\n선생님 진정하세요.\\n난 이야기했어. 경고했다는 말이야.  \n",
       "2                                                                                                                                                                                                                                                      너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미없어.\\n지영아 너가 키 160이지? 그럼 재는 160도 안돼는거네?\\n너 군대도 안가고 좋겠다.\\n니들이 나 작은데 보태준거 있냐?\\n난쟁이들도 장가가고하던데. 너도 희망을 가져봐 \\n더이상 하지마라. \\n그 키크는 수술도 있대잖아? 니네 엄마는 그거 안해주디?\\n나람 해줬어. 저 키로 어찌살아.\\n제발 그만 괴롭히라고!  \n",
       "3                                                                                                                                                                                                                                                                                                                                                            어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?\\n얘 돈 좀 있나봐\\n아니에요.돈 없어요\\n뒤져서 나오면 넌 죽는다\\n오늘 피시방 콜?\\n콜. 마지막 기회다. 있는거 다 내놔\\n정말 없어요  \n",
       "4  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요?\\n아 진짜요? 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요!\\n여기 한 번 발라보세요. 진짜 성분도 좋고 다 좋아요.\\n음. 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요. 피부가 따끔거리네요.\\n이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요.\\n네 많이 예민해요. 그럼 많이 파시고 안녕히 계세요.\\n아니 저기요 돈 안내요?\\n네? 발라보는것도 돈 내야 하나요?\\n그럼 이거 누구한테 팔아요? 당신이 바른거를?\\n아니 먼저 발라 보시라고 하셨잖아요. 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요?\\n내가 안 사도 된다고 말 한 적 있어? 그것도 모르고 바른걸 누구 탓 하나? 빨리 사 당신이 바른거 당신이 사야지\\n진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요. 화딱지나네  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a7111",
   "metadata": {},
   "source": [
    "전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "47ca72b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 3950\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거 및 Null 값 제거\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3c8d0612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"\\n\", \"\", sentence)   # 문장 끝에 온점 \n",
    "    sentence = re.sub(r\"([?.!,])\", \" \", sentence)   # 문자를 한번만 사용 .. --> .\n",
    "#     sentence = re.sub(r\"([?.!,])\\1+\", \"\\1\", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # white space 데이터를 empty value로 변경\n",
    "\n",
    "    # (문자, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r'[^\\w\\W]', \"\", sentence)\n",
    "    return sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0eab4cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요', '길동경찰서입니다 9시 40분 마트에 폭발물을 설치할거다 네 똑바로 들어 한번만 더 얘기한다 장난전화 걸지 마시죠 9시 40분 마트에 폭발물이 터지면 다 죽는거야 장난전화는 업무방해죄에 해당됩니다 판단은 너에게 달려있다 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지 선생님 진정하세요 난 이야기했어 경고했다는 말이야', '너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지영아 너가 키 160이지 그럼 재는 160도 안돼는거네 너 군대도 안가고 좋겠다 니들이 나 작은데 보태준거 있냐 난쟁이들도 장가가고하던데 너도 희망을 가져봐 더이상 하지마라 그 키크는 수술도 있대잖아 니네 엄마는 그거 안해주디 나람 해줬어 저 키로 어찌살아 제발 그만 괴롭히라고', '어이 거기예 너 말이야 너 이리 오라고무슨 일 너 옷 좋아보인다 얘 돈 좀 있나봐아니에요 돈 없어요뒤져서 나오면 넌 죽는다오늘 피시방 콜 콜 마지막 기회다 있는거 다 내놔정말 없어요', '저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요 아 진짜요 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요 여기 한 번 발라보세요 진짜 성분도 좋고 다 좋아요 음 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요 피부가 따끔거리네요 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요 네 많이 예민해요 그럼 많이 파시고 안녕히 계세요 아니 저기요 돈 안내요 네 발라보는것도 돈 내야 하나요 그럼 이거 누구한테 팔아요 당신이 바른거를 아니 먼저 발라 보시라고 하셨잖아요 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요 내가 안 사도 된다고 말 한 적 있어 그것도 모르고 바른걸 누구 탓 하나 빨리 사 당신이 바른거 당신이 사야지진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요 화딱지나네']\n"
     ]
    }
   ],
   "source": [
    "# Q 데이터 전처리\n",
    "clean_conv = data['conversation'].apply(lambda x: preprocess_sentence(x)).tolist()\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_conv[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "30e65792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 3950\n"
     ]
    }
   ],
   "source": [
    "data['conversation'] = clean_conv\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0, inplace=True)\n",
    "df_cleaned = data.copy(deep=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd555532",
   "metadata": {},
   "source": [
    "토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4a78838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8096]\n",
      "END_TOKEN의 번호 : [8097]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    clean_conv, target_vocab_size=2**13)\n",
    "\n",
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e78c435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['내가_', '너_', '야_', '네_', '나_', '이_', '좀_', '아_', '도_', '다_', '왜_', '고_', '지_', '게_', '진짜_', '는_', '가_', '아니_', '지금_', '요_', '면_', '을_', '니_', '거_', '서_', '에_', '돈_', '그럼_', '어_', '은_', '죄송합니다_', '그냥_', '안_', '내_', '제가_', '한_', '그래_', '저_', '무슨_', '뭐_', '를_', '이거_', '만_', '해_', '그_', '제발_', '더_', '그렇게_', '로_', '우리_', '니가_', '냐_', '잘_', '어떻게_', '수_', '너무_', '할_', '이', '아니야_', '오늘_', '지', '하고_', '못_', '빨리_', '정말_', '데_', '것_', '넌_', '그게_', '거야_', '너가_', '나도_', '어', '이렇게_', '요', '니까_', '그건_', '일_', '난_', '말_', '안', '줄_', '응_', '다', '가', '잖아_', '자', '고', '없어_', '라고_', '네', '는데_', '여기_', '아', '있어_', '까지_', '으로_', '뭘_', '나', '근데_']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 된 100개의 서브워드들을 출력\n",
    "print(tokenizer.subwords[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ad24bb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8098\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "631d7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 : [2, 427, 380, 4089, 115, 27, 5107, 1577, 86, 4304, 22, 11, 19, 5512, 3, 10, 2775, 3449, 191, 695, 106, 1206, 1, 11, 2718, 2252, 10, 2775, 6616, 2675, 51, 3763, 717, 27, 184, 2030, 123, 1924, 1707, 923, 707, 21, 2252, 6943, 4389, 3, 19, 6155, 89, 630, 2775, 3668, 397, 482, 802]\n",
      "기존 문장 : 너 저번에 친구 지갑에서 돈 훔쳤잖아 그말을 왜 지금 꺼내 야 다 불어버리기전에 얼마 나한테 줘봐 내가 왜 줘야되는데 다 불어버릴거라니까 니가 가져간 돈 주인이랑 모든 사람들에게 얼마면 되는데 10만원만 줘라 야 지금 10만원 없어 확 불어버릴까보다 내일까지 줄게\n"
     ]
    }
   ],
   "source": [
    "sample_string = data['conversation'][21]\n",
    "\n",
    "# 인코딩한 결과를 tokenized_string에 저장\n",
    "tokenized_string = tokenizer.encode(sample_string)\n",
    "print ('정수 인코딩 후의 문장 : {}'.format(tokenized_string))\n",
    "\n",
    "# 이를 다시 디코딩\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장 : {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5befd74f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conversation의 최소 길이 : 1\n",
      "conversation의 최대 길이 : 219\n",
      "conversation의 평균 길이 : 54.13569620253165\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "\n",
    "\n",
    "text_len = [len(s.split()) for s in data['conversation']]\n",
    "\n",
    "print('conversation의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('conversation의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('conversation의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('conversation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('conversation')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "388df1ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너 저번에 친구 지갑에서 돈 훔쳤잖아 그말을 왜 지금 꺼내 야 다 불어버리기전에 얼마 나한테 줘봐 내가 왜 줘야되는데 다 불어버릴거라니까 니가 가져간 돈 주인이랑 모든 사람들에게 얼마면 되는데 10만원만 줘라 야 지금 10만원 없어 확 불어버릴까보다 내일까지 줄게'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c762f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('conversation.txt', 'w', encoding='utf8') as f:\n",
    "    f.write('\\n'.join(data['conversation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5a6acb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(lowercase=False, strip_accents=False)\n",
    "\n",
    "data_file = 'conversation.txt'\n",
    "vocab_size = 30000\n",
    "limit_alphabet = 9000\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer.train(files=data_file,\n",
    "                vocab_size=vocab_size,\n",
    "                limit_alphabet=limit_alphabet,\n",
    "                min_frequency=min_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "26862cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./vocab.txt']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 저장\n",
    "tokenizer.save_model('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ca9b2697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[PAD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[CLS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0   [PAD]\n",
       "1   [UNK]\n",
       "2   [CLS]\n",
       "3   [SEP]\n",
       "4  [MASK]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab 로드\n",
    "df = pd.read_fwf('vocab.txt', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bf178628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 결과 : ['지금', '너', '스스로', '##를', '죽여', '##달라고', '애', '##원', '##하는', '것', '##인가', '아닙니다', '죄송합니다', '죽을', '거면', '혼자', '죽지', '우리', '##까지', '사건에', '휘', '##말', '##리게', '해', '진짜', '죽여버리고', '싶', '##게', '정말', '잘못했습니다', '너가', '선택해', '너가', '죽을래', '네', '가족을', '죽여줄까', '죄송합니다', '정말', '잘못했습니다', '너에게', '##는', '선택권', '##이', '없어', '선택', '못한다', '##면', '너와', '네', '가족까지', '모', '##조리', '죽여버릴거야', '선택', '못하겠', '##습니다', '한번만', '도와주세요', '그냥', '다', '죽여', '##버려야', '##겠군', '이', '##의', '없지', '제발', '도와주세요']\n",
      "정수 인코딩 : [2795, 230, 6465, 1535, 2913, 3473, 902, 1643, 2827, 50, 3393, 2930, 2799, 3252, 9608, 3170, 9085, 2801, 2809, 12060, 1468, 1727, 4140, 1413, 2792, 5605, 834, 1559, 2842, 5036, 2862, 10484, 2862, 5257, 238, 7173, 9335, 2799, 2842, 5036, 6956, 1549, 9503, 1493, 2818, 4341, 6985, 1586, 7728, 238, 9356, 572, 10092, 3564, 4341, 8140, 2784, 3124, 5032, 2805, 292, 2913, 6164, 7499, 1011, 1723, 3468, 2816, 5032]\n",
      "디코딩 : 지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode('지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요')\n",
    "print('토큰화 결과 :',encoded.tokens)\n",
    "print('정수 인코딩 :',encoded.ids)\n",
    "print('디코딩 :',tokenizer.decode(encoded.ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a945935",
   "metadata": {},
   "source": [
    "# koBERT\n",
    "\n",
    "참고 링크\n",
    "[https://velog.io/@jiyoung/Text-ClassificationKoBERT%EB%A1%9C-%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98%ED%95%98%EA%B8%B0-%EC%BD%94%EB%93%9C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2bfbd273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-0z5gmf28\n",
      "  Running command git clone --filter=blob:none -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-0z5gmf28\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3<=1.15.18\n",
      "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 5.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: gluonnlp<=0.10.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (0.10.0)\n",
      "Collecting mxnet<=1.7.0.post2,>=1.4.0\n",
      "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
      "     |████████████████████████████████| 54.7 MB 21 kB/s              \n",
      "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
      "  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "     |████████████████████████████████| 4.5 MB 55.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.9.1+cu111)\n",
      "Collecting transformers<=4.8.1,>=4.8.1\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "     |████████████████████████████████| 2.5 MB 53.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.4)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     |████████████████████████████████| 73 kB 3.6 MB/s             \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.19.0,>=1.18.18\n",
      "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
      "     |████████████████████████████████| 6.7 MB 58.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: cython in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.24)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.9/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.26.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (0.8.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.62.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.4.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.46)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2021.11.10)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
      "Collecting urllib3<1.26,>=1.20\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 97.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.0.3)\n",
      "Building wheels for collected packages: kobert\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15708 sha256=7d812be53339abe6a7680f5550f5362c08054b82a70f99ab05db8cb737b6d137\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5o6k9hbl/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
      "Successfully built kobert\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, huggingface-hub, transformers, onnxruntime, mxnet, boto3, kobert\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.20\n",
      "    Uninstalling urllib3-1.26.20:\n",
      "      Successfully uninstalled urllib3-1.26.20\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "  Attempting uninstall: mxnet\n",
      "    Found existing installation: mxnet 1.9.1\n",
      "    Uninstalling mxnet-1.9.1:\n",
      "      Successfully uninstalled mxnet-1.9.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentry-sdk 2.15.0 requires urllib3>=1.26.11, but you have urllib3 1.25.11 which is incompatible.\n",
      "selenium 4.0.0 requires urllib3[secure]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
      "datasets 1.14.0 requires huggingface-hub<0.1.0,>=0.0.19, but you have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.15.18 botocore-1.18.18 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 transformers-4.8.1 urllib3-1.25.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install gluonnlp\n",
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4a97dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cc7279d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다 9시 40분 마트에 폭발물을 설치할거다 네 똑바로 들어 한번만 더 얘기한다 장난전화 걸지 마시죠 9시 40분 마트에 폭발물이 터지면 다 죽는거야 장난전화는 업무방해죄에 해당됩니다 판단은 너에게 달려있다 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지 선생님 진정하세요 난 이야기했어 경고했다는 말이야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지영아 너가 키 160이지 그럼 재는 160도 안돼는거네 너 군대도 안가고 좋겠다 니들이 나 작은데 보태준거 있냐 난쟁이들도 장가가고하던데 너도 희망을 가져봐 더이상 하지마라 그 키크는 수술도 있대잖아 니네 엄마는 그거 안해주디 나람 해줬어 저 키로 어찌살아 제발 그만 괴롭히라고</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기예 너 말이야 너 이리 오라고무슨 일 너 옷 좋아보인다 얘 돈 좀 있나봐아니에요 돈 없어요뒤져서 나오면 넌 죽는다오늘 피시방 콜 콜 마지막 기회다 있는거 다 내놔정말 없어요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요 아 진짜요 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요 여기 한 번 발라보세요 진짜 성분도 좋고 다 좋아요 음 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요 피부가 따끔거리네요 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요 네 많이 예민해요 그럼 많이 파시고 안녕히 계세요 아니 저기요 돈 안내요 네 발라보는것도 돈 내야 하나요 그럼 이거 누구한테 팔아요 당신이 바른거를 아니 먼저 발라 보시라고 하셨잖아요 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요 내가 안 사도 된다고 말 한 적 있어 그것도 모르고 바른걸 누구 탓 하나 빨리 사 당신이 바른거 당신이 사야지진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요 화딱지나네</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>나 이틀뒤에 가나다 음식점 예약좀 해줘 저녁7시로 가나다 음식점이요 응 남자친구 부모님한테 인사드리려는데 거기가 예약이 좀 힘들어 그러니까 수진씨가 좀 해줘 저 팀장님 저도 월 말 프로젝트로 정신없어서 죄송하지만 사회생활 안 해본 티를 너무 내는거 아니야 프로젝트만 백날 잘하면 뭐해 윗 상사한테 잘 보이기도 해야지 하지만 팀 프로젝트라서 이번엔 말 참 이상하게 하네 이번엔 내가 뭐 매일같이 이런 심부름이나 시킨다는거야 뭐야 아닙니다 제가 말 실수 했습니다 말씀하신 예약 꼭 해두겠습니다 이러면 하고도 욕먹는거야 한번에 네네 알겠습니다 하면 좀 좋아 죄송합니다 알겠습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>35번 손님 아이스커피 두잔나왔습니다아이스커피 네 맛있게드세요저기요 아가씨 나는 아아스 시킨적이 없는데 아 분명 오늘 날이 더우시다고 아이스로 시키셨는데요내가 그랫어 네분명히 아그런 기억이 없는데 아가씨가잘못안거 아니야 아니요 오늘 손님이 첫 주문이라 확실히 기억하고 있습니다아가씨 왜이렇게 유도리가 없이 굴어 그냥 아 제가 잘못 주문 받았습니다 하면 되지 네 어휴 유도리 없어 그냥 마실게</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>경비아저씨 내차에 경고장 붙였어요 내가 여기 몇년째 사는데 아직도 내차인줄 모르고 이딴식으로 경고장을 붙여 입주민께서 정해진 주차자리가 아닌 통로에 계속 주차하셔서 입주민들 항의가 계속 들어옵니다 내가 항상 대는 거기 자리가 없으니까 그렇지 내차 이거 어떻게 할거요 얼마짜린줄 알아 수차례 인터폰도 드렸고 경고장을 올렸으나 몇일째 그대로 인지라 아파트규약에 맞춰 어쩔수 없었습니다그렇다고 본드로 붙이면 어떻게 당장 이거 깨끗히 원상복구시키고 세차해와그럴 수는 없습니다 내가 낸 관리비로 월급받는 주제에 뭐라는거야 안붙혔으면 되잖아 나 지금 거기 가요 말씀이 과하신거 아닙니까 딱 기다려요 이름뭐죠 나 지금 가면 당신 이제 끝이야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>이거 할인 된다면서요 확인해보겠습니다 아까 저 시람은 할인 해주던데 네 저 손님은 고객님이십니다 고객을 차별하는 거에요 저희는 등급제로 운영돼서 그런겁니다 점장 불러오세요 손님 이건 점장님도 해결해주시지 못합니다 그니까 점장 불러와 진정을 좀 해보세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>자네 세금계산서 보내준 거 메일이랑 수치가 다르던데 그게 얼마나 해먹었어 괜찮아 다른 업체들도 다 그래 회사에는 비밀로 해주십시오 그래그래 근데 성의는 표시해야지 어느정도 생각하시죠 10장 네가 많이도 해먹었는데 그정도는 되겠지 그건 너무 많습니다 그럼 김사장한테 직접 말하는 수밖에 없지 그리고 이 건은 국세청에도 걸리는거 알지 좀 더 기한을 주십시오</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx        class  \\\n",
       "0    0        협박 대화   \n",
       "1    1        협박 대화   \n",
       "2    2    기타 괴롭힘 대화   \n",
       "3    3        갈취 대화   \n",
       "4    4        갈취 대화   \n",
       "5    5  직장 내 괴롭힘 대화   \n",
       "6    6    기타 괴롭힘 대화   \n",
       "7    7    기타 괴롭힘 대화   \n",
       "8    8    기타 괴롭힘 대화   \n",
       "9    9        갈취 대화   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                            conversation  \n",
       "0                                                                                                                                                                                                                               지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요  \n",
       "1                                                                                                                                                                                                                                                                         길동경찰서입니다 9시 40분 마트에 폭발물을 설치할거다 네 똑바로 들어 한번만 더 얘기한다 장난전화 걸지 마시죠 9시 40분 마트에 폭발물이 터지면 다 죽는거야 장난전화는 업무방해죄에 해당됩니다 판단은 너에게 달려있다 길동경찰서에도 폭발물 터지면 꽤나 재미있겠지 선생님 진정하세요 난 이야기했어 경고했다는 말이야  \n",
       "2                                                                                                                                                                                                                                              너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지영아 너가 키 160이지 그럼 재는 160도 안돼는거네 너 군대도 안가고 좋겠다 니들이 나 작은데 보태준거 있냐 난쟁이들도 장가가고하던데 너도 희망을 가져봐 더이상 하지마라 그 키크는 수술도 있대잖아 니네 엄마는 그거 안해주디 나람 해줬어 저 키로 어찌살아 제발 그만 괴롭히라고  \n",
       "3                                                                                                                                                                                                                                                                                                                                                  어이 거기예 너 말이야 너 이리 오라고무슨 일 너 옷 좋아보인다 얘 돈 좀 있나봐아니에요 돈 없어요뒤져서 나오면 넌 죽는다오늘 피시방 콜 콜 마지막 기회다 있는거 다 내놔정말 없어요  \n",
       "4  저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발라보실래요 아 진짜요 안 그래도 선크림 필요해서 알아보던 중인데 한 번 발라 볼게요 여기 한 번 발라보세요 진짜 성분도 좋고 다 좋아요 음 성분이 좋다고 하셔서 좋은거 같기는 한데 제 피부에 맞지 않나봐요 피부가 따끔거리네요 이번에 진짜 열심히 연구해서 만든건데 피부가 많이 예민하신가봐요 네 많이 예민해요 그럼 많이 파시고 안녕히 계세요 아니 저기요 돈 안내요 네 발라보는것도 돈 내야 하나요 그럼 이거 누구한테 팔아요 당신이 바른거를 아니 먼저 발라 보시라고 하셨잖아요 먼저 권유해놓고 사라고 강매하는거 갈취인거 몰라요 내가 안 사도 된다고 말 한 적 있어 그것도 모르고 바른걸 누구 탓 하나 빨리 사 당신이 바른거 당신이 사야지진짜 어이가 없어서 다른 사람들한텐 이렇게 갈취하지마세요 화딱지나네  \n",
       "5                                                                                                                       나 이틀뒤에 가나다 음식점 예약좀 해줘 저녁7시로 가나다 음식점이요 응 남자친구 부모님한테 인사드리려는데 거기가 예약이 좀 힘들어 그러니까 수진씨가 좀 해줘 저 팀장님 저도 월 말 프로젝트로 정신없어서 죄송하지만 사회생활 안 해본 티를 너무 내는거 아니야 프로젝트만 백날 잘하면 뭐해 윗 상사한테 잘 보이기도 해야지 하지만 팀 프로젝트라서 이번엔 말 참 이상하게 하네 이번엔 내가 뭐 매일같이 이런 심부름이나 시킨다는거야 뭐야 아닙니다 제가 말 실수 했습니다 말씀하신 예약 꼭 해두겠습니다 이러면 하고도 욕먹는거야 한번에 네네 알겠습니다 하면 좀 좋아 죄송합니다 알겠습니다  \n",
       "6                                                                                                                                                                                                                             35번 손님 아이스커피 두잔나왔습니다아이스커피 네 맛있게드세요저기요 아가씨 나는 아아스 시킨적이 없는데 아 분명 오늘 날이 더우시다고 아이스로 시키셨는데요내가 그랫어 네분명히 아그런 기억이 없는데 아가씨가잘못안거 아니야 아니요 오늘 손님이 첫 주문이라 확실히 기억하고 있습니다아가씨 왜이렇게 유도리가 없이 굴어 그냥 아 제가 잘못 주문 받았습니다 하면 되지 네 어휴 유도리 없어 그냥 마실게  \n",
       "7                                                                                        경비아저씨 내차에 경고장 붙였어요 내가 여기 몇년째 사는데 아직도 내차인줄 모르고 이딴식으로 경고장을 붙여 입주민께서 정해진 주차자리가 아닌 통로에 계속 주차하셔서 입주민들 항의가 계속 들어옵니다 내가 항상 대는 거기 자리가 없으니까 그렇지 내차 이거 어떻게 할거요 얼마짜린줄 알아 수차례 인터폰도 드렸고 경고장을 올렸으나 몇일째 그대로 인지라 아파트규약에 맞춰 어쩔수 없었습니다그렇다고 본드로 붙이면 어떻게 당장 이거 깨끗히 원상복구시키고 세차해와그럴 수는 없습니다 내가 낸 관리비로 월급받는 주제에 뭐라는거야 안붙혔으면 되잖아 나 지금 거기 가요 말씀이 과하신거 아닙니까 딱 기다려요 이름뭐죠 나 지금 가면 당신 이제 끝이야  \n",
       "8                                                                                                                                                                                                                                                                                                           이거 할인 된다면서요 확인해보겠습니다 아까 저 시람은 할인 해주던데 네 저 손님은 고객님이십니다 고객을 차별하는 거에요 저희는 등급제로 운영돼서 그런겁니다 점장 불러오세요 손님 이건 점장님도 해결해주시지 못합니다 그니까 점장 불러와 진정을 좀 해보세요  \n",
       "9                                                                                                                                                                                                                                                  자네 세금계산서 보내준 거 메일이랑 수치가 다르던데 그게 얼마나 해먹었어 괜찮아 다른 업체들도 다 그래 회사에는 비밀로 해주십시오 그래그래 근데 성의는 표시해야지 어느정도 생각하시죠 10장 네가 많이도 해먹었는데 그정도는 되겠지 그건 너무 많습니다 그럼 김사장한테 직접 말하는 수밖에 없지 그리고 이 건은 국세청에도 걸리는거 알지 좀 더 기한을 주십시오  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "46712139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          협박 대화\n",
       "2      기타 괴롭힘 대화\n",
       "3          갈취 대화\n",
       "5    직장 내 괴롭힘 대화\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e6c4f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3c32cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_v1.zip\n",
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#bert 모델, vocab 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3e46b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['class'] == \"협박 대화\"), 'label'] = 0  #협박 대화 => 0\n",
    "data.loc[(data['class'] == \"갈취 대화\"), 'label'] = 1  #갈취 대화 => 1\n",
    "data.loc[(data['class'] == \"갈취 대화\"), 'label'] = 2  #갈취 대화 => 2\n",
    "data.loc[(data['class'] == \"직장 내 괴롭힘 대화\"), 'label'] = 3  #갈취 대화 => 3\n",
    "data.loc[(data['class'] == \"기타 괴롭힘 대화\"), 'label'] = 4  #갈취 대화 => 4\n",
    "data.loc[(data['class'] == \"일반 대화\"), 'label'] = 5  #기타 괴롭힘 대화 => 5\n",
    "\n",
    "data_list = []\n",
    "for content, label in zip(data['conversation'], data['label'])  :\n",
    "    temp = []\n",
    "    temp.append(content)\n",
    "    temp.append(str(int(label)))\n",
    "\n",
    "    data_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "40b1839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['지금 너 스스로를 죽여달라고 애원하는 것인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게 정말 잘못했습니다 너가 선택해 너가 죽을래 네 가족을 죽여줄까 죄송합니다 정말 잘못했습니다 너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야 선택 못하겠습니다 한번만 도와주세요 그냥 다 죽여버려야겠군 이의 없지 제발 도와주세요', '0']\n",
      "['희정씨네 주말에 시간이 넘쳐나나봐갑자기 왜그러세요 손이 빤짝빤짝 네일했니 네 여름이라 기분전환으로 어휴 그손으로 결제 받으러 가면 윗분들 눈 아프시겠다 정신사나워그냥 파스텔톤으로만 칠한건데 희정씨가 윗사람이야 얼른 가서 지우고와 네', '3']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6a313fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train & test 데이터로 나누기\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bf80c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "68959221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 300\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a91422af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "#BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5dc77cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치 및 데이터로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5b9a404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=46, ##주의: 클래스 수 바꾸어 주세요!##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3f08542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device) #gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a528c9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "f047daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "848a0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b8539a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55/2556362077.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9fe25ea2734b468c692213e2f311a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 14.58 GiB total capacity; 12.97 GiB already allocated; 231.56 MiB free; 13.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_55/2556362077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mvalid_length\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_55/1620188463.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, token_ids, valid_length, segment_ids)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdr_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    989\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         )\n\u001b[0;32m--> 991\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    992\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    580\u001b[0m                 )\n\u001b[1;32m    581\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    583\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     ):\n\u001b[0;32m--> 401\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relative_key_query\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 264.00 MiB (GPU 0; 14.58 GiB total capacity; 12.97 GiB already allocated; 231.56 MiB free; 13.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} validation acc {}\".format(e+1, test_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31171c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf737a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
