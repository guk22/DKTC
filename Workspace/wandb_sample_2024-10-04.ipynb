{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7740023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 라이브러리 임포트\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea53557c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전에 다운로드 받은 경로를 통해 데이터 불러오기\n",
    "filepath_train = os.getenv('HOME')+'/aiffel/dktc/data/train.csv'\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739134db",
   "metadata": {},
   "source": [
    "데이터 전처리 및 라벨링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1285fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 클렌징 함수\n",
    "def clean_text(text):\n",
    "    # 불필요한 특수 문자, 숫자 제거 (한글, 영문, 공백 제외)\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "train['conversation'] = train['conversation'].apply(clean_text)\n",
    "\n",
    "train.loc[(train['class'] == \"협박 대화\"), 'label'] = 0  # 협박 대화 => 0\n",
    "train.loc[(train['class'] == \"갈취 대화\"), 'label'] = 1  # 갈취 대화 => 1\n",
    "train.loc[(train['class'] == \"직장 내 괴롭힘 대화\"), 'label'] = 2  # 직장 내 괴롭힘 대화 => 2\n",
    "train.loc[(train['class'] == \"기타 괴롭힘 대화\"), 'label'] = 3  # 기타 괴롭힘 대화 => 3\n",
    "train.loc[(train['class'] == \"일반 대화\"), 'label'] = 4  # 일반 대화 => 4\n",
    "\n",
    "data_list = []\n",
    "for content, label in zip(train['conversation'], train['label'])  :\n",
    "    temp = []\n",
    "    temp.append(content)\n",
    "    temp.append(str(int(label)))\n",
    "\n",
    "    data_list.append(temp)\n",
    "\n",
    "train.drop(columns = 'idx', inplace = True)\n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8adc9",
   "metadata": {},
   "source": [
    "Bert 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e79a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_v1.zip\n",
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "using cached model. /aiffel/aiffel/dktc/src/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "## Setting model parameters\n",
    "max_len = 130     # 65~200,  패딩이 많아서 줄여봐도 좋을 듯\n",
    "batch_size = 50     #넘으면 안돌아감 \n",
    "warmup_ratio = 0.2     #  0.1 로 하니까 안 좋음\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5\n",
    "dr_rate = 0.5\n",
    "\n",
    "\n",
    "#bert 모델, vocab 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "\n",
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "#BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "#배치 및 데이터로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92787e4",
   "metadata": {},
   "source": [
    "모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ee2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert 모델 정의\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "    \n",
    "\n",
    "\n",
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
    "\n",
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e379db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb==0.16.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee37dc9",
   "metadata": {},
   "source": [
    "모델 학습 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769a8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ebe443",
   "metadata": {},
   "source": [
    "https://wandb.ai/settings 접속\n",
    "Danger Zone에서 키 생성 혹은 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4839dc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msjjky27\u001b[0m (\u001b[33msjjky27-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /aiffel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 본인키 복사\n",
    "\n",
    "wandb.login(key = \"9ee7086efc9eab486f064f510a83d43bc3ea91aa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b52219",
   "metadata": {},
   "source": [
    "Sweep 설정 [https://docs.wandb.ai/guides/sweeps/define-sweep-configuration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d7feb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "파라미터 설정\n",
    "\n",
    "- validation f1 score를 최대화하는 방향으로 하이퍼 파라미터 튜닝 (metrics 파라미터 세팅으로 변경할 수 있음)\n",
    "- 각 파라미터의 범위는 리스트로 지정 가능 또는 분포를 통해 지정할 수 있음\n",
    "- early_terminate의 경우 사전에 정해진 조건에 달할 경우 튜닝을 멈추는 \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# wandb parameter setting\n",
    "\n",
    "WANDB_ID = \"sjjky27\"  # 본인 wandb 계정 ID 넣어주기\n",
    "\n",
    "PROJ_NAME = \"kobert_hyperparameter_tuning\"\n",
    "\n",
    "sweep_config = {\n",
    "    \"name\": \"sweep_test_nlp\",\n",
    "    \"metric\": {\"name\": \"val_f1\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"random\",\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [5e-5]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"values\": [5, 10, 15]\n",
    "#             \"distribution\": \"int_uniform\",\n",
    "#             \"min\": 5,\n",
    "#             \"max\": 6\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [50]\n",
    "        },\n",
    "        \"max_len\": {\n",
    "            \"values\": [65, 100, 130]\n",
    "        }\n",
    "    },\n",
    "     \"early_terminate\": {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"eta\": 2,\n",
    "        \"min_iter\":2\n",
    "     }\n",
    "}\n",
    "\n",
    "\n",
    "# 성능 지표 계산 함수\n",
    "def calc_metrics(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    pred_labels = max_indices.cpu().numpy()\n",
    "    true_labels = Y.cpu().numpy()\n",
    "    \n",
    "    accuracy = (pred_labels == true_labels).sum() / len(true_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, f1, precision, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4834e1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr><tr><td>train_f1</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.03129</td></tr><tr><td>train_accuracy</td><td>0.94687</td></tr><tr><td>train_f1</td><td>0.94736</td></tr><tr><td>val_accuracy</td><td>0.0</td></tr><tr><td>val_f1</td><td>0.87374</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-1</strong> at: <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/runs/0lbly1vy' target=\"_blank\">https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/runs/0lbly1vy</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241004_095052-0lbly1vy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v1kpfd2p\n",
      "Sweep URL: https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/sweeps/v1kpfd2p\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0jsbmm0c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_len: 65\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/dktc/src/wandb/run-20241004_095311-0jsbmm0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/runs/0jsbmm0c' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/sweeps/v1kpfd2p' target=\"_blank\">https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/sweeps/v1kpfd2p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning' target=\"_blank\">https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/sweeps/v1kpfd2p' target=\"_blank\">https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/sweeps/v1kpfd2p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/runs/0jsbmm0c' target=\"_blank\">https://wandb.ai/sjjky27-personal/kobert_hyperparameter_tuning/runs/0jsbmm0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1, Loss 0.068119116127491, Accuracy 0.98\n",
      "Epoch 1: Validation Accuracy 0.8796875000000001, F10.8795726725774117, Precision 0.8898211927230406, Recall 0.8796875000000001\n",
      "Epoch 2, Batch 1, Loss 0.035277169197797775, Accuracy 0.98\n",
      "Epoch 2: Validation Accuracy 0.8671875000000001, F10.8678119734206196, Precision 0.8795278573550633, Recall 0.8671875000000001\n",
      "Epoch 3, Batch 1, Loss 0.012058161199092865, Accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train 함수 정의\n",
    "def train():\n",
    "    \n",
    "    wandb.init()  # Weights & Biases 초기화\n",
    "    config = wandb.config  # 설정 값 불러오기\n",
    "    \n",
    "\n",
    "    # 학습 루프\n",
    "    for epoch in range(config.epochs):\n",
    "        train_acc = 0.0\n",
    "        train_f1, train_precision, train_recall = 0.0, 0.0, 0.0  # 성능 지표 초기화\n",
    "        test_acc, val_f1, val_precision, val_recall = 0.0, 0.0, 0.0, 0.0  # Validation 성능 지표 초기화\n",
    "        \n",
    "        # 학습모드 전환\n",
    "        model.train()\n",
    "        \n",
    "        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            valid_length = valid_length.to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            out = model(token_ids, valid_length, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 성능 지표 계산\n",
    "            batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "            train_acc += batch_acc\n",
    "            train_f1 += batch_f1\n",
    "            train_precision += batch_precision\n",
    "            train_recall += batch_recall\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_id + 1}, Loss {loss.item()}, Accuracy {train_acc / (batch_id + 1)}\")\n",
    "\n",
    "        # 검증 모드 전환\n",
    "        model.eval()\n",
    "        val_acc = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
    "                token_ids = token_ids.long().to(device)\n",
    "                segment_ids = segment_ids.long().to(device)\n",
    "                valid_length = valid_length.to(device)\n",
    "                label = label.long().to(device)\n",
    "\n",
    "                out = model(token_ids, valid_length, segment_ids)\n",
    "                batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "                test_acc += batch_acc\n",
    "                val_f1 += batch_f1\n",
    "                val_precision += batch_precision\n",
    "                val_recall += batch_recall\n",
    "            print(f\"Epoch {epoch+1}: Validation Accuracy {test_acc/len(test_dataloader)}, F1{val_f1/len(test_dataloader)}, Precision {val_precision/len(test_dataloader)}, Recall {val_recall/len(test_dataloader)}\")\n",
    "\n",
    "        # Wandb에 학습 결과 기록\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_accuracy\": train_acc / len(train_dataloader),\n",
    "            \"val_accuracy\": val_acc / len(test_dataloader),\n",
    "            \"train_f1\": train_f1 / len(train_dataloader),\n",
    "            \"val_f1\": val_f1/len(test_dataloader),\n",
    "            \"loss\": loss.item()\n",
    "        })\n",
    "\n",
    "# Sweep 실행\n",
    "sweep_id = wandb.sweep(sweep_config, project=PROJ_NAME)\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818442d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
