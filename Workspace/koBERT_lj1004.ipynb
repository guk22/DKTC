{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a1719a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n",
      "  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-otgm16ka\n",
      "  Running command git clone --filter=blob:none -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-otgm16ka\n",
      "  Resolved https://****@github.com/SKTBrain/KoBERT.git to commit 5c46b1c68e4755b54879431bd302db621f4d2f47\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3<=1.15.18\n",
      "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
      "     |████████████████████████████████| 129 kB 4.9 MB/s            \n",
      "\u001b[?25hCollecting gluonnlp<=0.10.0,>=0.6.0\n",
      "  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n",
      "     |████████████████████████████████| 344 kB 56.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mxnet<=1.7.0.post2,>=1.4.0\n",
      "  Downloading mxnet-1.7.0.post2-py2.py3-none-manylinux2014_x86_64.whl (54.7 MB)\n",
      "     |████████████████████████████████| 54.7 MB 21 kB/s              \n",
      "\u001b[?25hCollecting onnxruntime<=1.8.0,==1.8.0\n",
      "  Downloading onnxruntime-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "     |████████████████████████████████| 4.5 MB 55.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: sentencepiece<=0.1.96,>=0.1.6 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (0.1.96)\n",
      "Requirement already satisfied: torch<=1.10.1,>=1.7.0 in /opt/conda/lib/python3.9/site-packages (from kobert==0.2.3) (1.9.1+cu111)\n",
      "Collecting transformers<=4.8.1,>=4.8.1\n",
      "  Downloading transformers-4.8.1-py3-none-any.whl (2.5 MB)\n",
      "     |████████████████████████████████| 2.5 MB 57.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.21.4)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.9/site-packages (from onnxruntime<=1.8.0,==1.8.0->kobert==0.2.3) (1.12)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     |████████████████████████████████| 73 kB 3.2 MB/s             \n",
      "\u001b[?25hCollecting botocore<1.19.0,>=1.18.18\n",
      "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
      "     |████████████████████████████████| 6.7 MB 51.0 MB/s            \n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: cython in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (0.29.24)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (21.3)\n",
      "Collecting graphviz<0.9.0,>=0.8.1\n",
      "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /opt/conda/lib/python3.9/site-packages (from mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch<=1.10.1,>=1.7.0->kobert==0.2.3) (4.0.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (2021.11.10)\n",
      "Collecting huggingface-hub==0.0.12\n",
      "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (3.4.0)\n",
      "Collecting urllib3<1.26,>=1.20\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "     |████████████████████████████████| 127 kB 41.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->kobert==0.2.3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->gluonnlp<=0.10.0,>=0.6.0->kobert==0.2.3) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.20.0->mxnet<=1.7.0.post2,>=1.4.0->kobert==0.2.3) (2021.10.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses->transformers<=4.8.1,>=4.8.1->kobert==0.2.3) (1.1.0)\n",
      "Building wheels for collected packages: kobert, gluonnlp\n",
      "  Building wheel for kobert (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kobert: filename=kobert-0.2.3-py3-none-any.whl size=15714 sha256=144d075016ef1c67aabfa6bbd0356f6e4a45955510c1cf209e8d8f2bad57555d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c12nf67o/wheels/0b/20/d8/031374f3d29b5150c59c814bed091fca7d6d4c8218148bf286\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp39-cp39-linux_x86_64.whl size=475209 sha256=a2523871e379fd783dd58c562f9f368402497a97ffc26ec17707484e28e26541\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/47/17/70/b257bc53879a458c4bfcc900e89271aa8b4f19366a54bd2455\n",
      "Successfully built kobert gluonnlp\n",
      "Installing collected packages: urllib3, jmespath, botocore, s3transfer, huggingface-hub, graphviz, transformers, onnxruntime, mxnet, gluonnlp, boto3, kobert\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.11.3\n",
      "    Uninstalling transformers-4.11.3:\n",
      "      Successfully uninstalled transformers-4.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "selenium 4.0.0 requires urllib3[secure]~=1.26, but you have urllib3 1.25.11 which is incompatible.\n",
      "datasets 1.14.0 requires huggingface-hub<0.1.0,>=0.0.19, but you have huggingface-hub 0.0.12 which is incompatible.\u001b[0m\n",
      "Successfully installed boto3-1.15.18 botocore-1.18.18 gluonnlp-0.10.0 graphviz-0.8.4 huggingface-hub-0.0.12 jmespath-0.10.0 kobert-0.2.3 mxnet-1.7.0.post2 onnxruntime-1.8.0 s3transfer-0.3.7 transformers-4.8.1 urllib3-1.25.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "eff4ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dd05ab7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_train = os.getenv('HOME')+'/aiffel/dktc/data/train.csv'\n",
    "\n",
    "train = pd.read_csv(filepath_train)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3769a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d8051c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "idx             0\n",
       "class           0\n",
       "conversation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "7e872059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          협박 대화\n",
       "2      기타 괴롭힘 대화\n",
       "3          갈취 대화\n",
       "5    직장 내 괴롭힘 대화\n",
       "Name: class, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['class'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9427b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 클렌징 함수\n",
    "def clean_text(text):\n",
    "    # 불필요한 특수 문자, 숫자 제거 (한글, 영문, 공백 제외)\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "train['conversation'] = train['conversation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "fcf53474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이: 839\n",
      "평균 길이: 214.31645569620252\n"
     ]
    }
   ],
   "source": [
    "max_length = train['conversation'].apply(len).max()\n",
    "mean_length = train['conversation'].apply(len).mean()\n",
    "print(\"최대 길이:\", max_length)\n",
    "print(\"평균 길이:\", mean_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7223f559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['length'] = train['conversation'].apply(len)\n",
    "\n",
    "# 히스토그램 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(train['length'], bins=30, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Conversation Lengths')\n",
    "plt.xlabel('Length of Conversations (Number of Tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2fe701fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발...</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>나 이틀뒤에 가나다 음식점 예약좀 해줘 저녁시로 가나다 음식점이요 응 남자친구 부모...</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>경비아저씨 내차에 경고장 붙였어요 내가 여기 몇년째 사는데 아직도 내차인줄 모르고 ...</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>네네 무슨 일 때문에 전화주셨나요 우리 애가 지우개 하나 훔친거 가지고 애들 앞에서...</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>너가 민수라는 남자 제자니 네 맞는데요 왜 그러시죠 혹시 저희 선생님께 무슨 일 생...</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3922</th>\n",
       "      <td>3922</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>잘 생각해봐 이건 윈윈이야 당신은 가정 지키고 난 돈 벌 수 있고 이봐 이렇게 까지...</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3934</th>\n",
       "      <td>3934</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>야 돈 좀 내놔 싫어 내가 돈이 어딨어 죽고싶냐 빨리 내놔너 엄마한테 용돈 만원 받...</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3938</th>\n",
       "      <td>3938</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>한국지검검사 윤동철 입니다 지금 박세미님 계좌가 범죄에 연루되어 있으니 빨리 저한테...</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3941</th>\n",
       "      <td>3941</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>동생아 내가 정말 급해서 그러는데 백만원만 빌려줄수있니 또 무슨일인데 그래 지난번에...</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3943</th>\n",
       "      <td>3943</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>인테리어 하는데 예상보다 돈이 만원 더 들어서 그 돈 주셔야 해요 이요 마감도 엉성...</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>564 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx        class                                       conversation  \\\n",
       "4        4        갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요 저희 회사에서 이 선크림 파는데 한 번 손등에 발...   \n",
       "5        5  직장 내 괴롭힘 대화  나 이틀뒤에 가나다 음식점 예약좀 해줘 저녁시로 가나다 음식점이요 응 남자친구 부모...   \n",
       "7        7    기타 괴롭힘 대화  경비아저씨 내차에 경고장 붙였어요 내가 여기 몇년째 사는데 아직도 내차인줄 모르고 ...   \n",
       "12      12        협박 대화  네네 무슨 일 때문에 전화주셨나요 우리 애가 지우개 하나 훔친거 가지고 애들 앞에서...   \n",
       "20      20        협박 대화  너가 민수라는 남자 제자니 네 맞는데요 왜 그러시죠 혹시 저희 선생님께 무슨 일 생...   \n",
       "...    ...          ...                                                ...   \n",
       "3922  3922        갈취 대화  잘 생각해봐 이건 윈윈이야 당신은 가정 지키고 난 돈 벌 수 있고 이봐 이렇게 까지...   \n",
       "3934  3934        갈취 대화  야 돈 좀 내놔 싫어 내가 돈이 어딨어 죽고싶냐 빨리 내놔너 엄마한테 용돈 만원 받...   \n",
       "3938  3938        협박 대화  한국지검검사 윤동철 입니다 지금 박세미님 계좌가 범죄에 연루되어 있으니 빨리 저한테...   \n",
       "3941  3941        갈취 대화  동생아 내가 정말 급해서 그러는데 백만원만 빌려줄수있니 또 무슨일인데 그래 지난번에...   \n",
       "3943  3943        갈취 대화  인테리어 하는데 예상보다 돈이 만원 더 들어서 그 돈 주셔야 해요 이요 마감도 엉성...   \n",
       "\n",
       "      length  \n",
       "4        438  \n",
       "5        317  \n",
       "7        352  \n",
       "12       387  \n",
       "20       304  \n",
       "...      ...  \n",
       "3922     322  \n",
       "3934     313  \n",
       "3938     505  \n",
       "3941     317  \n",
       "3943     562  \n",
       "\n",
       "[564 rows x 4 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_over_text = train[train['length'] > 300]\n",
    "df_over_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "fe9e4f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "454037a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/.cache/kobert_v1.zip\n",
      "using cached model. /aiffel/aiffel/dktc/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#bert 모델, vocab 불러오기\n",
    "bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8992dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['class'] == \"협박 대화\"), 'label'] = 0  # 협박 대화 => 0\n",
    "train.loc[(train['class'] == \"갈취 대화\"), 'label'] = 1  # 갈취 대화 => 1\n",
    "train.loc[(train['class'] == \"직장 내 괴롭힘 대화\"), 'label'] = 2  # 직장 내 괴롭힘 대화 => 2\n",
    "train.loc[(train['class'] == \"기타 괴롭힘 대화\"), 'label'] = 3  # 기타 괴롭힘 대화 => 3\n",
    "train.loc[(train['class'] == \"일반 대화\"), 'label'] = 4  # 일반 대화 => 4\n",
    "\n",
    "data_list = []\n",
    "for content, label in zip(train['conversation'], train['label'])  :\n",
    "    temp = []\n",
    "    temp.append(content)\n",
    "    temp.append(str(int(label)))\n",
    "\n",
    "    data_list.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a5aed890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.drop(columns = 'idx', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "82b9f7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['지금 너 스스로를 죽여달라고 애원하는 것인가  아닙니다 죄송합니다  죽을 거면 혼자 죽지 우리까지 사건에 휘말리게 해 진짜 죽여버리고 싶게  정말 잘못했습니다  너가 선택해 너가 죽을래 네 가족을 죽여줄까  죄송합니다 정말 잘못했습니다  너에게는 선택권이 없어 선택 못한다면 너와 네 가족까지 모조리 죽여버릴거야  선택 못하겠습니다 한번만 도와주세요  그냥 다 죽여버려야겠군 이의 없지  제발 도와주세요',\n",
       " '0']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "78362c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_val = train_test_split(data_list, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0c3d302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "        self.original_lengths = []  # 토큰화된 문장의 원래 길이를 저장할 리스트\n",
    "        self.sentences = []\n",
    "\n",
    "        # transform을 정의\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        for i in dataset:\n",
    "            # 먼저 토큰화를 수행하여 문장의 길이를 확인\n",
    "            tokenized_sentence = bert_tokenizer(i[sent_idx])  # 문자열로 토큰화\n",
    "            self.original_lengths.append(len(tokenized_sentence))  # 토큰화된 길이를 저장\n",
    "\n",
    "            # transform을 적용하여 max_len에 맞춰 자르기\n",
    "            self.sentences.append(transform([i[sent_idx]]))  # transform 후 저장\n",
    "\n",
    "        # 레이블 처리\n",
    "        if label_idx is not None:\n",
    "            self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.labels is not None:\n",
    "            return self.sentences[i] + (self.labels[i], )\n",
    "        else:\n",
    "            return self.sentences[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "2cf0ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 문장 1의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2의 토큰화된 길이: 94\n",
      "Train 데이터 문장 3의 토큰화된 길이: 148\n",
      "Train 데이터 문장 4의 토큰화된 길이: 73\n",
      "Train 데이터 문장 5의 토큰화된 길이: 94\n",
      "Train 데이터 문장 6의 토큰화된 길이: 86\n",
      "Train 데이터 문장 7의 토큰화된 길이: 154\n",
      "Train 데이터 문장 8의 토큰화된 길이: 111\n",
      "Train 데이터 문장 9의 토큰화된 길이: 216\n",
      "Train 데이터 문장 10의 토큰화된 길이: 111\n",
      "Train 데이터 문장 11의 토큰화된 길이: 97\n",
      "Train 데이터 문장 12의 토큰화된 길이: 150\n",
      "Train 데이터 문장 13의 토큰화된 길이: 26\n",
      "Train 데이터 문장 14의 토큰화된 길이: 140\n",
      "Train 데이터 문장 15의 토큰화된 길이: 170\n",
      "Train 데이터 문장 16의 토큰화된 길이: 214\n",
      "Train 데이터 문장 17의 토큰화된 길이: 152\n",
      "Train 데이터 문장 18의 토큰화된 길이: 159\n",
      "Train 데이터 문장 19의 토큰화된 길이: 77\n",
      "Train 데이터 문장 20의 토큰화된 길이: 73\n",
      "Train 데이터 문장 21의 토큰화된 길이: 115\n",
      "Train 데이터 문장 22의 토큰화된 길이: 130\n",
      "Train 데이터 문장 23의 토큰화된 길이: 123\n",
      "Train 데이터 문장 24의 토큰화된 길이: 76\n",
      "Train 데이터 문장 25의 토큰화된 길이: 133\n",
      "Train 데이터 문장 26의 토큰화된 길이: 113\n",
      "Train 데이터 문장 27의 토큰화된 길이: 161\n",
      "Train 데이터 문장 28의 토큰화된 길이: 74\n",
      "Train 데이터 문장 29의 토큰화된 길이: 113\n",
      "Train 데이터 문장 30의 토큰화된 길이: 80\n",
      "Train 데이터 문장 31의 토큰화된 길이: 78\n",
      "Train 데이터 문장 32의 토큰화된 길이: 126\n",
      "Train 데이터 문장 33의 토큰화된 길이: 134\n",
      "Train 데이터 문장 34의 토큰화된 길이: 109\n",
      "Train 데이터 문장 35의 토큰화된 길이: 144\n",
      "Train 데이터 문장 36의 토큰화된 길이: 67\n",
      "Train 데이터 문장 37의 토큰화된 길이: 142\n",
      "Train 데이터 문장 38의 토큰화된 길이: 167\n",
      "Train 데이터 문장 39의 토큰화된 길이: 96\n",
      "Train 데이터 문장 40의 토큰화된 길이: 157\n",
      "Train 데이터 문장 41의 토큰화된 길이: 134\n",
      "Train 데이터 문장 42의 토큰화된 길이: 75\n",
      "Train 데이터 문장 43의 토큰화된 길이: 164\n",
      "Train 데이터 문장 44의 토큰화된 길이: 103\n",
      "Train 데이터 문장 45의 토큰화된 길이: 80\n",
      "Train 데이터 문장 46의 토큰화된 길이: 139\n",
      "Train 데이터 문장 47의 토큰화된 길이: 104\n",
      "Train 데이터 문장 48의 토큰화된 길이: 71\n",
      "Train 데이터 문장 49의 토큰화된 길이: 95\n",
      "Train 데이터 문장 50의 토큰화된 길이: 122\n",
      "Train 데이터 문장 51의 토큰화된 길이: 60\n",
      "Train 데이터 문장 52의 토큰화된 길이: 70\n",
      "Train 데이터 문장 53의 토큰화된 길이: 221\n",
      "Train 데이터 문장 54의 토큰화된 길이: 148\n",
      "Train 데이터 문장 55의 토큰화된 길이: 109\n",
      "Train 데이터 문장 56의 토큰화된 길이: 114\n",
      "Train 데이터 문장 57의 토큰화된 길이: 95\n",
      "Train 데이터 문장 58의 토큰화된 길이: 107\n",
      "Train 데이터 문장 59의 토큰화된 길이: 79\n",
      "Train 데이터 문장 60의 토큰화된 길이: 77\n",
      "Train 데이터 문장 61의 토큰화된 길이: 101\n",
      "Train 데이터 문장 62의 토큰화된 길이: 191\n",
      "Train 데이터 문장 63의 토큰화된 길이: 91\n",
      "Train 데이터 문장 64의 토큰화된 길이: 110\n",
      "Train 데이터 문장 65의 토큰화된 길이: 98\n",
      "Train 데이터 문장 66의 토큰화된 길이: 94\n",
      "Train 데이터 문장 67의 토큰화된 길이: 175\n",
      "Train 데이터 문장 68의 토큰화된 길이: 141\n",
      "Train 데이터 문장 69의 토큰화된 길이: 139\n",
      "Train 데이터 문장 70의 토큰화된 길이: 145\n",
      "Train 데이터 문장 71의 토큰화된 길이: 94\n",
      "Train 데이터 문장 72의 토큰화된 길이: 93\n",
      "Train 데이터 문장 73의 토큰화된 길이: 206\n",
      "Train 데이터 문장 74의 토큰화된 길이: 92\n",
      "Train 데이터 문장 75의 토큰화된 길이: 54\n",
      "Train 데이터 문장 76의 토큰화된 길이: 86\n",
      "Train 데이터 문장 77의 토큰화된 길이: 168\n",
      "Train 데이터 문장 78의 토큰화된 길이: 191\n",
      "Train 데이터 문장 79의 토큰화된 길이: 140\n",
      "Train 데이터 문장 80의 토큰화된 길이: 120\n",
      "Train 데이터 문장 81의 토큰화된 길이: 172\n",
      "Train 데이터 문장 82의 토큰화된 길이: 127\n",
      "Train 데이터 문장 83의 토큰화된 길이: 194\n",
      "Train 데이터 문장 84의 토큰화된 길이: 110\n",
      "Train 데이터 문장 85의 토큰화된 길이: 115\n",
      "Train 데이터 문장 86의 토큰화된 길이: 106\n",
      "Train 데이터 문장 87의 토큰화된 길이: 111\n",
      "Train 데이터 문장 88의 토큰화된 길이: 205\n",
      "Train 데이터 문장 89의 토큰화된 길이: 97\n",
      "Train 데이터 문장 90의 토큰화된 길이: 85\n",
      "Train 데이터 문장 91의 토큰화된 길이: 48\n",
      "Train 데이터 문장 92의 토큰화된 길이: 74\n",
      "Train 데이터 문장 93의 토큰화된 길이: 68\n",
      "Train 데이터 문장 94의 토큰화된 길이: 87\n",
      "Train 데이터 문장 95의 토큰화된 길이: 125\n",
      "Train 데이터 문장 96의 토큰화된 길이: 53\n",
      "Train 데이터 문장 97의 토큰화된 길이: 110\n",
      "Train 데이터 문장 98의 토큰화된 길이: 144\n",
      "Train 데이터 문장 99의 토큰화된 길이: 128\n",
      "Train 데이터 문장 100의 토큰화된 길이: 100\n",
      "Train 데이터 문장 101의 토큰화된 길이: 171\n",
      "Train 데이터 문장 102의 토큰화된 길이: 153\n",
      "Train 데이터 문장 103의 토큰화된 길이: 73\n",
      "Train 데이터 문장 104의 토큰화된 길이: 93\n",
      "Train 데이터 문장 105의 토큰화된 길이: 86\n",
      "Train 데이터 문장 106의 토큰화된 길이: 114\n",
      "Train 데이터 문장 107의 토큰화된 길이: 83\n",
      "Train 데이터 문장 108의 토큰화된 길이: 192\n",
      "Train 데이터 문장 109의 토큰화된 길이: 58\n",
      "Train 데이터 문장 110의 토큰화된 길이: 87\n",
      "Train 데이터 문장 111의 토큰화된 길이: 122\n",
      "Train 데이터 문장 112의 토큰화된 길이: 144\n",
      "Train 데이터 문장 113의 토큰화된 길이: 104\n",
      "Train 데이터 문장 114의 토큰화된 길이: 84\n",
      "Train 데이터 문장 115의 토큰화된 길이: 176\n",
      "Train 데이터 문장 116의 토큰화된 길이: 86\n",
      "Train 데이터 문장 117의 토큰화된 길이: 123\n",
      "Train 데이터 문장 118의 토큰화된 길이: 125\n",
      "Train 데이터 문장 119의 토큰화된 길이: 73\n",
      "Train 데이터 문장 120의 토큰화된 길이: 221\n",
      "Train 데이터 문장 121의 토큰화된 길이: 190\n",
      "Train 데이터 문장 122의 토큰화된 길이: 145\n",
      "Train 데이터 문장 123의 토큰화된 길이: 139\n",
      "Train 데이터 문장 124의 토큰화된 길이: 69\n",
      "Train 데이터 문장 125의 토큰화된 길이: 93\n",
      "Train 데이터 문장 126의 토큰화된 길이: 120\n",
      "Train 데이터 문장 127의 토큰화된 길이: 94\n",
      "Train 데이터 문장 128의 토큰화된 길이: 131\n",
      "Train 데이터 문장 129의 토큰화된 길이: 76\n",
      "Train 데이터 문장 130의 토큰화된 길이: 81\n",
      "Train 데이터 문장 131의 토큰화된 길이: 76\n",
      "Train 데이터 문장 132의 토큰화된 길이: 121\n",
      "Train 데이터 문장 133의 토큰화된 길이: 176\n",
      "Train 데이터 문장 134의 토큰화된 길이: 188\n",
      "Train 데이터 문장 135의 토큰화된 길이: 142\n",
      "Train 데이터 문장 136의 토큰화된 길이: 93\n",
      "Train 데이터 문장 137의 토큰화된 길이: 133\n",
      "Train 데이터 문장 138의 토큰화된 길이: 206\n",
      "Train 데이터 문장 139의 토큰화된 길이: 61\n",
      "Train 데이터 문장 140의 토큰화된 길이: 107\n",
      "Train 데이터 문장 141의 토큰화된 길이: 78\n",
      "Train 데이터 문장 142의 토큰화된 길이: 74\n",
      "Train 데이터 문장 143의 토큰화된 길이: 82\n",
      "Train 데이터 문장 144의 토큰화된 길이: 68\n",
      "Train 데이터 문장 145의 토큰화된 길이: 162\n",
      "Train 데이터 문장 146의 토큰화된 길이: 139\n",
      "Train 데이터 문장 147의 토큰화된 길이: 208\n",
      "Train 데이터 문장 148의 토큰화된 길이: 159\n",
      "Train 데이터 문장 149의 토큰화된 길이: 146\n",
      "Train 데이터 문장 150의 토큰화된 길이: 64\n",
      "Train 데이터 문장 151의 토큰화된 길이: 236\n",
      "Train 데이터 문장 152의 토큰화된 길이: 207\n",
      "Train 데이터 문장 153의 토큰화된 길이: 71\n",
      "Train 데이터 문장 154의 토큰화된 길이: 42\n",
      "Train 데이터 문장 155의 토큰화된 길이: 68\n",
      "Train 데이터 문장 156의 토큰화된 길이: 66\n",
      "Train 데이터 문장 157의 토큰화된 길이: 135\n",
      "Train 데이터 문장 158의 토큰화된 길이: 64\n",
      "Train 데이터 문장 159의 토큰화된 길이: 125\n",
      "Train 데이터 문장 160의 토큰화된 길이: 93\n",
      "Train 데이터 문장 161의 토큰화된 길이: 131\n",
      "Train 데이터 문장 162의 토큰화된 길이: 65\n",
      "Train 데이터 문장 163의 토큰화된 길이: 114\n",
      "Train 데이터 문장 164의 토큰화된 길이: 243\n",
      "Train 데이터 문장 165의 토큰화된 길이: 75\n",
      "Train 데이터 문장 166의 토큰화된 길이: 369\n",
      "Train 데이터 문장 167의 토큰화된 길이: 160\n",
      "Train 데이터 문장 168의 토큰화된 길이: 57\n",
      "Train 데이터 문장 169의 토큰화된 길이: 177\n",
      "Train 데이터 문장 170의 토큰화된 길이: 146\n",
      "Train 데이터 문장 171의 토큰화된 길이: 142\n",
      "Train 데이터 문장 172의 토큰화된 길이: 244\n",
      "Train 데이터 문장 173의 토큰화된 길이: 75\n",
      "Train 데이터 문장 174의 토큰화된 길이: 69\n",
      "Train 데이터 문장 175의 토큰화된 길이: 74\n",
      "Train 데이터 문장 176의 토큰화된 길이: 112\n",
      "Train 데이터 문장 177의 토큰화된 길이: 166\n",
      "Train 데이터 문장 178의 토큰화된 길이: 111\n",
      "Train 데이터 문장 179의 토큰화된 길이: 116\n",
      "Train 데이터 문장 180의 토큰화된 길이: 123\n",
      "Train 데이터 문장 181의 토큰화된 길이: 69\n",
      "Train 데이터 문장 182의 토큰화된 길이: 197\n",
      "Train 데이터 문장 183의 토큰화된 길이: 105\n",
      "Train 데이터 문장 184의 토큰화된 길이: 158\n",
      "Train 데이터 문장 185의 토큰화된 길이: 179\n",
      "Train 데이터 문장 186의 토큰화된 길이: 115\n",
      "Train 데이터 문장 187의 토큰화된 길이: 78\n",
      "Train 데이터 문장 188의 토큰화된 길이: 201\n",
      "Train 데이터 문장 189의 토큰화된 길이: 83\n",
      "Train 데이터 문장 190의 토큰화된 길이: 183\n",
      "Train 데이터 문장 191의 토큰화된 길이: 133\n",
      "Train 데이터 문장 192의 토큰화된 길이: 144\n",
      "Train 데이터 문장 193의 토큰화된 길이: 140\n",
      "Train 데이터 문장 194의 토큰화된 길이: 146\n",
      "Train 데이터 문장 195의 토큰화된 길이: 131\n",
      "Train 데이터 문장 196의 토큰화된 길이: 145\n",
      "Train 데이터 문장 197의 토큰화된 길이: 164\n",
      "Train 데이터 문장 198의 토큰화된 길이: 118\n",
      "Train 데이터 문장 199의 토큰화된 길이: 101\n",
      "Train 데이터 문장 200의 토큰화된 길이: 93\n",
      "Train 데이터 문장 201의 토큰화된 길이: 72\n",
      "Train 데이터 문장 202의 토큰화된 길이: 55\n",
      "Train 데이터 문장 203의 토큰화된 길이: 134\n",
      "Train 데이터 문장 204의 토큰화된 길이: 67\n",
      "Train 데이터 문장 205의 토큰화된 길이: 168\n",
      "Train 데이터 문장 206의 토큰화된 길이: 89\n",
      "Train 데이터 문장 207의 토큰화된 길이: 255\n",
      "Train 데이터 문장 208의 토큰화된 길이: 101\n",
      "Train 데이터 문장 209의 토큰화된 길이: 92\n",
      "Train 데이터 문장 210의 토큰화된 길이: 140\n",
      "Train 데이터 문장 211의 토큰화된 길이: 81\n",
      "Train 데이터 문장 212의 토큰화된 길이: 82\n",
      "Train 데이터 문장 213의 토큰화된 길이: 129\n",
      "Train 데이터 문장 214의 토큰화된 길이: 116\n",
      "Train 데이터 문장 215의 토큰화된 길이: 143\n",
      "Train 데이터 문장 216의 토큰화된 길이: 88\n",
      "Train 데이터 문장 217의 토큰화된 길이: 77\n",
      "Train 데이터 문장 218의 토큰화된 길이: 126\n",
      "Train 데이터 문장 219의 토큰화된 길이: 137\n",
      "Train 데이터 문장 220의 토큰화된 길이: 136\n",
      "Train 데이터 문장 221의 토큰화된 길이: 63\n",
      "Train 데이터 문장 222의 토큰화된 길이: 88\n",
      "Train 데이터 문장 223의 토큰화된 길이: 151\n",
      "Train 데이터 문장 224의 토큰화된 길이: 99\n",
      "Train 데이터 문장 225의 토큰화된 길이: 93\n",
      "Train 데이터 문장 226의 토큰화된 길이: 50\n",
      "Train 데이터 문장 227의 토큰화된 길이: 145\n",
      "Train 데이터 문장 228의 토큰화된 길이: 46\n",
      "Train 데이터 문장 229의 토큰화된 길이: 82\n",
      "Train 데이터 문장 230의 토큰화된 길이: 116\n",
      "Train 데이터 문장 231의 토큰화된 길이: 89\n",
      "Train 데이터 문장 232의 토큰화된 길이: 114\n",
      "Train 데이터 문장 233의 토큰화된 길이: 75\n",
      "Train 데이터 문장 234의 토큰화된 길이: 111\n",
      "Train 데이터 문장 235의 토큰화된 길이: 268\n",
      "Train 데이터 문장 236의 토큰화된 길이: 195\n",
      "Train 데이터 문장 237의 토큰화된 길이: 127\n",
      "Train 데이터 문장 238의 토큰화된 길이: 121\n",
      "Train 데이터 문장 239의 토큰화된 길이: 120\n",
      "Train 데이터 문장 240의 토큰화된 길이: 153\n",
      "Train 데이터 문장 241의 토큰화된 길이: 61\n",
      "Train 데이터 문장 242의 토큰화된 길이: 77\n",
      "Train 데이터 문장 243의 토큰화된 길이: 88\n",
      "Train 데이터 문장 244의 토큰화된 길이: 57\n",
      "Train 데이터 문장 245의 토큰화된 길이: 103\n",
      "Train 데이터 문장 246의 토큰화된 길이: 119\n",
      "Train 데이터 문장 247의 토큰화된 길이: 184\n",
      "Train 데이터 문장 248의 토큰화된 길이: 178\n",
      "Train 데이터 문장 249의 토큰화된 길이: 153\n",
      "Train 데이터 문장 250의 토큰화된 길이: 106\n",
      "Train 데이터 문장 251의 토큰화된 길이: 93\n",
      "Train 데이터 문장 252의 토큰화된 길이: 98\n",
      "Train 데이터 문장 253의 토큰화된 길이: 167\n",
      "Train 데이터 문장 254의 토큰화된 길이: 60\n",
      "Train 데이터 문장 255의 토큰화된 길이: 111\n",
      "Train 데이터 문장 256의 토큰화된 길이: 129\n",
      "Train 데이터 문장 257의 토큰화된 길이: 107\n",
      "Train 데이터 문장 258의 토큰화된 길이: 124\n",
      "Train 데이터 문장 259의 토큰화된 길이: 183\n",
      "Train 데이터 문장 260의 토큰화된 길이: 163\n",
      "Train 데이터 문장 261의 토큰화된 길이: 115\n",
      "Train 데이터 문장 262의 토큰화된 길이: 99\n",
      "Train 데이터 문장 263의 토큰화된 길이: 89\n",
      "Train 데이터 문장 264의 토큰화된 길이: 158\n",
      "Train 데이터 문장 265의 토큰화된 길이: 134\n",
      "Train 데이터 문장 266의 토큰화된 길이: 179\n",
      "Train 데이터 문장 267의 토큰화된 길이: 88\n",
      "Train 데이터 문장 268의 토큰화된 길이: 83\n",
      "Train 데이터 문장 269의 토큰화된 길이: 124\n",
      "Train 데이터 문장 270의 토큰화된 길이: 129\n",
      "Train 데이터 문장 271의 토큰화된 길이: 92\n",
      "Train 데이터 문장 272의 토큰화된 길이: 121\n",
      "Train 데이터 문장 273의 토큰화된 길이: 140\n",
      "Train 데이터 문장 274의 토큰화된 길이: 165\n",
      "Train 데이터 문장 275의 토큰화된 길이: 120\n",
      "Train 데이터 문장 276의 토큰화된 길이: 87\n",
      "Train 데이터 문장 277의 토큰화된 길이: 150\n",
      "Train 데이터 문장 278의 토큰화된 길이: 127\n",
      "Train 데이터 문장 279의 토큰화된 길이: 93\n",
      "Train 데이터 문장 280의 토큰화된 길이: 82\n",
      "Train 데이터 문장 281의 토큰화된 길이: 126\n",
      "Train 데이터 문장 282의 토큰화된 길이: 114\n",
      "Train 데이터 문장 283의 토큰화된 길이: 126\n",
      "Train 데이터 문장 284의 토큰화된 길이: 72\n",
      "Train 데이터 문장 285의 토큰화된 길이: 165\n",
      "Train 데이터 문장 286의 토큰화된 길이: 47\n",
      "Train 데이터 문장 287의 토큰화된 길이: 89\n",
      "Train 데이터 문장 288의 토큰화된 길이: 132\n",
      "Train 데이터 문장 289의 토큰화된 길이: 109\n",
      "Train 데이터 문장 290의 토큰화된 길이: 197\n",
      "Train 데이터 문장 291의 토큰화된 길이: 278\n",
      "Train 데이터 문장 292의 토큰화된 길이: 220\n",
      "Train 데이터 문장 293의 토큰화된 길이: 88\n",
      "Train 데이터 문장 294의 토큰화된 길이: 204\n",
      "Train 데이터 문장 295의 토큰화된 길이: 54\n",
      "Train 데이터 문장 296의 토큰화된 길이: 118\n",
      "Train 데이터 문장 297의 토큰화된 길이: 158\n",
      "Train 데이터 문장 298의 토큰화된 길이: 70\n",
      "Train 데이터 문장 299의 토큰화된 길이: 215\n",
      "Train 데이터 문장 300의 토큰화된 길이: 41\n",
      "Train 데이터 문장 301의 토큰화된 길이: 128\n",
      "Train 데이터 문장 302의 토큰화된 길이: 119\n",
      "Train 데이터 문장 303의 토큰화된 길이: 82\n",
      "Train 데이터 문장 304의 토큰화된 길이: 71\n",
      "Train 데이터 문장 305의 토큰화된 길이: 76\n",
      "Train 데이터 문장 306의 토큰화된 길이: 82\n",
      "Train 데이터 문장 307의 토큰화된 길이: 73\n",
      "Train 데이터 문장 308의 토큰화된 길이: 122\n",
      "Train 데이터 문장 309의 토큰화된 길이: 68\n",
      "Train 데이터 문장 310의 토큰화된 길이: 120\n",
      "Train 데이터 문장 311의 토큰화된 길이: 118\n",
      "Train 데이터 문장 312의 토큰화된 길이: 75\n",
      "Train 데이터 문장 313의 토큰화된 길이: 82\n",
      "Train 데이터 문장 314의 토큰화된 길이: 188\n",
      "Train 데이터 문장 315의 토큰화된 길이: 183\n",
      "Train 데이터 문장 316의 토큰화된 길이: 134\n",
      "Train 데이터 문장 317의 토큰화된 길이: 376\n",
      "Train 데이터 문장 318의 토큰화된 길이: 100\n",
      "Train 데이터 문장 319의 토큰화된 길이: 195\n",
      "Train 데이터 문장 320의 토큰화된 길이: 146\n",
      "Train 데이터 문장 321의 토큰화된 길이: 176\n",
      "Train 데이터 문장 322의 토큰화된 길이: 149\n",
      "Train 데이터 문장 323의 토큰화된 길이: 86\n",
      "Train 데이터 문장 324의 토큰화된 길이: 121\n",
      "Train 데이터 문장 325의 토큰화된 길이: 69\n",
      "Train 데이터 문장 326의 토큰화된 길이: 148\n",
      "Train 데이터 문장 327의 토큰화된 길이: 43\n",
      "Train 데이터 문장 328의 토큰화된 길이: 93\n",
      "Train 데이터 문장 329의 토큰화된 길이: 159\n",
      "Train 데이터 문장 330의 토큰화된 길이: 118\n",
      "Train 데이터 문장 331의 토큰화된 길이: 105\n",
      "Train 데이터 문장 332의 토큰화된 길이: 63\n",
      "Train 데이터 문장 333의 토큰화된 길이: 107\n",
      "Train 데이터 문장 334의 토큰화된 길이: 89\n",
      "Train 데이터 문장 335의 토큰화된 길이: 56\n",
      "Train 데이터 문장 336의 토큰화된 길이: 234\n",
      "Train 데이터 문장 337의 토큰화된 길이: 146\n",
      "Train 데이터 문장 338의 토큰화된 길이: 145\n",
      "Train 데이터 문장 339의 토큰화된 길이: 88\n",
      "Train 데이터 문장 340의 토큰화된 길이: 182\n",
      "Train 데이터 문장 341의 토큰화된 길이: 84\n",
      "Train 데이터 문장 342의 토큰화된 길이: 111\n",
      "Train 데이터 문장 343의 토큰화된 길이: 208\n",
      "Train 데이터 문장 344의 토큰화된 길이: 152\n",
      "Train 데이터 문장 345의 토큰화된 길이: 133\n",
      "Train 데이터 문장 346의 토큰화된 길이: 119\n",
      "Train 데이터 문장 347의 토큰화된 길이: 137\n",
      "Train 데이터 문장 348의 토큰화된 길이: 97\n",
      "Train 데이터 문장 349의 토큰화된 길이: 156\n",
      "Train 데이터 문장 350의 토큰화된 길이: 132\n",
      "Train 데이터 문장 351의 토큰화된 길이: 75\n",
      "Train 데이터 문장 352의 토큰화된 길이: 108\n",
      "Train 데이터 문장 353의 토큰화된 길이: 112\n",
      "Train 데이터 문장 354의 토큰화된 길이: 139\n",
      "Train 데이터 문장 355의 토큰화된 길이: 107\n",
      "Train 데이터 문장 356의 토큰화된 길이: 76\n",
      "Train 데이터 문장 357의 토큰화된 길이: 81\n",
      "Train 데이터 문장 358의 토큰화된 길이: 126\n",
      "Train 데이터 문장 359의 토큰화된 길이: 190\n",
      "Train 데이터 문장 360의 토큰화된 길이: 97\n",
      "Train 데이터 문장 361의 토큰화된 길이: 102\n",
      "Train 데이터 문장 362의 토큰화된 길이: 227\n",
      "Train 데이터 문장 363의 토큰화된 길이: 100\n",
      "Train 데이터 문장 364의 토큰화된 길이: 124\n",
      "Train 데이터 문장 365의 토큰화된 길이: 139\n",
      "Train 데이터 문장 366의 토큰화된 길이: 94\n",
      "Train 데이터 문장 367의 토큰화된 길이: 115\n",
      "Train 데이터 문장 368의 토큰화된 길이: 116\n",
      "Train 데이터 문장 369의 토큰화된 길이: 104\n",
      "Train 데이터 문장 370의 토큰화된 길이: 96\n",
      "Train 데이터 문장 371의 토큰화된 길이: 89\n",
      "Train 데이터 문장 372의 토큰화된 길이: 110\n",
      "Train 데이터 문장 373의 토큰화된 길이: 168\n",
      "Train 데이터 문장 374의 토큰화된 길이: 213\n",
      "Train 데이터 문장 375의 토큰화된 길이: 321\n",
      "Train 데이터 문장 376의 토큰화된 길이: 66\n",
      "Train 데이터 문장 377의 토큰화된 길이: 116\n",
      "Train 데이터 문장 378의 토큰화된 길이: 326\n",
      "Train 데이터 문장 379의 토큰화된 길이: 116\n",
      "Train 데이터 문장 380의 토큰화된 길이: 165\n",
      "Train 데이터 문장 381의 토큰화된 길이: 130\n",
      "Train 데이터 문장 382의 토큰화된 길이: 127\n",
      "Train 데이터 문장 383의 토큰화된 길이: 187\n",
      "Train 데이터 문장 384의 토큰화된 길이: 101\n",
      "Train 데이터 문장 385의 토큰화된 길이: 107\n",
      "Train 데이터 문장 386의 토큰화된 길이: 83\n",
      "Train 데이터 문장 387의 토큰화된 길이: 171\n",
      "Train 데이터 문장 388의 토큰화된 길이: 138\n",
      "Train 데이터 문장 389의 토큰화된 길이: 109\n",
      "Train 데이터 문장 390의 토큰화된 길이: 58\n",
      "Train 데이터 문장 391의 토큰화된 길이: 152\n",
      "Train 데이터 문장 392의 토큰화된 길이: 214\n",
      "Train 데이터 문장 393의 토큰화된 길이: 63\n",
      "Train 데이터 문장 394의 토큰화된 길이: 127\n",
      "Train 데이터 문장 395의 토큰화된 길이: 194\n",
      "Train 데이터 문장 396의 토큰화된 길이: 69\n",
      "Train 데이터 문장 397의 토큰화된 길이: 104\n",
      "Train 데이터 문장 398의 토큰화된 길이: 91\n",
      "Train 데이터 문장 399의 토큰화된 길이: 46\n",
      "Train 데이터 문장 400의 토큰화된 길이: 132\n",
      "Train 데이터 문장 401의 토큰화된 길이: 94\n",
      "Train 데이터 문장 402의 토큰화된 길이: 173\n",
      "Train 데이터 문장 403의 토큰화된 길이: 174\n",
      "Train 데이터 문장 404의 토큰화된 길이: 240\n",
      "Train 데이터 문장 405의 토큰화된 길이: 44\n",
      "Train 데이터 문장 406의 토큰화된 길이: 146\n",
      "Train 데이터 문장 407의 토큰화된 길이: 85\n",
      "Train 데이터 문장 408의 토큰화된 길이: 106\n",
      "Train 데이터 문장 409의 토큰화된 길이: 79\n",
      "Train 데이터 문장 410의 토큰화된 길이: 51\n",
      "Train 데이터 문장 411의 토큰화된 길이: 92\n",
      "Train 데이터 문장 412의 토큰화된 길이: 53\n",
      "Train 데이터 문장 413의 토큰화된 길이: 89\n",
      "Train 데이터 문장 414의 토큰화된 길이: 153\n",
      "Train 데이터 문장 415의 토큰화된 길이: 142\n",
      "Train 데이터 문장 416의 토큰화된 길이: 261\n",
      "Train 데이터 문장 417의 토큰화된 길이: 185\n",
      "Train 데이터 문장 418의 토큰화된 길이: 290\n",
      "Train 데이터 문장 419의 토큰화된 길이: 99\n",
      "Train 데이터 문장 420의 토큰화된 길이: 104\n",
      "Train 데이터 문장 421의 토큰화된 길이: 54\n",
      "Train 데이터 문장 422의 토큰화된 길이: 95\n",
      "Train 데이터 문장 423의 토큰화된 길이: 76\n",
      "Train 데이터 문장 424의 토큰화된 길이: 194\n",
      "Train 데이터 문장 425의 토큰화된 길이: 157\n",
      "Train 데이터 문장 426의 토큰화된 길이: 67\n",
      "Train 데이터 문장 427의 토큰화된 길이: 156\n",
      "Train 데이터 문장 428의 토큰화된 길이: 105\n",
      "Train 데이터 문장 429의 토큰화된 길이: 140\n",
      "Train 데이터 문장 430의 토큰화된 길이: 93\n",
      "Train 데이터 문장 431의 토큰화된 길이: 160\n",
      "Train 데이터 문장 432의 토큰화된 길이: 96\n",
      "Train 데이터 문장 433의 토큰화된 길이: 50\n",
      "Train 데이터 문장 434의 토큰화된 길이: 165\n",
      "Train 데이터 문장 435의 토큰화된 길이: 136\n",
      "Train 데이터 문장 436의 토큰화된 길이: 165\n",
      "Train 데이터 문장 437의 토큰화된 길이: 135\n",
      "Train 데이터 문장 438의 토큰화된 길이: 116\n",
      "Train 데이터 문장 439의 토큰화된 길이: 104\n",
      "Train 데이터 문장 440의 토큰화된 길이: 124\n",
      "Train 데이터 문장 441의 토큰화된 길이: 64\n",
      "Train 데이터 문장 442의 토큰화된 길이: 93\n",
      "Train 데이터 문장 443의 토큰화된 길이: 74\n",
      "Train 데이터 문장 444의 토큰화된 길이: 107\n",
      "Train 데이터 문장 445의 토큰화된 길이: 231\n",
      "Train 데이터 문장 446의 토큰화된 길이: 188\n",
      "Train 데이터 문장 447의 토큰화된 길이: 108\n",
      "Train 데이터 문장 448의 토큰화된 길이: 81\n",
      "Train 데이터 문장 449의 토큰화된 길이: 181\n",
      "Train 데이터 문장 450의 토큰화된 길이: 92\n",
      "Train 데이터 문장 451의 토큰화된 길이: 211\n",
      "Train 데이터 문장 452의 토큰화된 길이: 83\n",
      "Train 데이터 문장 453의 토큰화된 길이: 71\n",
      "Train 데이터 문장 454의 토큰화된 길이: 44\n",
      "Train 데이터 문장 455의 토큰화된 길이: 92\n",
      "Train 데이터 문장 456의 토큰화된 길이: 122\n",
      "Train 데이터 문장 457의 토큰화된 길이: 148\n",
      "Train 데이터 문장 458의 토큰화된 길이: 142\n",
      "Train 데이터 문장 459의 토큰화된 길이: 95\n",
      "Train 데이터 문장 460의 토큰화된 길이: 164\n",
      "Train 데이터 문장 461의 토큰화된 길이: 82\n",
      "Train 데이터 문장 462의 토큰화된 길이: 198\n",
      "Train 데이터 문장 463의 토큰화된 길이: 198\n",
      "Train 데이터 문장 464의 토큰화된 길이: 145\n",
      "Train 데이터 문장 465의 토큰화된 길이: 89\n",
      "Train 데이터 문장 466의 토큰화된 길이: 210\n",
      "Train 데이터 문장 467의 토큰화된 길이: 96\n",
      "Train 데이터 문장 468의 토큰화된 길이: 104\n",
      "Train 데이터 문장 469의 토큰화된 길이: 164\n",
      "Train 데이터 문장 470의 토큰화된 길이: 88\n",
      "Train 데이터 문장 471의 토큰화된 길이: 106\n",
      "Train 데이터 문장 472의 토큰화된 길이: 287\n",
      "Train 데이터 문장 473의 토큰화된 길이: 102\n",
      "Train 데이터 문장 474의 토큰화된 길이: 177\n",
      "Train 데이터 문장 475의 토큰화된 길이: 84\n",
      "Train 데이터 문장 476의 토큰화된 길이: 190\n",
      "Train 데이터 문장 477의 토큰화된 길이: 110\n",
      "Train 데이터 문장 478의 토큰화된 길이: 139\n",
      "Train 데이터 문장 479의 토큰화된 길이: 126\n",
      "Train 데이터 문장 480의 토큰화된 길이: 116\n",
      "Train 데이터 문장 481의 토큰화된 길이: 104\n",
      "Train 데이터 문장 482의 토큰화된 길이: 190\n",
      "Train 데이터 문장 483의 토큰화된 길이: 117\n",
      "Train 데이터 문장 484의 토큰화된 길이: 57\n",
      "Train 데이터 문장 485의 토큰화된 길이: 158\n",
      "Train 데이터 문장 486의 토큰화된 길이: 98\n",
      "Train 데이터 문장 487의 토큰화된 길이: 145\n",
      "Train 데이터 문장 488의 토큰화된 길이: 99\n",
      "Train 데이터 문장 489의 토큰화된 길이: 93\n",
      "Train 데이터 문장 490의 토큰화된 길이: 76\n",
      "Train 데이터 문장 491의 토큰화된 길이: 167\n",
      "Train 데이터 문장 492의 토큰화된 길이: 133\n",
      "Train 데이터 문장 493의 토큰화된 길이: 306\n",
      "Train 데이터 문장 494의 토큰화된 길이: 129\n",
      "Train 데이터 문장 495의 토큰화된 길이: 88\n",
      "Train 데이터 문장 496의 토큰화된 길이: 73\n",
      "Train 데이터 문장 497의 토큰화된 길이: 170\n",
      "Train 데이터 문장 498의 토큰화된 길이: 161\n",
      "Train 데이터 문장 499의 토큰화된 길이: 128\n",
      "Train 데이터 문장 500의 토큰화된 길이: 93\n",
      "Train 데이터 문장 501의 토큰화된 길이: 41\n",
      "Train 데이터 문장 502의 토큰화된 길이: 93\n",
      "Train 데이터 문장 503의 토큰화된 길이: 201\n",
      "Train 데이터 문장 504의 토큰화된 길이: 138\n",
      "Train 데이터 문장 505의 토큰화된 길이: 147\n",
      "Train 데이터 문장 506의 토큰화된 길이: 93\n",
      "Train 데이터 문장 507의 토큰화된 길이: 57\n",
      "Train 데이터 문장 508의 토큰화된 길이: 132\n",
      "Train 데이터 문장 509의 토큰화된 길이: 86\n",
      "Train 데이터 문장 510의 토큰화된 길이: 102\n",
      "Train 데이터 문장 511의 토큰화된 길이: 112\n",
      "Train 데이터 문장 512의 토큰화된 길이: 176\n",
      "Train 데이터 문장 513의 토큰화된 길이: 84\n",
      "Train 데이터 문장 514의 토큰화된 길이: 185\n",
      "Train 데이터 문장 515의 토큰화된 길이: 103\n",
      "Train 데이터 문장 516의 토큰화된 길이: 71\n",
      "Train 데이터 문장 517의 토큰화된 길이: 121\n",
      "Train 데이터 문장 518의 토큰화된 길이: 165\n",
      "Train 데이터 문장 519의 토큰화된 길이: 105\n",
      "Train 데이터 문장 520의 토큰화된 길이: 209\n",
      "Train 데이터 문장 521의 토큰화된 길이: 156\n",
      "Train 데이터 문장 522의 토큰화된 길이: 140\n",
      "Train 데이터 문장 523의 토큰화된 길이: 95\n",
      "Train 데이터 문장 524의 토큰화된 길이: 117\n",
      "Train 데이터 문장 525의 토큰화된 길이: 175\n",
      "Train 데이터 문장 526의 토큰화된 길이: 234\n",
      "Train 데이터 문장 527의 토큰화된 길이: 88\n",
      "Train 데이터 문장 528의 토큰화된 길이: 81\n",
      "Train 데이터 문장 529의 토큰화된 길이: 56\n",
      "Train 데이터 문장 530의 토큰화된 길이: 128\n",
      "Train 데이터 문장 531의 토큰화된 길이: 65\n",
      "Train 데이터 문장 532의 토큰화된 길이: 69\n",
      "Train 데이터 문장 533의 토큰화된 길이: 71\n",
      "Train 데이터 문장 534의 토큰화된 길이: 130\n",
      "Train 데이터 문장 535의 토큰화된 길이: 162\n",
      "Train 데이터 문장 536의 토큰화된 길이: 71\n",
      "Train 데이터 문장 537의 토큰화된 길이: 48\n",
      "Train 데이터 문장 538의 토큰화된 길이: 263\n",
      "Train 데이터 문장 539의 토큰화된 길이: 179\n",
      "Train 데이터 문장 540의 토큰화된 길이: 136\n",
      "Train 데이터 문장 541의 토큰화된 길이: 97\n",
      "Train 데이터 문장 542의 토큰화된 길이: 117\n",
      "Train 데이터 문장 543의 토큰화된 길이: 137\n",
      "Train 데이터 문장 544의 토큰화된 길이: 88\n",
      "Train 데이터 문장 545의 토큰화된 길이: 67\n",
      "Train 데이터 문장 546의 토큰화된 길이: 78\n",
      "Train 데이터 문장 547의 토큰화된 길이: 98\n",
      "Train 데이터 문장 548의 토큰화된 길이: 70\n",
      "Train 데이터 문장 549의 토큰화된 길이: 128\n",
      "Train 데이터 문장 550의 토큰화된 길이: 95\n",
      "Train 데이터 문장 551의 토큰화된 길이: 75\n",
      "Train 데이터 문장 552의 토큰화된 길이: 86\n",
      "Train 데이터 문장 553의 토큰화된 길이: 187\n",
      "Train 데이터 문장 554의 토큰화된 길이: 144\n",
      "Train 데이터 문장 555의 토큰화된 길이: 100\n",
      "Train 데이터 문장 556의 토큰화된 길이: 56\n",
      "Train 데이터 문장 557의 토큰화된 길이: 158\n",
      "Train 데이터 문장 558의 토큰화된 길이: 114\n",
      "Train 데이터 문장 559의 토큰화된 길이: 76\n",
      "Train 데이터 문장 560의 토큰화된 길이: 70\n",
      "Train 데이터 문장 561의 토큰화된 길이: 74\n",
      "Train 데이터 문장 562의 토큰화된 길이: 102\n",
      "Train 데이터 문장 563의 토큰화된 길이: 61\n",
      "Train 데이터 문장 564의 토큰화된 길이: 116\n",
      "Train 데이터 문장 565의 토큰화된 길이: 166\n",
      "Train 데이터 문장 566의 토큰화된 길이: 132\n",
      "Train 데이터 문장 567의 토큰화된 길이: 118\n",
      "Train 데이터 문장 568의 토큰화된 길이: 114\n",
      "Train 데이터 문장 569의 토큰화된 길이: 223\n",
      "Train 데이터 문장 570의 토큰화된 길이: 97\n",
      "Train 데이터 문장 571의 토큰화된 길이: 122\n",
      "Train 데이터 문장 572의 토큰화된 길이: 73\n",
      "Train 데이터 문장 573의 토큰화된 길이: 70\n",
      "Train 데이터 문장 574의 토큰화된 길이: 77\n",
      "Train 데이터 문장 575의 토큰화된 길이: 175\n",
      "Train 데이터 문장 576의 토큰화된 길이: 94\n",
      "Train 데이터 문장 577의 토큰화된 길이: 184\n",
      "Train 데이터 문장 578의 토큰화된 길이: 152\n",
      "Train 데이터 문장 579의 토큰화된 길이: 82\n",
      "Train 데이터 문장 580의 토큰화된 길이: 181\n",
      "Train 데이터 문장 581의 토큰화된 길이: 82\n",
      "Train 데이터 문장 582의 토큰화된 길이: 236\n",
      "Train 데이터 문장 583의 토큰화된 길이: 185\n",
      "Train 데이터 문장 584의 토큰화된 길이: 184\n",
      "Train 데이터 문장 585의 토큰화된 길이: 52\n",
      "Train 데이터 문장 586의 토큰화된 길이: 188\n",
      "Train 데이터 문장 587의 토큰화된 길이: 75\n",
      "Train 데이터 문장 588의 토큰화된 길이: 143\n",
      "Train 데이터 문장 589의 토큰화된 길이: 127\n",
      "Train 데이터 문장 590의 토큰화된 길이: 89\n",
      "Train 데이터 문장 591의 토큰화된 길이: 94\n",
      "Train 데이터 문장 592의 토큰화된 길이: 108\n",
      "Train 데이터 문장 593의 토큰화된 길이: 75\n",
      "Train 데이터 문장 594의 토큰화된 길이: 145\n",
      "Train 데이터 문장 595의 토큰화된 길이: 103\n",
      "Train 데이터 문장 596의 토큰화된 길이: 116\n",
      "Train 데이터 문장 597의 토큰화된 길이: 142\n",
      "Train 데이터 문장 598의 토큰화된 길이: 96\n",
      "Train 데이터 문장 599의 토큰화된 길이: 58\n",
      "Train 데이터 문장 600의 토큰화된 길이: 153\n",
      "Train 데이터 문장 601의 토큰화된 길이: 182\n",
      "Train 데이터 문장 602의 토큰화된 길이: 59\n",
      "Train 데이터 문장 603의 토큰화된 길이: 105\n",
      "Train 데이터 문장 604의 토큰화된 길이: 117\n",
      "Train 데이터 문장 605의 토큰화된 길이: 145\n",
      "Train 데이터 문장 606의 토큰화된 길이: 127\n",
      "Train 데이터 문장 607의 토큰화된 길이: 89\n",
      "Train 데이터 문장 608의 토큰화된 길이: 213\n",
      "Train 데이터 문장 609의 토큰화된 길이: 178\n",
      "Train 데이터 문장 610의 토큰화된 길이: 77\n",
      "Train 데이터 문장 611의 토큰화된 길이: 66\n",
      "Train 데이터 문장 612의 토큰화된 길이: 171\n",
      "Train 데이터 문장 613의 토큰화된 길이: 162\n",
      "Train 데이터 문장 614의 토큰화된 길이: 97\n",
      "Train 데이터 문장 615의 토큰화된 길이: 121\n",
      "Train 데이터 문장 616의 토큰화된 길이: 60\n",
      "Train 데이터 문장 617의 토큰화된 길이: 110\n",
      "Train 데이터 문장 618의 토큰화된 길이: 189\n",
      "Train 데이터 문장 619의 토큰화된 길이: 124\n",
      "Train 데이터 문장 620의 토큰화된 길이: 180\n",
      "Train 데이터 문장 621의 토큰화된 길이: 101\n",
      "Train 데이터 문장 622의 토큰화된 길이: 100\n",
      "Train 데이터 문장 623의 토큰화된 길이: 118\n",
      "Train 데이터 문장 624의 토큰화된 길이: 110\n",
      "Train 데이터 문장 625의 토큰화된 길이: 115\n",
      "Train 데이터 문장 626의 토큰화된 길이: 136\n",
      "Train 데이터 문장 627의 토큰화된 길이: 103\n",
      "Train 데이터 문장 628의 토큰화된 길이: 75\n",
      "Train 데이터 문장 629의 토큰화된 길이: 125\n",
      "Train 데이터 문장 630의 토큰화된 길이: 86\n",
      "Train 데이터 문장 631의 토큰화된 길이: 98\n",
      "Train 데이터 문장 632의 토큰화된 길이: 96\n",
      "Train 데이터 문장 633의 토큰화된 길이: 213\n",
      "Train 데이터 문장 634의 토큰화된 길이: 191\n",
      "Train 데이터 문장 635의 토큰화된 길이: 222\n",
      "Train 데이터 문장 636의 토큰화된 길이: 78\n",
      "Train 데이터 문장 637의 토큰화된 길이: 117\n",
      "Train 데이터 문장 638의 토큰화된 길이: 186\n",
      "Train 데이터 문장 639의 토큰화된 길이: 108\n",
      "Train 데이터 문장 640의 토큰화된 길이: 122\n",
      "Train 데이터 문장 641의 토큰화된 길이: 74\n",
      "Train 데이터 문장 642의 토큰화된 길이: 123\n",
      "Train 데이터 문장 643의 토큰화된 길이: 74\n",
      "Train 데이터 문장 644의 토큰화된 길이: 89\n",
      "Train 데이터 문장 645의 토큰화된 길이: 160\n",
      "Train 데이터 문장 646의 토큰화된 길이: 104\n",
      "Train 데이터 문장 647의 토큰화된 길이: 158\n",
      "Train 데이터 문장 648의 토큰화된 길이: 78\n",
      "Train 데이터 문장 649의 토큰화된 길이: 350\n",
      "Train 데이터 문장 650의 토큰화된 길이: 67\n",
      "Train 데이터 문장 651의 토큰화된 길이: 41\n",
      "Train 데이터 문장 652의 토큰화된 길이: 104\n",
      "Train 데이터 문장 653의 토큰화된 길이: 70\n",
      "Train 데이터 문장 654의 토큰화된 길이: 65\n",
      "Train 데이터 문장 655의 토큰화된 길이: 196\n",
      "Train 데이터 문장 656의 토큰화된 길이: 77\n",
      "Train 데이터 문장 657의 토큰화된 길이: 138\n",
      "Train 데이터 문장 658의 토큰화된 길이: 116\n",
      "Train 데이터 문장 659의 토큰화된 길이: 140\n",
      "Train 데이터 문장 660의 토큰화된 길이: 64\n",
      "Train 데이터 문장 661의 토큰화된 길이: 141\n",
      "Train 데이터 문장 662의 토큰화된 길이: 129\n",
      "Train 데이터 문장 663의 토큰화된 길이: 74\n",
      "Train 데이터 문장 664의 토큰화된 길이: 103\n",
      "Train 데이터 문장 665의 토큰화된 길이: 92\n",
      "Train 데이터 문장 666의 토큰화된 길이: 75\n",
      "Train 데이터 문장 667의 토큰화된 길이: 218\n",
      "Train 데이터 문장 668의 토큰화된 길이: 90\n",
      "Train 데이터 문장 669의 토큰화된 길이: 186\n",
      "Train 데이터 문장 670의 토큰화된 길이: 119\n",
      "Train 데이터 문장 671의 토큰화된 길이: 115\n",
      "Train 데이터 문장 672의 토큰화된 길이: 100\n",
      "Train 데이터 문장 673의 토큰화된 길이: 84\n",
      "Train 데이터 문장 674의 토큰화된 길이: 128\n",
      "Train 데이터 문장 675의 토큰화된 길이: 131\n",
      "Train 데이터 문장 676의 토큰화된 길이: 79\n",
      "Train 데이터 문장 677의 토큰화된 길이: 137\n",
      "Train 데이터 문장 678의 토큰화된 길이: 72\n",
      "Train 데이터 문장 679의 토큰화된 길이: 98\n",
      "Train 데이터 문장 680의 토큰화된 길이: 115\n",
      "Train 데이터 문장 681의 토큰화된 길이: 229\n",
      "Train 데이터 문장 682의 토큰화된 길이: 77\n",
      "Train 데이터 문장 683의 토큰화된 길이: 91\n",
      "Train 데이터 문장 684의 토큰화된 길이: 110\n",
      "Train 데이터 문장 685의 토큰화된 길이: 112\n",
      "Train 데이터 문장 686의 토큰화된 길이: 165\n",
      "Train 데이터 문장 687의 토큰화된 길이: 141\n",
      "Train 데이터 문장 688의 토큰화된 길이: 364\n",
      "Train 데이터 문장 689의 토큰화된 길이: 154\n",
      "Train 데이터 문장 690의 토큰화된 길이: 140\n",
      "Train 데이터 문장 691의 토큰화된 길이: 85\n",
      "Train 데이터 문장 692의 토큰화된 길이: 63\n",
      "Train 데이터 문장 693의 토큰화된 길이: 123\n",
      "Train 데이터 문장 694의 토큰화된 길이: 110\n",
      "Train 데이터 문장 695의 토큰화된 길이: 71\n",
      "Train 데이터 문장 696의 토큰화된 길이: 61\n",
      "Train 데이터 문장 697의 토큰화된 길이: 216\n",
      "Train 데이터 문장 698의 토큰화된 길이: 142\n",
      "Train 데이터 문장 699의 토큰화된 길이: 113\n",
      "Train 데이터 문장 700의 토큰화된 길이: 80\n",
      "Train 데이터 문장 701의 토큰화된 길이: 203\n",
      "Train 데이터 문장 702의 토큰화된 길이: 194\n",
      "Train 데이터 문장 703의 토큰화된 길이: 410\n",
      "Train 데이터 문장 704의 토큰화된 길이: 37\n",
      "Train 데이터 문장 705의 토큰화된 길이: 214\n",
      "Train 데이터 문장 706의 토큰화된 길이: 122\n",
      "Train 데이터 문장 707의 토큰화된 길이: 137\n",
      "Train 데이터 문장 708의 토큰화된 길이: 166\n",
      "Train 데이터 문장 709의 토큰화된 길이: 105\n",
      "Train 데이터 문장 710의 토큰화된 길이: 53\n",
      "Train 데이터 문장 711의 토큰화된 길이: 133\n",
      "Train 데이터 문장 712의 토큰화된 길이: 118\n",
      "Train 데이터 문장 713의 토큰화된 길이: 52\n",
      "Train 데이터 문장 714의 토큰화된 길이: 105\n",
      "Train 데이터 문장 715의 토큰화된 길이: 90\n",
      "Train 데이터 문장 716의 토큰화된 길이: 98\n",
      "Train 데이터 문장 717의 토큰화된 길이: 103\n",
      "Train 데이터 문장 718의 토큰화된 길이: 62\n",
      "Train 데이터 문장 719의 토큰화된 길이: 148\n",
      "Train 데이터 문장 720의 토큰화된 길이: 139\n",
      "Train 데이터 문장 721의 토큰화된 길이: 97\n",
      "Train 데이터 문장 722의 토큰화된 길이: 449\n",
      "Train 데이터 문장 723의 토큰화된 길이: 151\n",
      "Train 데이터 문장 724의 토큰화된 길이: 79\n",
      "Train 데이터 문장 725의 토큰화된 길이: 74\n",
      "Train 데이터 문장 726의 토큰화된 길이: 89\n",
      "Train 데이터 문장 727의 토큰화된 길이: 132\n",
      "Train 데이터 문장 728의 토큰화된 길이: 170\n",
      "Train 데이터 문장 729의 토큰화된 길이: 113\n",
      "Train 데이터 문장 730의 토큰화된 길이: 174\n",
      "Train 데이터 문장 731의 토큰화된 길이: 81\n",
      "Train 데이터 문장 732의 토큰화된 길이: 138\n",
      "Train 데이터 문장 733의 토큰화된 길이: 152\n",
      "Train 데이터 문장 734의 토큰화된 길이: 102\n",
      "Train 데이터 문장 735의 토큰화된 길이: 107\n",
      "Train 데이터 문장 736의 토큰화된 길이: 112\n",
      "Train 데이터 문장 737의 토큰화된 길이: 161\n",
      "Train 데이터 문장 738의 토큰화된 길이: 110\n",
      "Train 데이터 문장 739의 토큰화된 길이: 136\n",
      "Train 데이터 문장 740의 토큰화된 길이: 110\n",
      "Train 데이터 문장 741의 토큰화된 길이: 67\n",
      "Train 데이터 문장 742의 토큰화된 길이: 62\n",
      "Train 데이터 문장 743의 토큰화된 길이: 110\n",
      "Train 데이터 문장 744의 토큰화된 길이: 140\n",
      "Train 데이터 문장 745의 토큰화된 길이: 132\n",
      "Train 데이터 문장 746의 토큰화된 길이: 56\n",
      "Train 데이터 문장 747의 토큰화된 길이: 74\n",
      "Train 데이터 문장 748의 토큰화된 길이: 88\n",
      "Train 데이터 문장 749의 토큰화된 길이: 60\n",
      "Train 데이터 문장 750의 토큰화된 길이: 34\n",
      "Train 데이터 문장 751의 토큰화된 길이: 115\n",
      "Train 데이터 문장 752의 토큰화된 길이: 84\n",
      "Train 데이터 문장 753의 토큰화된 길이: 65\n",
      "Train 데이터 문장 754의 토큰화된 길이: 69\n",
      "Train 데이터 문장 755의 토큰화된 길이: 51\n",
      "Train 데이터 문장 756의 토큰화된 길이: 123\n",
      "Train 데이터 문장 757의 토큰화된 길이: 146\n",
      "Train 데이터 문장 758의 토큰화된 길이: 458\n",
      "Train 데이터 문장 759의 토큰화된 길이: 89\n",
      "Train 데이터 문장 760의 토큰화된 길이: 143\n",
      "Train 데이터 문장 761의 토큰화된 길이: 78\n",
      "Train 데이터 문장 762의 토큰화된 길이: 30\n",
      "Train 데이터 문장 763의 토큰화된 길이: 72\n",
      "Train 데이터 문장 764의 토큰화된 길이: 157\n",
      "Train 데이터 문장 765의 토큰화된 길이: 116\n",
      "Train 데이터 문장 766의 토큰화된 길이: 63\n",
      "Train 데이터 문장 767의 토큰화된 길이: 160\n",
      "Train 데이터 문장 768의 토큰화된 길이: 75\n",
      "Train 데이터 문장 769의 토큰화된 길이: 85\n",
      "Train 데이터 문장 770의 토큰화된 길이: 68\n",
      "Train 데이터 문장 771의 토큰화된 길이: 75\n",
      "Train 데이터 문장 772의 토큰화된 길이: 91\n",
      "Train 데이터 문장 773의 토큰화된 길이: 302\n",
      "Train 데이터 문장 774의 토큰화된 길이: 110\n",
      "Train 데이터 문장 775의 토큰화된 길이: 106\n",
      "Train 데이터 문장 776의 토큰화된 길이: 67\n",
      "Train 데이터 문장 777의 토큰화된 길이: 103\n",
      "Train 데이터 문장 778의 토큰화된 길이: 79\n",
      "Train 데이터 문장 779의 토큰화된 길이: 123\n",
      "Train 데이터 문장 780의 토큰화된 길이: 79\n",
      "Train 데이터 문장 781의 토큰화된 길이: 152\n",
      "Train 데이터 문장 782의 토큰화된 길이: 140\n",
      "Train 데이터 문장 783의 토큰화된 길이: 129\n",
      "Train 데이터 문장 784의 토큰화된 길이: 72\n",
      "Train 데이터 문장 785의 토큰화된 길이: 141\n",
      "Train 데이터 문장 786의 토큰화된 길이: 102\n",
      "Train 데이터 문장 787의 토큰화된 길이: 49\n",
      "Train 데이터 문장 788의 토큰화된 길이: 146\n",
      "Train 데이터 문장 789의 토큰화된 길이: 180\n",
      "Train 데이터 문장 790의 토큰화된 길이: 112\n",
      "Train 데이터 문장 791의 토큰화된 길이: 94\n",
      "Train 데이터 문장 792의 토큰화된 길이: 73\n",
      "Train 데이터 문장 793의 토큰화된 길이: 152\n",
      "Train 데이터 문장 794의 토큰화된 길이: 55\n",
      "Train 데이터 문장 795의 토큰화된 길이: 142\n",
      "Train 데이터 문장 796의 토큰화된 길이: 163\n",
      "Train 데이터 문장 797의 토큰화된 길이: 72\n",
      "Train 데이터 문장 798의 토큰화된 길이: 118\n",
      "Train 데이터 문장 799의 토큰화된 길이: 214\n",
      "Train 데이터 문장 800의 토큰화된 길이: 128\n",
      "Train 데이터 문장 801의 토큰화된 길이: 78\n",
      "Train 데이터 문장 802의 토큰화된 길이: 138\n",
      "Train 데이터 문장 803의 토큰화된 길이: 46\n",
      "Train 데이터 문장 804의 토큰화된 길이: 93\n",
      "Train 데이터 문장 805의 토큰화된 길이: 105\n",
      "Train 데이터 문장 806의 토큰화된 길이: 125\n",
      "Train 데이터 문장 807의 토큰화된 길이: 127\n",
      "Train 데이터 문장 808의 토큰화된 길이: 79\n",
      "Train 데이터 문장 809의 토큰화된 길이: 121\n",
      "Train 데이터 문장 810의 토큰화된 길이: 82\n",
      "Train 데이터 문장 811의 토큰화된 길이: 164\n",
      "Train 데이터 문장 812의 토큰화된 길이: 107\n",
      "Train 데이터 문장 813의 토큰화된 길이: 157\n",
      "Train 데이터 문장 814의 토큰화된 길이: 65\n",
      "Train 데이터 문장 815의 토큰화된 길이: 151\n",
      "Train 데이터 문장 816의 토큰화된 길이: 77\n",
      "Train 데이터 문장 817의 토큰화된 길이: 124\n",
      "Train 데이터 문장 818의 토큰화된 길이: 210\n",
      "Train 데이터 문장 819의 토큰화된 길이: 94\n",
      "Train 데이터 문장 820의 토큰화된 길이: 368\n",
      "Train 데이터 문장 821의 토큰화된 길이: 313\n",
      "Train 데이터 문장 822의 토큰화된 길이: 128\n",
      "Train 데이터 문장 823의 토큰화된 길이: 92\n",
      "Train 데이터 문장 824의 토큰화된 길이: 91\n",
      "Train 데이터 문장 825의 토큰화된 길이: 130\n",
      "Train 데이터 문장 826의 토큰화된 길이: 142\n",
      "Train 데이터 문장 827의 토큰화된 길이: 68\n",
      "Train 데이터 문장 828의 토큰화된 길이: 96\n",
      "Train 데이터 문장 829의 토큰화된 길이: 159\n",
      "Train 데이터 문장 830의 토큰화된 길이: 177\n",
      "Train 데이터 문장 831의 토큰화된 길이: 159\n",
      "Train 데이터 문장 832의 토큰화된 길이: 75\n",
      "Train 데이터 문장 833의 토큰화된 길이: 114\n",
      "Train 데이터 문장 834의 토큰화된 길이: 122\n",
      "Train 데이터 문장 835의 토큰화된 길이: 319\n",
      "Train 데이터 문장 836의 토큰화된 길이: 135\n",
      "Train 데이터 문장 837의 토큰화된 길이: 139\n",
      "Train 데이터 문장 838의 토큰화된 길이: 83\n",
      "Train 데이터 문장 839의 토큰화된 길이: 131\n",
      "Train 데이터 문장 840의 토큰화된 길이: 214\n",
      "Train 데이터 문장 841의 토큰화된 길이: 116\n",
      "Train 데이터 문장 842의 토큰화된 길이: 95\n",
      "Train 데이터 문장 843의 토큰화된 길이: 80\n",
      "Train 데이터 문장 844의 토큰화된 길이: 116\n",
      "Train 데이터 문장 845의 토큰화된 길이: 144\n",
      "Train 데이터 문장 846의 토큰화된 길이: 134\n",
      "Train 데이터 문장 847의 토큰화된 길이: 195\n",
      "Train 데이터 문장 848의 토큰화된 길이: 328\n",
      "Train 데이터 문장 849의 토큰화된 길이: 87\n",
      "Train 데이터 문장 850의 토큰화된 길이: 51\n",
      "Train 데이터 문장 851의 토큰화된 길이: 131\n",
      "Train 데이터 문장 852의 토큰화된 길이: 79\n",
      "Train 데이터 문장 853의 토큰화된 길이: 72\n",
      "Train 데이터 문장 854의 토큰화된 길이: 120\n",
      "Train 데이터 문장 855의 토큰화된 길이: 189\n",
      "Train 데이터 문장 856의 토큰화된 길이: 81\n",
      "Train 데이터 문장 857의 토큰화된 길이: 94\n",
      "Train 데이터 문장 858의 토큰화된 길이: 73\n",
      "Train 데이터 문장 859의 토큰화된 길이: 57\n",
      "Train 데이터 문장 860의 토큰화된 길이: 128\n",
      "Train 데이터 문장 861의 토큰화된 길이: 329\n",
      "Train 데이터 문장 862의 토큰화된 길이: 136\n",
      "Train 데이터 문장 863의 토큰화된 길이: 72\n",
      "Train 데이터 문장 864의 토큰화된 길이: 123\n",
      "Train 데이터 문장 865의 토큰화된 길이: 109\n",
      "Train 데이터 문장 866의 토큰화된 길이: 79\n",
      "Train 데이터 문장 867의 토큰화된 길이: 80\n",
      "Train 데이터 문장 868의 토큰화된 길이: 182\n",
      "Train 데이터 문장 869의 토큰화된 길이: 94\n",
      "Train 데이터 문장 870의 토큰화된 길이: 140\n",
      "Train 데이터 문장 871의 토큰화된 길이: 142\n",
      "Train 데이터 문장 872의 토큰화된 길이: 107\n",
      "Train 데이터 문장 873의 토큰화된 길이: 131\n",
      "Train 데이터 문장 874의 토큰화된 길이: 106\n",
      "Train 데이터 문장 875의 토큰화된 길이: 97\n",
      "Train 데이터 문장 876의 토큰화된 길이: 78\n",
      "Train 데이터 문장 877의 토큰화된 길이: 150\n",
      "Train 데이터 문장 878의 토큰화된 길이: 123\n",
      "Train 데이터 문장 879의 토큰화된 길이: 98\n",
      "Train 데이터 문장 880의 토큰화된 길이: 113\n",
      "Train 데이터 문장 881의 토큰화된 길이: 103\n",
      "Train 데이터 문장 882의 토큰화된 길이: 139\n",
      "Train 데이터 문장 883의 토큰화된 길이: 161\n",
      "Train 데이터 문장 884의 토큰화된 길이: 77\n",
      "Train 데이터 문장 885의 토큰화된 길이: 61\n",
      "Train 데이터 문장 886의 토큰화된 길이: 97\n",
      "Train 데이터 문장 887의 토큰화된 길이: 202\n",
      "Train 데이터 문장 888의 토큰화된 길이: 215\n",
      "Train 데이터 문장 889의 토큰화된 길이: 118\n",
      "Train 데이터 문장 890의 토큰화된 길이: 81\n",
      "Train 데이터 문장 891의 토큰화된 길이: 174\n",
      "Train 데이터 문장 892의 토큰화된 길이: 58\n",
      "Train 데이터 문장 893의 토큰화된 길이: 110\n",
      "Train 데이터 문장 894의 토큰화된 길이: 110\n",
      "Train 데이터 문장 895의 토큰화된 길이: 96\n",
      "Train 데이터 문장 896의 토큰화된 길이: 83\n",
      "Train 데이터 문장 897의 토큰화된 길이: 105\n",
      "Train 데이터 문장 898의 토큰화된 길이: 102\n",
      "Train 데이터 문장 899의 토큰화된 길이: 166\n",
      "Train 데이터 문장 900의 토큰화된 길이: 119\n",
      "Train 데이터 문장 901의 토큰화된 길이: 285\n",
      "Train 데이터 문장 902의 토큰화된 길이: 136\n",
      "Train 데이터 문장 903의 토큰화된 길이: 184\n",
      "Train 데이터 문장 904의 토큰화된 길이: 93\n",
      "Train 데이터 문장 905의 토큰화된 길이: 114\n",
      "Train 데이터 문장 906의 토큰화된 길이: 88\n",
      "Train 데이터 문장 907의 토큰화된 길이: 84\n",
      "Train 데이터 문장 908의 토큰화된 길이: 315\n",
      "Train 데이터 문장 909의 토큰화된 길이: 102\n",
      "Train 데이터 문장 910의 토큰화된 길이: 113\n",
      "Train 데이터 문장 911의 토큰화된 길이: 185\n",
      "Train 데이터 문장 912의 토큰화된 길이: 182\n",
      "Train 데이터 문장 913의 토큰화된 길이: 129\n",
      "Train 데이터 문장 914의 토큰화된 길이: 86\n",
      "Train 데이터 문장 915의 토큰화된 길이: 122\n",
      "Train 데이터 문장 916의 토큰화된 길이: 202\n",
      "Train 데이터 문장 917의 토큰화된 길이: 125\n",
      "Train 데이터 문장 918의 토큰화된 길이: 130\n",
      "Train 데이터 문장 919의 토큰화된 길이: 129\n",
      "Train 데이터 문장 920의 토큰화된 길이: 56\n",
      "Train 데이터 문장 921의 토큰화된 길이: 109\n",
      "Train 데이터 문장 922의 토큰화된 길이: 179\n",
      "Train 데이터 문장 923의 토큰화된 길이: 105\n",
      "Train 데이터 문장 924의 토큰화된 길이: 60\n",
      "Train 데이터 문장 925의 토큰화된 길이: 165\n",
      "Train 데이터 문장 926의 토큰화된 길이: 238\n",
      "Train 데이터 문장 927의 토큰화된 길이: 109\n",
      "Train 데이터 문장 928의 토큰화된 길이: 129\n",
      "Train 데이터 문장 929의 토큰화된 길이: 77\n",
      "Train 데이터 문장 930의 토큰화된 길이: 91\n",
      "Train 데이터 문장 931의 토큰화된 길이: 56\n",
      "Train 데이터 문장 932의 토큰화된 길이: 102\n",
      "Train 데이터 문장 933의 토큰화된 길이: 72\n",
      "Train 데이터 문장 934의 토큰화된 길이: 109\n",
      "Train 데이터 문장 935의 토큰화된 길이: 63\n",
      "Train 데이터 문장 936의 토큰화된 길이: 142\n",
      "Train 데이터 문장 937의 토큰화된 길이: 89\n",
      "Train 데이터 문장 938의 토큰화된 길이: 123\n",
      "Train 데이터 문장 939의 토큰화된 길이: 72\n",
      "Train 데이터 문장 940의 토큰화된 길이: 119\n",
      "Train 데이터 문장 941의 토큰화된 길이: 102\n",
      "Train 데이터 문장 942의 토큰화된 길이: 70\n",
      "Train 데이터 문장 943의 토큰화된 길이: 129\n",
      "Train 데이터 문장 944의 토큰화된 길이: 89\n",
      "Train 데이터 문장 945의 토큰화된 길이: 83\n",
      "Train 데이터 문장 946의 토큰화된 길이: 108\n",
      "Train 데이터 문장 947의 토큰화된 길이: 70\n",
      "Train 데이터 문장 948의 토큰화된 길이: 193\n",
      "Train 데이터 문장 949의 토큰화된 길이: 117\n",
      "Train 데이터 문장 950의 토큰화된 길이: 110\n",
      "Train 데이터 문장 951의 토큰화된 길이: 278\n",
      "Train 데이터 문장 952의 토큰화된 길이: 143\n",
      "Train 데이터 문장 953의 토큰화된 길이: 97\n",
      "Train 데이터 문장 954의 토큰화된 길이: 110\n",
      "Train 데이터 문장 955의 토큰화된 길이: 67\n",
      "Train 데이터 문장 956의 토큰화된 길이: 165\n",
      "Train 데이터 문장 957의 토큰화된 길이: 148\n",
      "Train 데이터 문장 958의 토큰화된 길이: 74\n",
      "Train 데이터 문장 959의 토큰화된 길이: 125\n",
      "Train 데이터 문장 960의 토큰화된 길이: 87\n",
      "Train 데이터 문장 961의 토큰화된 길이: 100\n",
      "Train 데이터 문장 962의 토큰화된 길이: 118\n",
      "Train 데이터 문장 963의 토큰화된 길이: 255\n",
      "Train 데이터 문장 964의 토큰화된 길이: 95\n",
      "Train 데이터 문장 965의 토큰화된 길이: 109\n",
      "Train 데이터 문장 966의 토큰화된 길이: 88\n",
      "Train 데이터 문장 967의 토큰화된 길이: 133\n",
      "Train 데이터 문장 968의 토큰화된 길이: 109\n",
      "Train 데이터 문장 969의 토큰화된 길이: 98\n",
      "Train 데이터 문장 970의 토큰화된 길이: 91\n",
      "Train 데이터 문장 971의 토큰화된 길이: 132\n",
      "Train 데이터 문장 972의 토큰화된 길이: 122\n",
      "Train 데이터 문장 973의 토큰화된 길이: 304\n",
      "Train 데이터 문장 974의 토큰화된 길이: 221\n",
      "Train 데이터 문장 975의 토큰화된 길이: 168\n",
      "Train 데이터 문장 976의 토큰화된 길이: 71\n",
      "Train 데이터 문장 977의 토큰화된 길이: 127\n",
      "Train 데이터 문장 978의 토큰화된 길이: 116\n",
      "Train 데이터 문장 979의 토큰화된 길이: 131\n",
      "Train 데이터 문장 980의 토큰화된 길이: 183\n",
      "Train 데이터 문장 981의 토큰화된 길이: 159\n",
      "Train 데이터 문장 982의 토큰화된 길이: 85\n",
      "Train 데이터 문장 983의 토큰화된 길이: 77\n",
      "Train 데이터 문장 984의 토큰화된 길이: 158\n",
      "Train 데이터 문장 985의 토큰화된 길이: 110\n",
      "Train 데이터 문장 986의 토큰화된 길이: 119\n",
      "Train 데이터 문장 987의 토큰화된 길이: 242\n",
      "Train 데이터 문장 988의 토큰화된 길이: 87\n",
      "Train 데이터 문장 989의 토큰화된 길이: 45\n",
      "Train 데이터 문장 990의 토큰화된 길이: 124\n",
      "Train 데이터 문장 991의 토큰화된 길이: 194\n",
      "Train 데이터 문장 992의 토큰화된 길이: 96\n",
      "Train 데이터 문장 993의 토큰화된 길이: 132\n",
      "Train 데이터 문장 994의 토큰화된 길이: 116\n",
      "Train 데이터 문장 995의 토큰화된 길이: 38\n",
      "Train 데이터 문장 996의 토큰화된 길이: 84\n",
      "Train 데이터 문장 997의 토큰화된 길이: 150\n",
      "Train 데이터 문장 998의 토큰화된 길이: 91\n",
      "Train 데이터 문장 999의 토큰화된 길이: 60\n",
      "Train 데이터 문장 1000의 토큰화된 길이: 60\n",
      "Train 데이터 문장 1001의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1002의 토큰화된 길이: 172\n",
      "Train 데이터 문장 1003의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1004의 토큰화된 길이: 131\n",
      "Train 데이터 문장 1005의 토큰화된 길이: 169\n",
      "Train 데이터 문장 1006의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1007의 토큰화된 길이: 128\n",
      "Train 데이터 문장 1008의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1009의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1010의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1011의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1012의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1013의 토큰화된 길이: 137\n",
      "Train 데이터 문장 1014의 토큰화된 길이: 172\n",
      "Train 데이터 문장 1015의 토큰화된 길이: 266\n",
      "Train 데이터 문장 1016의 토큰화된 길이: 86\n",
      "Train 데이터 문장 1017의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1018의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1019의 토큰화된 길이: 59\n",
      "Train 데이터 문장 1020의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1021의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1022의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1023의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1024의 토큰화된 길이: 173\n",
      "Train 데이터 문장 1025의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1026의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1027의 토큰화된 길이: 124\n",
      "Train 데이터 문장 1028의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1029의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1030의 토큰화된 길이: 128\n",
      "Train 데이터 문장 1031의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1032의 토큰화된 길이: 59\n",
      "Train 데이터 문장 1033의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1034의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1035의 토큰화된 길이: 162\n",
      "Train 데이터 문장 1036의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1037의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1038의 토큰화된 길이: 144\n",
      "Train 데이터 문장 1039의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1040의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1041의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1042의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1043의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1044의 토큰화된 길이: 62\n",
      "Train 데이터 문장 1045의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1046의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1047의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1048의 토큰화된 길이: 43\n",
      "Train 데이터 문장 1049의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1050의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1051의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1052의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1053의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1054의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1055의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1056의 토큰화된 길이: 181\n",
      "Train 데이터 문장 1057의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1058의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1059의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1060의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1061의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1062의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1063의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1064의 토큰화된 길이: 173\n",
      "Train 데이터 문장 1065의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1066의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1067의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1068의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1069의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1070의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1071의 토큰화된 길이: 62\n",
      "Train 데이터 문장 1072의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1073의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1074의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1075의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1076의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1077의 토큰화된 길이: 65\n",
      "Train 데이터 문장 1078의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1079의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1080의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1081의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1082의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1083의 토큰화된 길이: 65\n",
      "Train 데이터 문장 1084의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1085의 토큰화된 길이: 294\n",
      "Train 데이터 문장 1086의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1087의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1088의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1089의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1090의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1091의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1092의 토큰화된 길이: 103\n",
      "Train 데이터 문장 1093의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1094의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1095의 토큰화된 길이: 52\n",
      "Train 데이터 문장 1096의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1097의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1098의 토큰화된 길이: 237\n",
      "Train 데이터 문장 1099의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1100의 토큰화된 길이: 209\n",
      "Train 데이터 문장 1101의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1102의 토큰화된 길이: 189\n",
      "Train 데이터 문장 1103의 토큰화된 길이: 54\n",
      "Train 데이터 문장 1104의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1105의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1106의 토큰화된 길이: 184\n",
      "Train 데이터 문장 1107의 토큰화된 길이: 149\n",
      "Train 데이터 문장 1108의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1109의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1110의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1111의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1112의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1113의 토큰화된 길이: 187\n",
      "Train 데이터 문장 1114의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1115의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1116의 토큰화된 길이: 318\n",
      "Train 데이터 문장 1117의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1118의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1119의 토큰화된 길이: 199\n",
      "Train 데이터 문장 1120의 토큰화된 길이: 38\n",
      "Train 데이터 문장 1121의 토큰화된 길이: 154\n",
      "Train 데이터 문장 1122의 토큰화된 길이: 186\n",
      "Train 데이터 문장 1123의 토큰화된 길이: 153\n",
      "Train 데이터 문장 1124의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1125의 토큰화된 길이: 103\n",
      "Train 데이터 문장 1126의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1127의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1128의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1129의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1130의 토큰화된 길이: 86\n",
      "Train 데이터 문장 1131의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1132의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1133의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1134의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1135의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1136의 토큰화된 길이: 214\n",
      "Train 데이터 문장 1137의 토큰화된 길이: 168\n",
      "Train 데이터 문장 1138의 토큰화된 길이: 59\n",
      "Train 데이터 문장 1139의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1140의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1141의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1142의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1143의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1144의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1145의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1146의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1147의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1148의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1149의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1150의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1151의 토큰화된 길이: 49\n",
      "Train 데이터 문장 1152의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1153의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1154의 토큰화된 길이: 139\n",
      "Train 데이터 문장 1155의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1156의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1157의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1158의 토큰화된 길이: 241\n",
      "Train 데이터 문장 1159의 토큰화된 길이: 159\n",
      "Train 데이터 문장 1160의 토큰화된 길이: 205\n",
      "Train 데이터 문장 1161의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1162의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1163의 토큰화된 길이: 57\n",
      "Train 데이터 문장 1164의 토큰화된 길이: 242\n",
      "Train 데이터 문장 1165의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1166의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1167의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1168의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1169의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1170의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1171의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1172의 토큰화된 길이: 163\n",
      "Train 데이터 문장 1173의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1174의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1175의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1176의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1177의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1178의 토큰화된 길이: 220\n",
      "Train 데이터 문장 1179의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1180의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1181의 토큰화된 길이: 177\n",
      "Train 데이터 문장 1182의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1183의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1184의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1185의 토큰화된 길이: 308\n",
      "Train 데이터 문장 1186의 토큰화된 길이: 270\n",
      "Train 데이터 문장 1187의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1188의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1189의 토큰화된 길이: 45\n",
      "Train 데이터 문장 1190의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1191의 토큰화된 길이: 147\n",
      "Train 데이터 문장 1192의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1193의 토큰화된 길이: 54\n",
      "Train 데이터 문장 1194의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1195의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1196의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1197의 토큰화된 길이: 241\n",
      "Train 데이터 문장 1198의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1199의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1200의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1201의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1202의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1203의 토큰화된 길이: 153\n",
      "Train 데이터 문장 1204의 토큰화된 길이: 389\n",
      "Train 데이터 문장 1205의 토큰화된 길이: 156\n",
      "Train 데이터 문장 1206의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1207의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1208의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1209의 토큰화된 길이: 70\n",
      "Train 데이터 문장 1210의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1211의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1212의 토큰화된 길이: 263\n",
      "Train 데이터 문장 1213의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1214의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1215의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1216의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1217의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1218의 토큰화된 길이: 170\n",
      "Train 데이터 문장 1219의 토큰화된 길이: 226\n",
      "Train 데이터 문장 1220의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1221의 토큰화된 길이: 201\n",
      "Train 데이터 문장 1222의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1223의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1224의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1225의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1226의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1227의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1228의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1229의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1230의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1231의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1232의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1233의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1234의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1235의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1236의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1237의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1238의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1239의 토큰화된 길이: 153\n",
      "Train 데이터 문장 1240의 토큰화된 길이: 199\n",
      "Train 데이터 문장 1241의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1242의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1243의 토큰화된 길이: 157\n",
      "Train 데이터 문장 1244의 토큰화된 길이: 157\n",
      "Train 데이터 문장 1245의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1246의 토큰화된 길이: 152\n",
      "Train 데이터 문장 1247의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1248의 토큰화된 길이: 143\n",
      "Train 데이터 문장 1249의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1250의 토큰화된 길이: 158\n",
      "Train 데이터 문장 1251의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1252의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1253의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1254의 토큰화된 길이: 84\n",
      "Train 데이터 문장 1255의 토큰화된 길이: 336\n",
      "Train 데이터 문장 1256의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1257의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1258의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1259의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1260의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1261의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1262의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1263의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1264의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1265의 토큰화된 길이: 84\n",
      "Train 데이터 문장 1266의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1267의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1268의 토큰화된 길이: 176\n",
      "Train 데이터 문장 1269의 토큰화된 길이: 183\n",
      "Train 데이터 문장 1270의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1271의 토큰화된 길이: 107\n",
      "Train 데이터 문장 1272의 토큰화된 길이: 359\n",
      "Train 데이터 문장 1273의 토큰화된 길이: 175\n",
      "Train 데이터 문장 1274의 토큰화된 길이: 180\n",
      "Train 데이터 문장 1275의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1276의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1277의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1278의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1279의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1280의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1281의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1282의 토큰화된 길이: 107\n",
      "Train 데이터 문장 1283의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1284의 토큰화된 길이: 180\n",
      "Train 데이터 문장 1285의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1286의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1287의 토큰화된 길이: 124\n",
      "Train 데이터 문장 1288의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1289의 토큰화된 길이: 163\n",
      "Train 데이터 문장 1290의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1291의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1292의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1293의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1294의 토큰화된 길이: 333\n",
      "Train 데이터 문장 1295의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1296의 토큰화된 길이: 176\n",
      "Train 데이터 문장 1297의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1298의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1299의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1300의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1301의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1302의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1303의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1304의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1305의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1306의 토큰화된 길이: 84\n",
      "Train 데이터 문장 1307의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1308의 토큰화된 길이: 188\n",
      "Train 데이터 문장 1309의 토큰화된 길이: 186\n",
      "Train 데이터 문장 1310의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1311의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1312의 토큰화된 길이: 239\n",
      "Train 데이터 문장 1313의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1314의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1315의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1316의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1317의 토큰화된 길이: 52\n",
      "Train 데이터 문장 1318의 토큰화된 길이: 218\n",
      "Train 데이터 문장 1319의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1320의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1321의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1322의 토큰화된 길이: 45\n",
      "Train 데이터 문장 1323의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1324의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1325의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1326의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1327의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1328의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1329의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1330의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1331의 토큰화된 길이: 86\n",
      "Train 데이터 문장 1332의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1333의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1334의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1335의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1336의 토큰화된 길이: 194\n",
      "Train 데이터 문장 1337의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1338의 토큰화된 길이: 233\n",
      "Train 데이터 문장 1339의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1340의 토큰화된 길이: 43\n",
      "Train 데이터 문장 1341의 토큰화된 길이: 224\n",
      "Train 데이터 문장 1342의 토큰화된 길이: 156\n",
      "Train 데이터 문장 1343의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1344의 토큰화된 길이: 342\n",
      "Train 데이터 문장 1345의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1346의 토큰화된 길이: 319\n",
      "Train 데이터 문장 1347의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1348의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1349의 토큰화된 길이: 205\n",
      "Train 데이터 문장 1350의 토큰화된 길이: 58\n",
      "Train 데이터 문장 1351의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1352의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1353의 토큰화된 길이: 205\n",
      "Train 데이터 문장 1354의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1355의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1356의 토큰화된 길이: 124\n",
      "Train 데이터 문장 1357의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1358의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1359의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1360의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1361의 토큰화된 길이: 53\n",
      "Train 데이터 문장 1362의 토큰화된 길이: 61\n",
      "Train 데이터 문장 1363의 토큰화된 길이: 176\n",
      "Train 데이터 문장 1364의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1365의 토큰화된 길이: 169\n",
      "Train 데이터 문장 1366의 토큰화된 길이: 182\n",
      "Train 데이터 문장 1367의 토큰화된 길이: 152\n",
      "Train 데이터 문장 1368의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1369의 토큰화된 길이: 147\n",
      "Train 데이터 문장 1370의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1371의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1372의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1373의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1374의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1375의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1376의 토큰화된 길이: 163\n",
      "Train 데이터 문장 1377의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1378의 토큰화된 길이: 352\n",
      "Train 데이터 문장 1379의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1380의 토큰화된 길이: 86\n",
      "Train 데이터 문장 1381의 토큰화된 길이: 139\n",
      "Train 데이터 문장 1382의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1383의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1384의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1385의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1386의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1387의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1388의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1389의 토큰화된 길이: 167\n",
      "Train 데이터 문장 1390의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1391의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1392의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1393의 토큰화된 길이: 53\n",
      "Train 데이터 문장 1394의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1395의 토큰화된 길이: 58\n",
      "Train 데이터 문장 1396의 토큰화된 길이: 152\n",
      "Train 데이터 문장 1397의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1398의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1399의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1400의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1401의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1402의 토큰화된 길이: 128\n",
      "Train 데이터 문장 1403의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1404의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1405의 토큰화된 길이: 217\n",
      "Train 데이터 문장 1406의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1407의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1408의 토큰화된 길이: 174\n",
      "Train 데이터 문장 1409의 토큰화된 길이: 241\n",
      "Train 데이터 문장 1410의 토큰화된 길이: 61\n",
      "Train 데이터 문장 1411의 토큰화된 길이: 163\n",
      "Train 데이터 문장 1412의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1413의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1414의 토큰화된 길이: 202\n",
      "Train 데이터 문장 1415의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1416의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1417의 토큰화된 길이: 197\n",
      "Train 데이터 문장 1418의 토큰화된 길이: 166\n",
      "Train 데이터 문장 1419의 토큰화된 길이: 70\n",
      "Train 데이터 문장 1420의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1421의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1422의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1423의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1424의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1425의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1426의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1427의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1428의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1429의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1430의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1431의 토큰화된 길이: 213\n",
      "Train 데이터 문장 1432의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1433의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1434의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1435의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1436의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1437의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1438의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1439의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1440의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1441의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1442의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1443의 토큰화된 길이: 187\n",
      "Train 데이터 문장 1444의 토큰화된 길이: 57\n",
      "Train 데이터 문장 1445의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1446의 토큰화된 길이: 225\n",
      "Train 데이터 문장 1447의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1448의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1449의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1450의 토큰화된 길이: 277\n",
      "Train 데이터 문장 1451의 토큰화된 길이: 158\n",
      "Train 데이터 문장 1452의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1453의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1454의 토큰화된 길이: 210\n",
      "Train 데이터 문장 1455의 토큰화된 길이: 138\n",
      "Train 데이터 문장 1456의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1457의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1458의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1459의 토큰화된 길이: 215\n",
      "Train 데이터 문장 1460의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1461의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1462의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1463의 토큰화된 길이: 160\n",
      "Train 데이터 문장 1464의 토큰화된 길이: 124\n",
      "Train 데이터 문장 1465의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1466의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1467의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1468의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1469의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1470의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1471의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1472의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1473의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1474의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1475의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1476의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1477의 토큰화된 길이: 178\n",
      "Train 데이터 문장 1478의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1479의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1480의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1481의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1482의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1483의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1484의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1485의 토큰화된 길이: 329\n",
      "Train 데이터 문장 1486의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1487의 토큰화된 길이: 50\n",
      "Train 데이터 문장 1488의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1489의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1490의 토큰화된 길이: 65\n",
      "Train 데이터 문장 1491의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1492의 토큰화된 길이: 128\n",
      "Train 데이터 문장 1493의 토큰화된 길이: 178\n",
      "Train 데이터 문장 1494의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1495의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1496의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1497의 토큰화된 길이: 162\n",
      "Train 데이터 문장 1498의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1499의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1500의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1501의 토큰화된 길이: 146\n",
      "Train 데이터 문장 1502의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1503의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1504의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1505의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1506의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1507의 토큰화된 길이: 327\n",
      "Train 데이터 문장 1508의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1509의 토큰화된 길이: 153\n",
      "Train 데이터 문장 1510의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1511의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1512의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1513의 토큰화된 길이: 45\n",
      "Train 데이터 문장 1514의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1515의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1516의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1517의 토큰화된 길이: 204\n",
      "Train 데이터 문장 1518의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1519의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1520의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1521의 토큰화된 길이: 220\n",
      "Train 데이터 문장 1522의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1523의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1524의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1525의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1526의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1527의 토큰화된 길이: 174\n",
      "Train 데이터 문장 1528의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1529의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1530의 토큰화된 길이: 52\n",
      "Train 데이터 문장 1531의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1532의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1533의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1534의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1535의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1536의 토큰화된 길이: 65\n",
      "Train 데이터 문장 1537의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1538의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1539의 토큰화된 길이: 58\n",
      "Train 데이터 문장 1540의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1541의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1542의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1543의 토큰화된 길이: 201\n",
      "Train 데이터 문장 1544의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1545의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1546의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1547의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1548의 토큰화된 길이: 164\n",
      "Train 데이터 문장 1549의 토큰화된 길이: 149\n",
      "Train 데이터 문장 1550의 토큰화된 길이: 128\n",
      "Train 데이터 문장 1551의 토큰화된 길이: 155\n",
      "Train 데이터 문장 1552의 토큰화된 길이: 58\n",
      "Train 데이터 문장 1553의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1554의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1555의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1556의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1557의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1558의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1559의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1560의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1561의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1562의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1563의 토큰화된 길이: 299\n",
      "Train 데이터 문장 1564의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1565의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1566의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1567의 토큰화된 길이: 236\n",
      "Train 데이터 문장 1568의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1569의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1570의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1571의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1572의 토큰화된 길이: 156\n",
      "Train 데이터 문장 1573의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1574의 토큰화된 길이: 359\n",
      "Train 데이터 문장 1575의 토큰화된 길이: 147\n",
      "Train 데이터 문장 1576의 토큰화된 길이: 124\n",
      "Train 데이터 문장 1577의 토큰화된 길이: 183\n",
      "Train 데이터 문장 1578의 토큰화된 길이: 236\n",
      "Train 데이터 문장 1579의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1580의 토큰화된 길이: 107\n",
      "Train 데이터 문장 1581의 토큰화된 길이: 154\n",
      "Train 데이터 문장 1582의 토큰화된 길이: 188\n",
      "Train 데이터 문장 1583의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1584의 토큰화된 길이: 225\n",
      "Train 데이터 문장 1585의 토큰화된 길이: 147\n",
      "Train 데이터 문장 1586의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1587의 토큰화된 길이: 188\n",
      "Train 데이터 문장 1588의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1589의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1590의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1591의 토큰화된 길이: 175\n",
      "Train 데이터 문장 1592의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1593의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1594의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1595의 토큰화된 길이: 223\n",
      "Train 데이터 문장 1596의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1597의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1598의 토큰화된 길이: 241\n",
      "Train 데이터 문장 1599의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1600의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1601의 토큰화된 길이: 149\n",
      "Train 데이터 문장 1602의 토큰화된 길이: 292\n",
      "Train 데이터 문장 1603의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1604의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1605의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1606의 토큰화된 길이: 267\n",
      "Train 데이터 문장 1607의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1608의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1609의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1610의 토큰화된 길이: 54\n",
      "Train 데이터 문장 1611의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1612의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1613의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1614의 토큰화된 길이: 59\n",
      "Train 데이터 문장 1615의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1616의 토큰화된 길이: 164\n",
      "Train 데이터 문장 1617의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1618의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1619의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1620의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1621의 토큰화된 길이: 119\n",
      "Train 데이터 문장 1622의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1623의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1624의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1625의 토큰화된 길이: 350\n",
      "Train 데이터 문장 1626의 토큰화된 길이: 62\n",
      "Train 데이터 문장 1627의 토큰화된 길이: 255\n",
      "Train 데이터 문장 1628의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1629의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1630의 토큰화된 길이: 237\n",
      "Train 데이터 문장 1631의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1632의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1633의 토큰화된 길이: 73\n",
      "Train 데이터 문장 1634의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1635의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1636의 토큰화된 길이: 170\n",
      "Train 데이터 문장 1637의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1638의 토큰화된 길이: 65\n",
      "Train 데이터 문장 1639의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1640의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1641의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1642의 토큰화된 길이: 107\n",
      "Train 데이터 문장 1643의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1644의 토큰화된 길이: 157\n",
      "Train 데이터 문장 1645의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1646의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1647의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1648의 토큰화된 길이: 105\n",
      "Train 데이터 문장 1649의 토큰화된 길이: 166\n",
      "Train 데이터 문장 1650의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1651의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1652의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1653의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1654의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1655의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1656의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1657의 토큰화된 길이: 84\n",
      "Train 데이터 문장 1658의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1659의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1660의 토큰화된 길이: 47\n",
      "Train 데이터 문장 1661의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1662의 토큰화된 길이: 143\n",
      "Train 데이터 문장 1663의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1664의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1665의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1666의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1667의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1668의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1669의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1670의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1671의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1672의 토큰화된 길이: 123\n",
      "Train 데이터 문장 1673의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1674의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1675의 토큰화된 길이: 170\n",
      "Train 데이터 문장 1676의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1677의 토큰화된 길이: 156\n",
      "Train 데이터 문장 1678의 토큰화된 길이: 287\n",
      "Train 데이터 문장 1679의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1680의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1681의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1682의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1683의 토큰화된 길이: 74\n",
      "Train 데이터 문장 1684의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1685의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1686의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1687의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1688의 토큰화된 길이: 60\n",
      "Train 데이터 문장 1689의 토큰화된 길이: 49\n",
      "Train 데이터 문장 1690의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1691의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1692의 토큰화된 길이: 103\n",
      "Train 데이터 문장 1693의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1694의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1695의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1696의 토큰화된 길이: 185\n",
      "Train 데이터 문장 1697의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1698의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1699의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1700의 토큰화된 길이: 164\n",
      "Train 데이터 문장 1701의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1702의 토큰화된 길이: 167\n",
      "Train 데이터 문장 1703의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1704의 토큰화된 길이: 335\n",
      "Train 데이터 문장 1705의 토큰화된 길이: 138\n",
      "Train 데이터 문장 1706의 토큰화된 길이: 226\n",
      "Train 데이터 문장 1707의 토큰화된 길이: 180\n",
      "Train 데이터 문장 1708의 토큰화된 길이: 275\n",
      "Train 데이터 문장 1709의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1710의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1711의 토큰화된 길이: 168\n",
      "Train 데이터 문장 1712의 토큰화된 길이: 283\n",
      "Train 데이터 문장 1713의 토큰화된 길이: 235\n",
      "Train 데이터 문장 1714의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1715의 토큰화된 길이: 56\n",
      "Train 데이터 문장 1716의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1717의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1718의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1719의 토큰화된 길이: 92\n",
      "Train 데이터 문장 1720의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1721의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1722의 토큰화된 길이: 210\n",
      "Train 데이터 문장 1723의 토큰화된 길이: 266\n",
      "Train 데이터 문장 1724의 토큰화된 길이: 215\n",
      "Train 데이터 문장 1725의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1726의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1727의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1728의 토큰화된 길이: 286\n",
      "Train 데이터 문장 1729의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1730의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1731의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1732의 토큰화된 길이: 185\n",
      "Train 데이터 문장 1733의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1734의 토큰화된 길이: 169\n",
      "Train 데이터 문장 1735의 토큰화된 길이: 136\n",
      "Train 데이터 문장 1736의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1737의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1738의 토큰화된 길이: 60\n",
      "Train 데이터 문장 1739의 토큰화된 길이: 165\n",
      "Train 데이터 문장 1740의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1741의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1742의 토큰화된 길이: 64\n",
      "Train 데이터 문장 1743의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1744의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1745의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1746의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1747의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1748의 토큰화된 길이: 205\n",
      "Train 데이터 문장 1749의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1750의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1751의 토큰화된 길이: 161\n",
      "Train 데이터 문장 1752의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1753의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1754의 토큰화된 길이: 42\n",
      "Train 데이터 문장 1755의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1756의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1757의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1758의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1759의 토큰화된 길이: 263\n",
      "Train 데이터 문장 1760의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1761의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1762의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1763의 토큰화된 길이: 115\n",
      "Train 데이터 문장 1764의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1765의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1766의 토큰화된 길이: 217\n",
      "Train 데이터 문장 1767의 토큰화된 길이: 144\n",
      "Train 데이터 문장 1768의 토큰화된 길이: 121\n",
      "Train 데이터 문장 1769의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1770의 토큰화된 길이: 159\n",
      "Train 데이터 문장 1771의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1772의 토큰화된 길이: 52\n",
      "Train 데이터 문장 1773의 토큰화된 길이: 298\n",
      "Train 데이터 문장 1774의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1775의 토큰화된 길이: 162\n",
      "Train 데이터 문장 1776의 토큰화된 길이: 24\n",
      "Train 데이터 문장 1777의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1778의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1779의 토큰화된 길이: 70\n",
      "Train 데이터 문장 1780의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1781의 토큰화된 길이: 189\n",
      "Train 데이터 문장 1782의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1783의 토큰화된 길이: 195\n",
      "Train 데이터 문장 1784의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1785의 토큰화된 길이: 137\n",
      "Train 데이터 문장 1786의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1787의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1788의 토큰화된 길이: 144\n",
      "Train 데이터 문장 1789의 토큰화된 길이: 45\n",
      "Train 데이터 문장 1790의 토큰화된 길이: 223\n",
      "Train 데이터 문장 1791의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1792의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1793의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1794의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1795의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1796의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1797의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1798의 토큰화된 길이: 59\n",
      "Train 데이터 문장 1799의 토큰화된 길이: 151\n",
      "Train 데이터 문장 1800의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1801의 토큰화된 길이: 82\n",
      "Train 데이터 문장 1802의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1803의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1804의 토큰화된 길이: 254\n",
      "Train 데이터 문장 1805의 토큰화된 길이: 164\n",
      "Train 데이터 문장 1806의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1807의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1808의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1809의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1810의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1811의 토큰화된 길이: 193\n",
      "Train 데이터 문장 1812의 토큰화된 길이: 169\n",
      "Train 데이터 문장 1813의 토큰화된 길이: 44\n",
      "Train 데이터 문장 1814의 토큰화된 길이: 50\n",
      "Train 데이터 문장 1815의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1816의 토큰화된 길이: 51\n",
      "Train 데이터 문장 1817의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1818의 토큰화된 길이: 173\n",
      "Train 데이터 문장 1819의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1820의 토큰화된 길이: 149\n",
      "Train 데이터 문장 1821의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1822의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1823의 토큰화된 길이: 160\n",
      "Train 데이터 문장 1824의 토큰화된 길이: 150\n",
      "Train 데이터 문장 1825의 토큰화된 길이: 83\n",
      "Train 데이터 문장 1826의 토큰화된 길이: 168\n",
      "Train 데이터 문장 1827의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1828의 토큰화된 길이: 101\n",
      "Train 데이터 문장 1829의 토큰화된 길이: 158\n",
      "Train 데이터 문장 1830의 토큰화된 길이: 69\n",
      "Train 데이터 문장 1831의 토큰화된 길이: 169\n",
      "Train 데이터 문장 1832의 토큰화된 길이: 189\n",
      "Train 데이터 문장 1833의 토큰화된 길이: 219\n",
      "Train 데이터 문장 1834의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1835의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1836의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1837의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1838의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1839의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1840의 토큰화된 길이: 173\n",
      "Train 데이터 문장 1841의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1842의 토큰화된 길이: 71\n",
      "Train 데이터 문장 1843의 토큰화된 길이: 131\n",
      "Train 데이터 문장 1844의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1845의 토큰화된 길이: 85\n",
      "Train 데이터 문장 1846의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1847의 토큰화된 길이: 131\n",
      "Train 데이터 문장 1848의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1849의 토큰화된 길이: 72\n",
      "Train 데이터 문장 1850의 토큰화된 길이: 56\n",
      "Train 데이터 문장 1851의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1852의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1853의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1854의 토큰화된 길이: 107\n",
      "Train 데이터 문장 1855의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1856의 토큰화된 길이: 255\n",
      "Train 데이터 문장 1857의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1858의 토큰화된 길이: 79\n",
      "Train 데이터 문장 1859의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1860의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1861의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1862의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1863의 토큰화된 길이: 188\n",
      "Train 데이터 문장 1864의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1865의 토큰화된 길이: 56\n",
      "Train 데이터 문장 1866의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1867의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1868의 토큰화된 길이: 190\n",
      "Train 데이터 문장 1869의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1870의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1871의 토큰화된 길이: 66\n",
      "Train 데이터 문장 1872의 토큰화된 길이: 88\n",
      "Train 데이터 문장 1873의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1874의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1875의 토큰화된 길이: 75\n",
      "Train 데이터 문장 1876의 토큰화된 길이: 117\n",
      "Train 데이터 문장 1877의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1878의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1879의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1880의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1881의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1882의 토큰화된 길이: 111\n",
      "Train 데이터 문장 1883의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1884의 토큰화된 길이: 141\n",
      "Train 데이터 문장 1885의 토큰화된 길이: 233\n",
      "Train 데이터 문장 1886의 토큰화된 길이: 231\n",
      "Train 데이터 문장 1887의 토큰화된 길이: 96\n",
      "Train 데이터 문장 1888의 토큰화된 길이: 100\n",
      "Train 데이터 문장 1889의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1890의 토큰화된 길이: 93\n",
      "Train 데이터 문장 1891의 토큰화된 길이: 188\n",
      "Train 데이터 문장 1892의 토큰화된 길이: 113\n",
      "Train 데이터 문장 1893의 토큰화된 길이: 122\n",
      "Train 데이터 문장 1894의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1895의 토큰화된 길이: 110\n",
      "Train 데이터 문장 1896의 토큰화된 길이: 61\n",
      "Train 데이터 문장 1897의 토큰화된 길이: 61\n",
      "Train 데이터 문장 1898의 토큰화된 길이: 98\n",
      "Train 데이터 문장 1899의 토큰화된 길이: 280\n",
      "Train 데이터 문장 1900의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1901의 토큰화된 길이: 131\n",
      "Train 데이터 문장 1902의 토큰화된 길이: 129\n",
      "Train 데이터 문장 1903의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1904의 토큰화된 길이: 51\n",
      "Train 데이터 문장 1905의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1906의 토큰화된 길이: 87\n",
      "Train 데이터 문장 1907의 토큰화된 길이: 196\n",
      "Train 데이터 문장 1908의 토큰화된 길이: 140\n",
      "Train 데이터 문장 1909의 토큰화된 길이: 164\n",
      "Train 데이터 문장 1910의 토큰화된 길이: 68\n",
      "Train 데이터 문장 1911의 토큰화된 길이: 144\n",
      "Train 데이터 문장 1912의 토큰화된 길이: 171\n",
      "Train 데이터 문장 1913의 토큰화된 길이: 103\n",
      "Train 데이터 문장 1914의 토큰화된 길이: 155\n",
      "Train 데이터 문장 1915의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1916의 토큰화된 길이: 166\n",
      "Train 데이터 문장 1917의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1918의 토큰화된 길이: 196\n",
      "Train 데이터 문장 1919의 토큰화된 길이: 231\n",
      "Train 데이터 문장 1920의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1921의 토큰화된 길이: 157\n",
      "Train 데이터 문장 1922의 토큰화된 길이: 61\n",
      "Train 데이터 문장 1923의 토큰화된 길이: 139\n",
      "Train 데이터 문장 1924의 토큰화된 길이: 51\n",
      "Train 데이터 문장 1925의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1926의 토큰화된 길이: 288\n",
      "Train 데이터 문장 1927의 토큰화된 길이: 177\n",
      "Train 데이터 문장 1928의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1929의 토큰화된 길이: 125\n",
      "Train 데이터 문장 1930의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1931의 토큰화된 길이: 179\n",
      "Train 데이터 문장 1932의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1933의 토큰화된 길이: 159\n",
      "Train 데이터 문장 1934의 토큰화된 길이: 196\n",
      "Train 데이터 문장 1935의 토큰화된 길이: 145\n",
      "Train 데이터 문장 1936의 토큰화된 길이: 329\n",
      "Train 데이터 문장 1937의 토큰화된 길이: 144\n",
      "Train 데이터 문장 1938의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1939의 토큰화된 길이: 106\n",
      "Train 데이터 문장 1940의 토큰화된 길이: 203\n",
      "Train 데이터 문장 1941의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1942의 토큰화된 길이: 84\n",
      "Train 데이터 문장 1943의 토큰화된 길이: 133\n",
      "Train 데이터 문장 1944의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1945의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1946의 토큰화된 길이: 120\n",
      "Train 데이터 문장 1947의 토큰화된 길이: 156\n",
      "Train 데이터 문장 1948의 토큰화된 길이: 95\n",
      "Train 데이터 문장 1949의 토큰화된 길이: 112\n",
      "Train 데이터 문장 1950의 토큰화된 길이: 139\n",
      "Train 데이터 문장 1951의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1952의 토큰화된 길이: 126\n",
      "Train 데이터 문장 1953의 토큰화된 길이: 60\n",
      "Train 데이터 문장 1954의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1955의 토큰화된 길이: 278\n",
      "Train 데이터 문장 1956의 토큰화된 길이: 135\n",
      "Train 데이터 문장 1957의 토큰화된 길이: 94\n",
      "Train 데이터 문장 1958의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1959의 토큰화된 길이: 80\n",
      "Train 데이터 문장 1960의 토큰화된 길이: 63\n",
      "Train 데이터 문장 1961의 토큰화된 길이: 108\n",
      "Train 데이터 문장 1962의 토큰화된 길이: 109\n",
      "Train 데이터 문장 1963의 토큰화된 길이: 89\n",
      "Train 데이터 문장 1964의 토큰화된 길이: 148\n",
      "Train 데이터 문장 1965의 토큰화된 길이: 178\n",
      "Train 데이터 문장 1966의 토큰화된 길이: 104\n",
      "Train 데이터 문장 1967의 토큰화된 길이: 272\n",
      "Train 데이터 문장 1968의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1969의 토큰화된 길이: 118\n",
      "Train 데이터 문장 1970의 토큰화된 길이: 114\n",
      "Train 데이터 문장 1971의 토큰화된 길이: 76\n",
      "Train 데이터 문장 1972의 토큰화된 길이: 97\n",
      "Train 데이터 문장 1973의 토큰화된 길이: 159\n",
      "Train 데이터 문장 1974의 토큰화된 길이: 192\n",
      "Train 데이터 문장 1975의 토큰화된 길이: 142\n",
      "Train 데이터 문장 1976의 토큰화된 길이: 237\n",
      "Train 데이터 문장 1977의 토큰화된 길이: 116\n",
      "Train 데이터 문장 1978의 토큰화된 길이: 130\n",
      "Train 데이터 문장 1979의 토큰화된 길이: 131\n",
      "Train 데이터 문장 1980의 토큰화된 길이: 81\n",
      "Train 데이터 문장 1981의 토큰화된 길이: 209\n",
      "Train 데이터 문장 1982의 토큰화된 길이: 200\n",
      "Train 데이터 문장 1983의 토큰화된 길이: 322\n",
      "Train 데이터 문장 1984의 토큰화된 길이: 132\n",
      "Train 데이터 문장 1985의 토큰화된 길이: 90\n",
      "Train 데이터 문장 1986의 토큰화된 길이: 127\n",
      "Train 데이터 문장 1987의 토큰화된 길이: 77\n",
      "Train 데이터 문장 1988의 토큰화된 길이: 99\n",
      "Train 데이터 문장 1989의 토큰화된 길이: 102\n",
      "Train 데이터 문장 1990의 토큰화된 길이: 91\n",
      "Train 데이터 문장 1991의 토큰화된 길이: 262\n",
      "Train 데이터 문장 1992의 토큰화된 길이: 78\n",
      "Train 데이터 문장 1993의 토큰화된 길이: 55\n",
      "Train 데이터 문장 1994의 토큰화된 길이: 256\n",
      "Train 데이터 문장 1995의 토큰화된 길이: 67\n",
      "Train 데이터 문장 1996의 토큰화된 길이: 336\n",
      "Train 데이터 문장 1997의 토큰화된 길이: 134\n",
      "Train 데이터 문장 1998의 토큰화된 길이: 158\n",
      "Train 데이터 문장 1999의 토큰화된 길이: 76\n",
      "Train 데이터 문장 2000의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2001의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2002의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2003의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2004의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2005의 토큰화된 길이: 147\n",
      "Train 데이터 문장 2006의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2007의 토큰화된 길이: 177\n",
      "Train 데이터 문장 2008의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2009의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2010의 토큰화된 길이: 143\n",
      "Train 데이터 문장 2011의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2012의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2013의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2014의 토큰화된 길이: 241\n",
      "Train 데이터 문장 2015의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2016의 토큰화된 길이: 155\n",
      "Train 데이터 문장 2017의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2018의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2019의 토큰화된 길이: 49\n",
      "Train 데이터 문장 2020의 토큰화된 길이: 74\n",
      "Train 데이터 문장 2021의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2022의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2023의 토큰화된 길이: 153\n",
      "Train 데이터 문장 2024의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2025의 토큰화된 길이: 199\n",
      "Train 데이터 문장 2026의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2027의 토큰화된 길이: 226\n",
      "Train 데이터 문장 2028의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2029의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2030의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2031의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2032의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2033의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2034의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2035의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2036의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2037의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2038의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2039의 토큰화된 길이: 170\n",
      "Train 데이터 문장 2040의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2041의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2042의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2043의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2044의 토큰화된 길이: 195\n",
      "Train 데이터 문장 2045의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2046의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2047의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2048의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2049의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2050의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2051의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2052의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2053의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2054의 토큰화된 길이: 143\n",
      "Train 데이터 문장 2055의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2056의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2057의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2058의 토큰화된 길이: 199\n",
      "Train 데이터 문장 2059의 토큰화된 길이: 187\n",
      "Train 데이터 문장 2060의 토큰화된 길이: 33\n",
      "Train 데이터 문장 2061의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2062의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2063의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2064의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2065의 토큰화된 길이: 171\n",
      "Train 데이터 문장 2066의 토큰화된 길이: 196\n",
      "Train 데이터 문장 2067의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2068의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2069의 토큰화된 길이: 158\n",
      "Train 데이터 문장 2070의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2071의 토큰화된 길이: 153\n",
      "Train 데이터 문장 2072의 토큰화된 길이: 247\n",
      "Train 데이터 문장 2073의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2074의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2075의 토큰화된 길이: 128\n",
      "Train 데이터 문장 2076의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2077의 토큰화된 길이: 164\n",
      "Train 데이터 문장 2078의 토큰화된 길이: 194\n",
      "Train 데이터 문장 2079의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2080의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2081의 토큰화된 길이: 191\n",
      "Train 데이터 문장 2082의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2083의 토큰화된 길이: 205\n",
      "Train 데이터 문장 2084의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2085의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2086의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2087의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2088의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2089의 토큰화된 길이: 175\n",
      "Train 데이터 문장 2090의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2091의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2092의 토큰화된 길이: 321\n",
      "Train 데이터 문장 2093의 토큰화된 길이: 185\n",
      "Train 데이터 문장 2094의 토큰화된 길이: 65\n",
      "Train 데이터 문장 2095의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2096의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2097의 토큰화된 길이: 55\n",
      "Train 데이터 문장 2098의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2099의 토큰화된 길이: 129\n",
      "Train 데이터 문장 2100의 토큰화된 길이: 223\n",
      "Train 데이터 문장 2101의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2102의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2103의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2104의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2105의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2106의 토큰화된 길이: 276\n",
      "Train 데이터 문장 2107의 토큰화된 길이: 173\n",
      "Train 데이터 문장 2108의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2109의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2110의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2111의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2112의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2113의 토큰화된 길이: 191\n",
      "Train 데이터 문장 2114의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2115의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2116의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2117의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2118의 토큰화된 길이: 100\n",
      "Train 데이터 문장 2119의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2120의 토큰화된 길이: 250\n",
      "Train 데이터 문장 2121의 토큰화된 길이: 137\n",
      "Train 데이터 문장 2122의 토큰화된 길이: 166\n",
      "Train 데이터 문장 2123의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2124의 토큰화된 길이: 146\n",
      "Train 데이터 문장 2125의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2126의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2127의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2128의 토큰화된 길이: 152\n",
      "Train 데이터 문장 2129의 토큰화된 길이: 52\n",
      "Train 데이터 문장 2130의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2131의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2132의 토큰화된 길이: 198\n",
      "Train 데이터 문장 2133의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2134의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2135의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2136의 토큰화된 길이: 149\n",
      "Train 데이터 문장 2137의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2138의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2139의 토큰화된 길이: 100\n",
      "Train 데이터 문장 2140의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2141의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2142의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2143의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2144의 토큰화된 길이: 167\n",
      "Train 데이터 문장 2145의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2146의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2147의 토큰화된 길이: 164\n",
      "Train 데이터 문장 2148의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2149의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2150의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2151의 토큰화된 길이: 64\n",
      "Train 데이터 문장 2152의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2153의 토큰화된 길이: 194\n",
      "Train 데이터 문장 2154의 토큰화된 길이: 199\n",
      "Train 데이터 문장 2155의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2156의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2157의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2158의 토큰화된 길이: 337\n",
      "Train 데이터 문장 2159의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2160의 토큰화된 길이: 235\n",
      "Train 데이터 문장 2161의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2162의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2163의 토큰화된 길이: 74\n",
      "Train 데이터 문장 2164의 토큰화된 길이: 457\n",
      "Train 데이터 문장 2165의 토큰화된 길이: 203\n",
      "Train 데이터 문장 2166의 토큰화된 길이: 146\n",
      "Train 데이터 문장 2167의 토큰화된 길이: 94\n",
      "Train 데이터 문장 2168의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2169의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2170의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2171의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2172의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2173의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2174의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2175의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2176의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2177의 토큰화된 길이: 202\n",
      "Train 데이터 문장 2178의 토큰화된 길이: 361\n",
      "Train 데이터 문장 2179의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2180의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2181의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2182의 토큰화된 길이: 365\n",
      "Train 데이터 문장 2183의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2184의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2185의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2186의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2187의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2188의 토큰화된 길이: 166\n",
      "Train 데이터 문장 2189의 토큰화된 길이: 145\n",
      "Train 데이터 문장 2190의 토큰화된 길이: 417\n",
      "Train 데이터 문장 2191의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2192의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2193의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2194의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2195의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2196의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2197의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2198의 토큰화된 길이: 65\n",
      "Train 데이터 문장 2199의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2200의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2201의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2202의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2203의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2204의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2205의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2206의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2207의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2208의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2209의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2210의 토큰화된 길이: 169\n",
      "Train 데이터 문장 2211의 토큰화된 길이: 198\n",
      "Train 데이터 문장 2212의 토큰화된 길이: 145\n",
      "Train 데이터 문장 2213의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2214의 토큰화된 길이: 197\n",
      "Train 데이터 문장 2215의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2216의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2217의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2218의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2219의 토큰화된 길이: 214\n",
      "Train 데이터 문장 2220의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2221의 토큰화된 길이: 218\n",
      "Train 데이터 문장 2222의 토큰화된 길이: 215\n",
      "Train 데이터 문장 2223의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2224의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2225의 토큰화된 길이: 60\n",
      "Train 데이터 문장 2226의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2227의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2228의 토큰화된 길이: 43\n",
      "Train 데이터 문장 2229의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2230의 토큰화된 길이: 192\n",
      "Train 데이터 문장 2231의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2232의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2233의 토큰화된 길이: 151\n",
      "Train 데이터 문장 2234의 토큰화된 길이: 200\n",
      "Train 데이터 문장 2235의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2236의 토큰화된 길이: 249\n",
      "Train 데이터 문장 2237의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2238의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2239의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2240의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2241의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2242의 토큰화된 길이: 229\n",
      "Train 데이터 문장 2243의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2244의 토큰화된 길이: 59\n",
      "Train 데이터 문장 2245의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2246의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2247의 토큰화된 길이: 157\n",
      "Train 데이터 문장 2248의 토큰화된 길이: 366\n",
      "Train 데이터 문장 2249의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2250의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2251의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2252의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2253의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2254의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2255의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2256의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2257의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2258의 토큰화된 길이: 224\n",
      "Train 데이터 문장 2259의 토큰화된 길이: 224\n",
      "Train 데이터 문장 2260의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2261의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2262의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2263의 토큰화된 길이: 112\n",
      "Train 데이터 문장 2264의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2265의 토큰화된 길이: 215\n",
      "Train 데이터 문장 2266의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2267의 토큰화된 길이: 157\n",
      "Train 데이터 문장 2268의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2269의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2270의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2271의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2272의 토큰화된 길이: 55\n",
      "Train 데이터 문장 2273의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2274의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2275의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2276의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2277의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2278의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2279의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2280의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2281의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2282의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2283의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2284의 토큰화된 길이: 133\n",
      "Train 데이터 문장 2285의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2286의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2287의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2288의 토큰화된 길이: 149\n",
      "Train 데이터 문장 2289의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2290의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2291의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2292의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2293의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2294의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2295의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2296의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2297의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2298의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2299의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2300의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2301의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2302의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2303의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2304의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2305의 토큰화된 길이: 50\n",
      "Train 데이터 문장 2306의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2307의 토큰화된 길이: 56\n",
      "Train 데이터 문장 2308의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2309의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2310의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2311의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2312의 토큰화된 길이: 157\n",
      "Train 데이터 문장 2313의 토큰화된 길이: 322\n",
      "Train 데이터 문장 2314의 토큰화된 길이: 58\n",
      "Train 데이터 문장 2315의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2316의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2317의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2318의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2319의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2320의 토큰화된 길이: 49\n",
      "Train 데이터 문장 2321의 토큰화된 길이: 169\n",
      "Train 데이터 문장 2322의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2323의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2324의 토큰화된 길이: 167\n",
      "Train 데이터 문장 2325의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2326의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2327의 토큰화된 길이: 224\n",
      "Train 데이터 문장 2328의 토큰화된 길이: 219\n",
      "Train 데이터 문장 2329의 토큰화된 길이: 128\n",
      "Train 데이터 문장 2330의 토큰화된 길이: 155\n",
      "Train 데이터 문장 2331의 토큰화된 길이: 131\n",
      "Train 데이터 문장 2332의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2333의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2334의 토큰화된 길이: 303\n",
      "Train 데이터 문장 2335의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2336의 토큰화된 길이: 175\n",
      "Train 데이터 문장 2337의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2338의 토큰화된 길이: 308\n",
      "Train 데이터 문장 2339의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2340의 토큰화된 길이: 367\n",
      "Train 데이터 문장 2341의 토큰화된 길이: 164\n",
      "Train 데이터 문장 2342의 토큰화된 길이: 253\n",
      "Train 데이터 문장 2343의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2344의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2345의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2346의 토큰화된 길이: 146\n",
      "Train 데이터 문장 2347의 토큰화된 길이: 185\n",
      "Train 데이터 문장 2348의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2349의 토큰화된 길이: 330\n",
      "Train 데이터 문장 2350의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2351의 토큰화된 길이: 218\n",
      "Train 데이터 문장 2352의 토큰화된 길이: 140\n",
      "Train 데이터 문장 2353의 토큰화된 길이: 150\n",
      "Train 데이터 문장 2354의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2355의 토큰화된 길이: 375\n",
      "Train 데이터 문장 2356의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2357의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2358의 토큰화된 길이: 165\n",
      "Train 데이터 문장 2359의 토큰화된 길이: 57\n",
      "Train 데이터 문장 2360의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2361의 토큰화된 길이: 74\n",
      "Train 데이터 문장 2362의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2363의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2364의 토큰화된 길이: 129\n",
      "Train 데이터 문장 2365의 토큰화된 길이: 198\n",
      "Train 데이터 문장 2366의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2367의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2368의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2369의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2370의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2371의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2372의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2373의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2374의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2375의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2376의 토큰화된 길이: 196\n",
      "Train 데이터 문장 2377의 토큰화된 길이: 135\n",
      "Train 데이터 문장 2378의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2379의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2380의 토큰화된 길이: 62\n",
      "Train 데이터 문장 2381의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2382의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2383의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2384의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2385의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2386의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2387의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2388의 토큰화된 길이: 135\n",
      "Train 데이터 문장 2389의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2390의 토큰화된 길이: 166\n",
      "Train 데이터 문장 2391의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2392의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2393의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2394의 토큰화된 길이: 256\n",
      "Train 데이터 문장 2395의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2396의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2397의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2398의 토큰화된 길이: 233\n",
      "Train 데이터 문장 2399의 토큰화된 길이: 152\n",
      "Train 데이터 문장 2400의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2401의 토큰화된 길이: 131\n",
      "Train 데이터 문장 2402의 토큰화된 길이: 55\n",
      "Train 데이터 문장 2403의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2404의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2405의 토큰화된 길이: 180\n",
      "Train 데이터 문장 2406의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2407의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2408의 토큰화된 길이: 181\n",
      "Train 데이터 문장 2409의 토큰화된 길이: 373\n",
      "Train 데이터 문장 2410의 토큰화된 길이: 163\n",
      "Train 데이터 문장 2411의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2412의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2413의 토큰화된 길이: 53\n",
      "Train 데이터 문장 2414의 토큰화된 길이: 157\n",
      "Train 데이터 문장 2415의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2416의 토큰화된 길이: 76\n",
      "Train 데이터 문장 2417의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2418의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2419의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2420의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2421의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2422의 토큰화된 길이: 286\n",
      "Train 데이터 문장 2423의 토큰화된 길이: 59\n",
      "Train 데이터 문장 2424의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2425의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2426의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2427의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2428의 토큰화된 길이: 152\n",
      "Train 데이터 문장 2429의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2430의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2431의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2432의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2433의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2434의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2435의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2436의 토큰화된 길이: 62\n",
      "Train 데이터 문장 2437의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2438의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2439의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2440의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2441의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2442의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2443의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2444의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2445의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2446의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2447의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2448의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2449의 토큰화된 길이: 198\n",
      "Train 데이터 문장 2450의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2451의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2452의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2453의 토큰화된 길이: 126\n",
      "Train 데이터 문장 2454의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2455의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2456의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2457의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2458의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2459의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2460의 토큰화된 길이: 216\n",
      "Train 데이터 문장 2461의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2462의 토큰화된 길이: 170\n",
      "Train 데이터 문장 2463의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2464의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2465의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2466의 토큰화된 길이: 135\n",
      "Train 데이터 문장 2467의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2468의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2469의 토큰화된 길이: 248\n",
      "Train 데이터 문장 2470의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2471의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2472의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2473의 토큰화된 길이: 143\n",
      "Train 데이터 문장 2474의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2475의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2476의 토큰화된 길이: 323\n",
      "Train 데이터 문장 2477의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2478의 토큰화된 길이: 234\n",
      "Train 데이터 문장 2479의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2480의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2481의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2482의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2483의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2484의 토큰화된 길이: 146\n",
      "Train 데이터 문장 2485의 토큰화된 길이: 147\n",
      "Train 데이터 문장 2486의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2487의 토큰화된 길이: 174\n",
      "Train 데이터 문장 2488의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2489의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2490의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2491의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2492의 토큰화된 길이: 263\n",
      "Train 데이터 문장 2493의 토큰화된 길이: 147\n",
      "Train 데이터 문장 2494의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2495의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2496의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2497의 토큰화된 길이: 179\n",
      "Train 데이터 문장 2498의 토큰화된 길이: 126\n",
      "Train 데이터 문장 2499의 토큰화된 길이: 163\n",
      "Train 데이터 문장 2500의 토큰화된 길이: 94\n",
      "Train 데이터 문장 2501의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2502의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2503의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2504의 토큰화된 길이: 191\n",
      "Train 데이터 문장 2505의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2506의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2507의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2508의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2509의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2510의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2511의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2512의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2513의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2514의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2515의 토큰화된 길이: 135\n",
      "Train 데이터 문장 2516의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2517의 토큰화된 길이: 131\n",
      "Train 데이터 문장 2518의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2519의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2520의 토큰화된 길이: 237\n",
      "Train 데이터 문장 2521의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2522의 토큰화된 길이: 204\n",
      "Train 데이터 문장 2523의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2524의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2525의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2526의 토큰화된 길이: 171\n",
      "Train 데이터 문장 2527의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2528의 토큰화된 길이: 60\n",
      "Train 데이터 문장 2529의 토큰화된 길이: 170\n",
      "Train 데이터 문장 2530의 토큰화된 길이: 165\n",
      "Train 데이터 문장 2531의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2532의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2533의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2534의 토큰화된 길이: 58\n",
      "Train 데이터 문장 2535의 토큰화된 길이: 133\n",
      "Train 데이터 문장 2536의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2537의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2538의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2539의 토큰화된 길이: 188\n",
      "Train 데이터 문장 2540의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2541의 토큰화된 길이: 344\n",
      "Train 데이터 문장 2542의 토큰화된 길이: 173\n",
      "Train 데이터 문장 2543의 토큰화된 길이: 59\n",
      "Train 데이터 문장 2544의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2545의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2546의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2547의 토큰화된 길이: 152\n",
      "Train 데이터 문장 2548의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2549의 토큰화된 길이: 243\n",
      "Train 데이터 문장 2550의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2551의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2552의 토큰화된 길이: 145\n",
      "Train 데이터 문장 2553의 토큰화된 길이: 171\n",
      "Train 데이터 문장 2554의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2555의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2556의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2557의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2558의 토큰화된 길이: 100\n",
      "Train 데이터 문장 2559의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2560의 토큰화된 길이: 140\n",
      "Train 데이터 문장 2561의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2562의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2563의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2564의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2565의 토큰화된 길이: 149\n",
      "Train 데이터 문장 2566의 토큰화된 길이: 158\n",
      "Train 데이터 문장 2567의 토큰화된 길이: 112\n",
      "Train 데이터 문장 2568의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2569의 토큰화된 길이: 144\n",
      "Train 데이터 문장 2570의 토큰화된 길이: 290\n",
      "Train 데이터 문장 2571의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2572의 토큰화된 길이: 226\n",
      "Train 데이터 문장 2573의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2574의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2575의 토큰화된 길이: 137\n",
      "Train 데이터 문장 2576의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2577의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2578의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2579의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2580의 토큰화된 길이: 74\n",
      "Train 데이터 문장 2581의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2582의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2583의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2584의 토큰화된 길이: 243\n",
      "Train 데이터 문장 2585의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2586의 토큰화된 길이: 131\n",
      "Train 데이터 문장 2587의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2588의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2589의 토큰화된 길이: 160\n",
      "Train 데이터 문장 2590의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2591의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2592의 토큰화된 길이: 164\n",
      "Train 데이터 문장 2593의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2594의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2595의 토큰화된 길이: 128\n",
      "Train 데이터 문장 2596의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2597의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2598의 토큰화된 길이: 165\n",
      "Train 데이터 문장 2599의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2600의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2601의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2602의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2603의 토큰화된 길이: 194\n",
      "Train 데이터 문장 2604의 토큰화된 길이: 65\n",
      "Train 데이터 문장 2605의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2606의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2607의 토큰화된 길이: 200\n",
      "Train 데이터 문장 2608의 토큰화된 길이: 74\n",
      "Train 데이터 문장 2609의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2610의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2611의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2612의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2613의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2614의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2615의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2616의 토큰화된 길이: 160\n",
      "Train 데이터 문장 2617의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2618의 토큰화된 길이: 226\n",
      "Train 데이터 문장 2619의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2620의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2621의 토큰화된 길이: 98\n",
      "Train 데이터 문장 2622의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2623의 토큰화된 길이: 212\n",
      "Train 데이터 문장 2624의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2625의 토큰화된 길이: 260\n",
      "Train 데이터 문장 2626의 토큰화된 길이: 191\n",
      "Train 데이터 문장 2627의 토큰화된 길이: 194\n",
      "Train 데이터 문장 2628의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2629의 토큰화된 길이: 54\n",
      "Train 데이터 문장 2630의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2631의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2632의 토큰화된 길이: 354\n",
      "Train 데이터 문장 2633의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2634의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2635의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2636의 토큰화된 길이: 137\n",
      "Train 데이터 문장 2637의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2638의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2639의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2640의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2641의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2642의 토큰화된 길이: 57\n",
      "Train 데이터 문장 2643의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2644의 토큰화된 길이: 185\n",
      "Train 데이터 문장 2645의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2646의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2647의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2648의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2649의 토큰화된 길이: 180\n",
      "Train 데이터 문장 2650의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2651의 토큰화된 길이: 321\n",
      "Train 데이터 문장 2652의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2653의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2654의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2655의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2656의 토큰화된 길이: 159\n",
      "Train 데이터 문장 2657의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2658의 토큰화된 길이: 379\n",
      "Train 데이터 문장 2659의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2660의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2661의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2662의 토큰화된 길이: 147\n",
      "Train 데이터 문장 2663의 토큰화된 길이: 101\n",
      "Train 데이터 문장 2664의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2665의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2666의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2667의 토큰화된 길이: 228\n",
      "Train 데이터 문장 2668의 토큰화된 길이: 165\n",
      "Train 데이터 문장 2669의 토큰화된 길이: 42\n",
      "Train 데이터 문장 2670의 토큰화된 길이: 61\n",
      "Train 데이터 문장 2671의 토큰화된 길이: 185\n",
      "Train 데이터 문장 2672의 토큰화된 길이: 131\n",
      "Train 데이터 문장 2673의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2674의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2675의 토큰화된 길이: 303\n",
      "Train 데이터 문장 2676의 토큰화된 길이: 261\n",
      "Train 데이터 문장 2677의 토큰화된 길이: 245\n",
      "Train 데이터 문장 2678의 토큰화된 길이: 51\n",
      "Train 데이터 문장 2679의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2680의 토큰화된 길이: 121\n",
      "Train 데이터 문장 2681의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2682의 토큰화된 길이: 174\n",
      "Train 데이터 문장 2683의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2684의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2685의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2686의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2687의 토큰화된 길이: 186\n",
      "Train 데이터 문장 2688의 토큰화된 길이: 185\n",
      "Train 데이터 문장 2689의 토큰화된 길이: 137\n",
      "Train 데이터 문장 2690의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2691의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2692의 토큰화된 길이: 150\n",
      "Train 데이터 문장 2693의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2694의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2695의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2696의 토큰화된 길이: 76\n",
      "Train 데이터 문장 2697의 토큰화된 길이: 124\n",
      "Train 데이터 문장 2698의 토큰화된 길이: 126\n",
      "Train 데이터 문장 2699의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2700의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2701의 토큰화된 길이: 129\n",
      "Train 데이터 문장 2702의 토큰화된 길이: 231\n",
      "Train 데이터 문장 2703의 토큰화된 길이: 209\n",
      "Train 데이터 문장 2704의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2705의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2706의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2707의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2708의 토큰화된 길이: 162\n",
      "Train 데이터 문장 2709의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2710의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2711의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2712의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2713의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2714의 토큰화된 길이: 56\n",
      "Train 데이터 문장 2715의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2716의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2717의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2718의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2719의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2720의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2721의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2722의 토큰화된 길이: 183\n",
      "Train 데이터 문장 2723의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2724의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2725의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2726의 토큰화된 길이: 93\n",
      "Train 데이터 문장 2727의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2728의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2729의 토큰화된 길이: 409\n",
      "Train 데이터 문장 2730의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2731의 토큰화된 길이: 133\n",
      "Train 데이터 문장 2732의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2733의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2734의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2735의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2736의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2737의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2738의 토큰화된 길이: 149\n",
      "Train 데이터 문장 2739의 토큰화된 길이: 168\n",
      "Train 데이터 문장 2740의 토큰화된 길이: 177\n",
      "Train 데이터 문장 2741의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2742의 토큰화된 길이: 265\n",
      "Train 데이터 문장 2743의 토큰화된 길이: 106\n",
      "Train 데이터 문장 2744의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2745의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2746의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2747의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2748의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2749의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2750의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2751의 토큰화된 길이: 112\n",
      "Train 데이터 문장 2752의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2753의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2754의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2755의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2756의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2757의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2758의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2759의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2760의 토큰화된 길이: 76\n",
      "Train 데이터 문장 2761의 토큰화된 길이: 157\n",
      "Train 데이터 문장 2762의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2763의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2764의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2765의 토큰화된 길이: 153\n",
      "Train 데이터 문장 2766의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2767의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2768의 토큰화된 길이: 171\n",
      "Train 데이터 문장 2769의 토큰화된 길이: 167\n",
      "Train 데이터 문장 2770의 토큰화된 길이: 193\n",
      "Train 데이터 문장 2771의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2772의 토큰화된 길이: 235\n",
      "Train 데이터 문장 2773의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2774의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2775의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2776의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2777의 토큰화된 길이: 52\n",
      "Train 데이터 문장 2778의 토큰화된 길이: 76\n",
      "Train 데이터 문장 2779의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2780의 토큰화된 길이: 176\n",
      "Train 데이터 문장 2781의 토큰화된 길이: 256\n",
      "Train 데이터 문장 2782의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2783의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2784의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2785의 토큰화된 길이: 129\n",
      "Train 데이터 문장 2786의 토큰화된 길이: 51\n",
      "Train 데이터 문장 2787의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2788의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2789의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2790의 토큰화된 길이: 64\n",
      "Train 데이터 문장 2791의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2792의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2793의 토큰화된 길이: 163\n",
      "Train 데이터 문장 2794의 토큰화된 길이: 176\n",
      "Train 데이터 문장 2795의 토큰화된 길이: 46\n",
      "Train 데이터 문장 2796의 토큰화된 길이: 153\n",
      "Train 데이터 문장 2797의 토큰화된 길이: 158\n",
      "Train 데이터 문장 2798의 토큰화된 길이: 234\n",
      "Train 데이터 문장 2799의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2800의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2801의 토큰화된 길이: 167\n",
      "Train 데이터 문장 2802의 토큰화된 길이: 173\n",
      "Train 데이터 문장 2803의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2804의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2805의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2806의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2807의 토큰화된 길이: 178\n",
      "Train 데이터 문장 2808의 토큰화된 길이: 38\n",
      "Train 데이터 문장 2809의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2810의 토큰화된 길이: 112\n",
      "Train 데이터 문장 2811의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2812의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2813의 토큰화된 길이: 161\n",
      "Train 데이터 문장 2814의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2815의 토큰화된 길이: 112\n",
      "Train 데이터 문장 2816의 토큰화된 길이: 307\n",
      "Train 데이터 문장 2817의 토큰화된 길이: 96\n",
      "Train 데이터 문장 2818의 토큰화된 길이: 97\n",
      "Train 데이터 문장 2819의 토큰화된 길이: 144\n",
      "Train 데이터 문장 2820의 토큰화된 길이: 47\n",
      "Train 데이터 문장 2821의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2822의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2823의 토큰화된 길이: 395\n",
      "Train 데이터 문장 2824의 토큰화된 길이: 183\n",
      "Train 데이터 문장 2825의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2826의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2827의 토큰화된 길이: 216\n",
      "Train 데이터 문장 2828의 토큰화된 길이: 91\n",
      "Train 데이터 문장 2829의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2830의 토큰화된 길이: 136\n",
      "Train 데이터 문장 2831의 토큰화된 길이: 213\n",
      "Train 데이터 문장 2832의 토큰화된 길이: 286\n",
      "Train 데이터 문장 2833의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2834의 토큰화된 길이: 113\n",
      "Train 데이터 문장 2835의 토큰화된 길이: 84\n",
      "Train 데이터 문장 2836의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2837의 토큰화된 길이: 144\n",
      "Train 데이터 문장 2838의 토큰화된 길이: 78\n",
      "Train 데이터 문장 2839의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2840의 토큰화된 길이: 58\n",
      "Train 데이터 문장 2841의 토큰화된 길이: 128\n",
      "Train 데이터 문장 2842의 토큰화된 길이: 148\n",
      "Train 데이터 문장 2843의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2844의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2845의 토큰화된 길이: 143\n",
      "Train 데이터 문장 2846의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2847의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2848의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2849의 토큰화된 길이: 243\n",
      "Train 데이터 문장 2850의 토큰화된 길이: 141\n",
      "Train 데이터 문장 2851의 토큰화된 길이: 306\n",
      "Train 데이터 문장 2852의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2853의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2854의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2855의 토큰화된 길이: 48\n",
      "Train 데이터 문장 2856의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2857의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2858의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2859의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2860의 토큰화된 길이: 156\n",
      "Train 데이터 문장 2861의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2862의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2863의 토큰화된 길이: 100\n",
      "Train 데이터 문장 2864의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2865의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2866의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2867의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2868의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2869의 토큰화된 길이: 63\n",
      "Train 데이터 문장 2870의 토큰화된 길이: 71\n",
      "Train 데이터 문장 2871의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2872의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2873의 토큰화된 길이: 284\n",
      "Train 데이터 문장 2874의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2875의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2876의 토큰화된 길이: 151\n",
      "Train 데이터 문장 2877의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2878의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2879의 토큰화된 길이: 79\n",
      "Train 데이터 문장 2880의 토큰화된 길이: 47\n",
      "Train 데이터 문장 2881의 토큰화된 길이: 142\n",
      "Train 데이터 문장 2882의 토큰화된 길이: 138\n",
      "Train 데이터 문장 2883의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2884의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2885의 토큰화된 길이: 70\n",
      "Train 데이터 문장 2886의 토큰화된 길이: 152\n",
      "Train 데이터 문장 2887의 토큰화된 길이: 103\n",
      "Train 데이터 문장 2888의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2889의 토큰화된 길이: 155\n",
      "Train 데이터 문장 2890의 토큰화된 길이: 44\n",
      "Train 데이터 문장 2891의 토큰화된 길이: 94\n",
      "Train 데이터 문장 2892의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2893의 토큰화된 길이: 213\n",
      "Train 데이터 문장 2894의 토큰화된 길이: 88\n",
      "Train 데이터 문장 2895의 토큰화된 길이: 128\n",
      "Train 데이터 문장 2896의 토큰화된 길이: 75\n",
      "Train 데이터 문장 2897의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2898의 토큰화된 길이: 283\n",
      "Train 데이터 문장 2899의 토큰화된 길이: 109\n",
      "Train 데이터 문장 2900의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2901의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2902의 토큰화된 길이: 114\n",
      "Train 데이터 문장 2903의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2904의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2905의 토큰화된 길이: 180\n",
      "Train 데이터 문장 2906의 토큰화된 길이: 181\n",
      "Train 데이터 문장 2907의 토큰화된 길이: 150\n",
      "Train 데이터 문장 2908의 토큰화된 길이: 59\n",
      "Train 데이터 문장 2909의 토큰화된 길이: 151\n",
      "Train 데이터 문장 2910의 토큰화된 길이: 66\n",
      "Train 데이터 문장 2911의 토큰화된 길이: 184\n",
      "Train 데이터 문장 2912의 토큰화된 길이: 58\n",
      "Train 데이터 문장 2913의 토큰화된 길이: 146\n",
      "Train 데이터 문장 2914의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2915의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2916의 토큰화된 길이: 173\n",
      "Train 데이터 문장 2917의 토큰화된 길이: 67\n",
      "Train 데이터 문장 2918의 토큰화된 길이: 324\n",
      "Train 데이터 문장 2919의 토큰화된 길이: 69\n",
      "Train 데이터 문장 2920의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2921의 토큰화된 길이: 134\n",
      "Train 데이터 문장 2922의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2923의 토큰화된 길이: 284\n",
      "Train 데이터 문장 2924의 토큰화된 길이: 90\n",
      "Train 데이터 문장 2925의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2926의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2927의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2928의 토큰화된 길이: 160\n",
      "Train 데이터 문장 2929의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2930의 토큰화된 길이: 188\n",
      "Train 데이터 문장 2931의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2932의 토큰화된 길이: 100\n",
      "Train 데이터 문장 2933의 토큰화된 길이: 77\n",
      "Train 데이터 문장 2934의 토큰화된 길이: 108\n",
      "Train 데이터 문장 2935의 토큰화된 길이: 99\n",
      "Train 데이터 문장 2936의 토큰화된 길이: 133\n",
      "Train 데이터 문장 2937의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2938의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2939의 토큰화된 길이: 54\n",
      "Train 데이터 문장 2940의 토큰화된 길이: 110\n",
      "Train 데이터 문장 2941의 토큰화된 길이: 240\n",
      "Train 데이터 문장 2942의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2943의 토큰화된 길이: 37\n",
      "Train 데이터 문장 2944의 토큰화된 길이: 176\n",
      "Train 데이터 문장 2945의 토큰화된 길이: 73\n",
      "Train 데이터 문장 2946의 토큰화된 길이: 119\n",
      "Train 데이터 문장 2947의 토큰화된 길이: 198\n",
      "Train 데이터 문장 2948의 토큰화된 길이: 118\n",
      "Train 데이터 문장 2949의 토큰화된 길이: 122\n",
      "Train 데이터 문장 2950의 토큰화된 길이: 176\n",
      "Train 데이터 문장 2951의 토큰화된 길이: 116\n",
      "Train 데이터 문장 2952의 토큰화된 길이: 115\n",
      "Train 데이터 문장 2953의 토큰화된 길이: 167\n",
      "Train 데이터 문장 2954의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2955의 토큰화된 길이: 132\n",
      "Train 데이터 문장 2956의 토큰화된 길이: 105\n",
      "Train 데이터 문장 2957의 토큰화된 길이: 86\n",
      "Train 데이터 문장 2958의 토큰화된 길이: 38\n",
      "Train 데이터 문장 2959의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2960의 토큰화된 길이: 139\n",
      "Train 데이터 문장 2961의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2962의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2963의 토큰화된 길이: 127\n",
      "Train 데이터 문장 2964의 토큰화된 길이: 163\n",
      "Train 데이터 문장 2965의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2966의 토큰화된 길이: 117\n",
      "Train 데이터 문장 2967의 토큰화된 길이: 81\n",
      "Train 데이터 문장 2968의 토큰화된 길이: 47\n",
      "Train 데이터 문장 2969의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2970의 토큰화된 길이: 107\n",
      "Train 데이터 문장 2971의 토큰화된 길이: 94\n",
      "Train 데이터 문장 2972의 토큰화된 길이: 183\n",
      "Train 데이터 문장 2973의 토큰화된 길이: 120\n",
      "Train 데이터 문장 2974의 토큰화된 길이: 178\n",
      "Train 데이터 문장 2975의 토큰화된 길이: 92\n",
      "Train 데이터 문장 2976의 토큰화된 길이: 130\n",
      "Train 데이터 문장 2977의 토큰화된 길이: 41\n",
      "Train 데이터 문장 2978의 토큰화된 길이: 123\n",
      "Train 데이터 문장 2979의 토큰화된 길이: 35\n",
      "Train 데이터 문장 2980의 토큰화된 길이: 195\n",
      "Train 데이터 문장 2981의 토큰화된 길이: 102\n",
      "Train 데이터 문장 2982의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2983의 토큰화된 길이: 83\n",
      "Train 데이터 문장 2984의 토큰화된 길이: 95\n",
      "Train 데이터 문장 2985의 토큰화된 길이: 85\n",
      "Train 데이터 문장 2986의 토큰화된 길이: 94\n",
      "Train 데이터 문장 2987의 토큰화된 길이: 89\n",
      "Train 데이터 문장 2988의 토큰화된 길이: 111\n",
      "Train 데이터 문장 2989의 토큰화된 길이: 82\n",
      "Train 데이터 문장 2990의 토큰화된 길이: 80\n",
      "Train 데이터 문장 2991의 토큰화된 길이: 125\n",
      "Train 데이터 문장 2992의 토큰화된 길이: 87\n",
      "Train 데이터 문장 2993의 토큰화된 길이: 104\n",
      "Train 데이터 문장 2994의 토큰화된 길이: 166\n",
      "Train 데이터 문장 2995의 토큰화된 길이: 72\n",
      "Train 데이터 문장 2996의 토큰화된 길이: 154\n",
      "Train 데이터 문장 2997의 토큰화된 길이: 189\n",
      "Train 데이터 문장 2998의 토큰화된 길이: 68\n",
      "Train 데이터 문장 2999의 토큰화된 길이: 268\n",
      "Train 데이터 문장 3000의 토큰화된 길이: 85\n",
      "Train 데이터 문장 3001의 토큰화된 길이: 109\n",
      "Train 데이터 문장 3002의 토큰화된 길이: 199\n",
      "Train 데이터 문장 3003의 토큰화된 길이: 71\n",
      "Train 데이터 문장 3004의 토큰화된 길이: 94\n",
      "Train 데이터 문장 3005의 토큰화된 길이: 108\n",
      "Train 데이터 문장 3006의 토큰화된 길이: 94\n",
      "Train 데이터 문장 3007의 토큰화된 길이: 63\n",
      "Train 데이터 문장 3008의 토큰화된 길이: 250\n",
      "Train 데이터 문장 3009의 토큰화된 길이: 184\n",
      "Train 데이터 문장 3010의 토큰화된 길이: 320\n",
      "Train 데이터 문장 3011의 토큰화된 길이: 116\n",
      "Train 데이터 문장 3012의 토큰화된 길이: 92\n",
      "Train 데이터 문장 3013의 토큰화된 길이: 278\n",
      "Train 데이터 문장 3014의 토큰화된 길이: 76\n",
      "Train 데이터 문장 3015의 토큰화된 길이: 101\n",
      "Train 데이터 문장 3016의 토큰화된 길이: 76\n",
      "Train 데이터 문장 3017의 토큰화된 길이: 249\n",
      "Train 데이터 문장 3018의 토큰화된 길이: 138\n",
      "Train 데이터 문장 3019의 토큰화된 길이: 130\n",
      "Train 데이터 문장 3020의 토큰화된 길이: 71\n",
      "Train 데이터 문장 3021의 토큰화된 길이: 151\n",
      "Train 데이터 문장 3022의 토큰화된 길이: 132\n",
      "Train 데이터 문장 3023의 토큰화된 길이: 157\n",
      "Train 데이터 문장 3024의 토큰화된 길이: 67\n",
      "Train 데이터 문장 3025의 토큰화된 길이: 169\n",
      "Train 데이터 문장 3026의 토큰화된 길이: 324\n",
      "Train 데이터 문장 3027의 토큰화된 길이: 92\n",
      "Train 데이터 문장 3028의 토큰화된 길이: 92\n",
      "Train 데이터 문장 3029의 토큰화된 길이: 238\n",
      "Train 데이터 문장 3030의 토큰화된 길이: 73\n",
      "Train 데이터 문장 3031의 토큰화된 길이: 115\n",
      "Train 데이터 문장 3032의 토큰화된 길이: 177\n",
      "Train 데이터 문장 3033의 토큰화된 길이: 100\n",
      "Train 데이터 문장 3034의 토큰화된 길이: 192\n",
      "Train 데이터 문장 3035의 토큰화된 길이: 122\n",
      "Train 데이터 문장 3036의 토큰화된 길이: 122\n",
      "Train 데이터 문장 3037의 토큰화된 길이: 129\n",
      "Train 데이터 문장 3038의 토큰화된 길이: 178\n",
      "Train 데이터 문장 3039의 토큰화된 길이: 75\n",
      "Train 데이터 문장 3040의 토큰화된 길이: 124\n",
      "Train 데이터 문장 3041의 토큰화된 길이: 121\n",
      "Train 데이터 문장 3042의 토큰화된 길이: 72\n",
      "Train 데이터 문장 3043의 토큰화된 길이: 109\n",
      "Train 데이터 문장 3044의 토큰화된 길이: 208\n",
      "Train 데이터 문장 3045의 토큰화된 길이: 169\n",
      "Train 데이터 문장 3046의 토큰화된 길이: 164\n",
      "Train 데이터 문장 3047의 토큰화된 길이: 84\n",
      "Train 데이터 문장 3048의 토큰화된 길이: 95\n",
      "Train 데이터 문장 3049의 토큰화된 길이: 79\n",
      "Train 데이터 문장 3050의 토큰화된 길이: 169\n",
      "Train 데이터 문장 3051의 토큰화된 길이: 69\n",
      "Train 데이터 문장 3052의 토큰화된 길이: 66\n",
      "Train 데이터 문장 3053의 토큰화된 길이: 91\n",
      "Train 데이터 문장 3054의 토큰화된 길이: 140\n",
      "Train 데이터 문장 3055의 토큰화된 길이: 150\n",
      "Train 데이터 문장 3056의 토큰화된 길이: 77\n",
      "Train 데이터 문장 3057의 토큰화된 길이: 108\n",
      "Train 데이터 문장 3058의 토큰화된 길이: 124\n",
      "Train 데이터 문장 3059의 토큰화된 길이: 89\n",
      "Train 데이터 문장 3060의 토큰화된 길이: 81\n",
      "Train 데이터 문장 3061의 토큰화된 길이: 72\n",
      "Train 데이터 문장 3062의 토큰화된 길이: 91\n",
      "Train 데이터 문장 3063의 토큰화된 길이: 84\n",
      "Train 데이터 문장 3064의 토큰화된 길이: 88\n",
      "Train 데이터 문장 3065의 토큰화된 길이: 177\n",
      "Train 데이터 문장 3066의 토큰화된 길이: 111\n",
      "Train 데이터 문장 3067의 토큰화된 길이: 107\n",
      "Train 데이터 문장 3068의 토큰화된 길이: 65\n",
      "Train 데이터 문장 3069의 토큰화된 길이: 236\n",
      "Train 데이터 문장 3070의 토큰화된 길이: 217\n",
      "Train 데이터 문장 3071의 토큰화된 길이: 105\n",
      "Train 데이터 문장 3072의 토큰화된 길이: 126\n",
      "Train 데이터 문장 3073의 토큰화된 길이: 149\n",
      "Train 데이터 문장 3074의 토큰화된 길이: 93\n",
      "Train 데이터 문장 3075의 토큰화된 길이: 227\n",
      "Train 데이터 문장 3076의 토큰화된 길이: 257\n",
      "Train 데이터 문장 3077의 토큰화된 길이: 78\n",
      "Train 데이터 문장 3078의 토큰화된 길이: 290\n",
      "Train 데이터 문장 3079의 토큰화된 길이: 91\n",
      "Train 데이터 문장 3080의 토큰화된 길이: 154\n",
      "Train 데이터 문장 3081의 토큰화된 길이: 215\n",
      "Train 데이터 문장 3082의 토큰화된 길이: 108\n",
      "Train 데이터 문장 3083의 토큰화된 길이: 195\n",
      "Train 데이터 문장 3084의 토큰화된 길이: 94\n",
      "Train 데이터 문장 3085의 토큰화된 길이: 98\n",
      "Train 데이터 문장 3086의 토큰화된 길이: 174\n",
      "Train 데이터 문장 3087의 토큰화된 길이: 69\n",
      "Train 데이터 문장 3088의 토큰화된 길이: 96\n",
      "Train 데이터 문장 3089의 토큰화된 길이: 104\n",
      "Train 데이터 문장 3090의 토큰화된 길이: 106\n",
      "Train 데이터 문장 3091의 토큰화된 길이: 63\n",
      "Train 데이터 문장 3092의 토큰화된 길이: 90\n",
      "Train 데이터 문장 3093의 토큰화된 길이: 58\n",
      "Train 데이터 문장 3094의 토큰화된 길이: 182\n",
      "Train 데이터 문장 3095의 토큰화된 길이: 68\n",
      "Train 데이터 문장 3096의 토큰화된 길이: 79\n",
      "Train 데이터 문장 3097의 토큰화된 길이: 87\n",
      "Train 데이터 문장 3098의 토큰화된 길이: 132\n",
      "Train 데이터 문장 3099의 토큰화된 길이: 125\n",
      "Train 데이터 문장 3100의 토큰화된 길이: 131\n",
      "Train 데이터 문장 3101의 토큰화된 길이: 151\n",
      "Train 데이터 문장 3102의 토큰화된 길이: 152\n",
      "Train 데이터 문장 3103의 토큰화된 길이: 128\n",
      "Train 데이터 문장 3104의 토큰화된 길이: 159\n",
      "Train 데이터 문장 3105의 토큰화된 길이: 130\n",
      "Train 데이터 문장 3106의 토큰화된 길이: 64\n",
      "Train 데이터 문장 3107의 토큰화된 길이: 75\n",
      "Train 데이터 문장 3108의 토큰화된 길이: 112\n",
      "Train 데이터 문장 3109의 토큰화된 길이: 105\n",
      "Train 데이터 문장 3110의 토큰화된 길이: 135\n",
      "Train 데이터 문장 3111의 토큰화된 길이: 72\n",
      "Train 데이터 문장 3112의 토큰화된 길이: 107\n",
      "Train 데이터 문장 3113의 토큰화된 길이: 82\n",
      "Train 데이터 문장 3114의 토큰화된 길이: 82\n",
      "Train 데이터 문장 3115의 토큰화된 길이: 108\n",
      "Train 데이터 문장 3116의 토큰화된 길이: 56\n",
      "Train 데이터 문장 3117의 토큰화된 길이: 109\n",
      "Train 데이터 문장 3118의 토큰화된 길이: 142\n",
      "Train 데이터 문장 3119의 토큰화된 길이: 125\n",
      "Train 데이터 문장 3120의 토큰화된 길이: 77\n",
      "Train 데이터 문장 3121의 토큰화된 길이: 151\n",
      "Train 데이터 문장 3122의 토큰화된 길이: 66\n",
      "Train 데이터 문장 3123의 토큰화된 길이: 44\n",
      "Train 데이터 문장 3124의 토큰화된 길이: 98\n",
      "Train 데이터 문장 3125의 토큰화된 길이: 118\n",
      "Train 데이터 문장 3126의 토큰화된 길이: 87\n",
      "Train 데이터 문장 3127의 토큰화된 길이: 136\n",
      "Train 데이터 문장 3128의 토큰화된 길이: 167\n",
      "Train 데이터 문장 3129의 토큰화된 길이: 90\n",
      "Train 데이터 문장 3130의 토큰화된 길이: 97\n",
      "Train 데이터 문장 3131의 토큰화된 길이: 101\n",
      "Train 데이터 문장 3132의 토큰화된 길이: 144\n",
      "Train 데이터 문장 3133의 토큰화된 길이: 173\n",
      "Train 데이터 문장 3134의 토큰화된 길이: 97\n",
      "Train 데이터 문장 3135의 토큰화된 길이: 66\n",
      "Train 데이터 문장 3136의 토큰화된 길이: 90\n",
      "Train 데이터 문장 3137의 토큰화된 길이: 124\n",
      "Train 데이터 문장 3138의 토큰화된 길이: 137\n",
      "Train 데이터 문장 3139의 토큰화된 길이: 136\n",
      "Train 데이터 문장 3140의 토큰화된 길이: 83\n",
      "Train 데이터 문장 3141의 토큰화된 길이: 155\n",
      "Train 데이터 문장 3142의 토큰화된 길이: 133\n",
      "Train 데이터 문장 3143의 토큰화된 길이: 146\n",
      "Train 데이터 문장 3144의 토큰화된 길이: 93\n",
      "Train 데이터 문장 3145의 토큰화된 길이: 260\n",
      "Train 데이터 문장 3146의 토큰화된 길이: 104\n",
      "Train 데이터 문장 3147의 토큰화된 길이: 74\n",
      "Train 데이터 문장 3148의 토큰화된 길이: 104\n",
      "Train 데이터 문장 3149의 토큰화된 길이: 157\n",
      "Train 데이터 문장 3150의 토큰화된 길이: 185\n",
      "Train 데이터 문장 3151의 토큰화된 길이: 88\n",
      "Train 데이터 문장 3152의 토큰화된 길이: 155\n",
      "Train 데이터 문장 3153의 토큰화된 길이: 132\n",
      "Train 데이터 문장 3154의 토큰화된 길이: 78\n",
      "Train 데이터 문장 3155의 토큰화된 길이: 123\n",
      "Train 데이터 문장 3156의 토큰화된 길이: 54\n",
      "Train 데이터 문장 3157의 토큰화된 길이: 121\n",
      "Train 데이터 문장 3158의 토큰화된 길이: 133\n",
      "Train 데이터 문장 3159의 토큰화된 길이: 123\n",
      "Train 데이터 문장 3160의 토큰화된 길이: 98\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 생성\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "\n",
    "# 각 문장의 토큰화된 길이 확인\n",
    "for i, length in enumerate(data_train.original_lengths):\n",
    "    print(f\"Train 데이터 문장 {i+1}의 토큰화된 길이: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "310989af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 문장 길이 계산\n",
    "lengths = data_train.original_lengths\n",
    "\n",
    "# 평균 및 최댓값 계산\n",
    "mean_length = np.mean(lengths)\n",
    "max_length = np.max(lengths)\n",
    "\n",
    "# 토큰화된 문장 길이 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(mean_length, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_length:.2f}')\n",
    "plt.axvline(max_length, color='green', linestyle='dashed', linewidth=2, label=f'Max: {max_length}')\n",
    "\n",
    "plt.title('Tokenized Sentence Length Distribution', fontsize=15)\n",
    "plt.xlabel('Sentence Length (tokens)', fontsize=12)\n",
    "plt.ylabel('Number of Sentences', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "63bc6c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation</th>\n",
       "      <th>Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>학부모님 안녕하세요  선생님 우리 애 성적이 왜 이래요  개똥이가 이번에 성적이 많...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>원영 학생 지금 교수실로 와 예 교수님 저 지금 스케줄 없습니다 지금 우리 아들 유...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>내가 맘만 먹으면 너같은 거 여기 발도 못붙이게 할 수있는거 알지 제가 뭘 잘못했다...</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>아니 끼어든 것은 아저씨잖아요  참나 눈을 감고 운전하셨나  눈을 감기는 직전차선에...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>너 손에 그거 대체 뭐야 내가 오늘만을 기다렸다 계속 기다렸어 내가 분명 그 일은 ...</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>이거 허니 버터칩 아니야 대박 이걸 구한 사람이 있다니 이거 맛있다 너도 사봐 난 ...</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>김마리씨 내일까지 제출하라는 서류 오늘 밤까지 부탁해요 과장님 저 오늘 엄마 생신이...</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>정한씨 내일 나랑 거래처 가기로 한 거 혼자 갈 수 있지  네 갑자기요 브리핑 준비...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>중요한 전화 올 수도 있으니까 점심시간에 사무실 대기 좀 해줘요 그러면 제 점심은 ...</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>나 우울해  약 잘 챙겨먹고있어 오늘 왜 그냥 다 힘드네 죽고싶다 하  죽고싶다는 ...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1092 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           conversation  Length\n",
       "0     학부모님 안녕하세요  선생님 우리 애 성적이 왜 이래요  개똥이가 이번에 성적이 많...     148\n",
       "1     원영 학생 지금 교수실로 와 예 교수님 저 지금 스케줄 없습니다 지금 우리 아들 유...     154\n",
       "2     내가 맘만 먹으면 너같은 거 여기 발도 못붙이게 할 수있는거 알지 제가 뭘 잘못했다...     216\n",
       "3     아니 끼어든 것은 아저씨잖아요  참나 눈을 감고 운전하셨나  눈을 감기는 직전차선에...     150\n",
       "4     너 손에 그거 대체 뭐야 내가 오늘만을 기다렸다 계속 기다렸어 내가 분명 그 일은 ...     140\n",
       "...                                                 ...     ...\n",
       "1087  이거 허니 버터칩 아니야 대박 이걸 구한 사람이 있다니 이거 맛있다 너도 사봐 난 ...     157\n",
       "1088  김마리씨 내일까지 제출하라는 서류 오늘 밤까지 부탁해요 과장님 저 오늘 엄마 생신이...     185\n",
       "1089  정한씨 내일 나랑 거래처 가기로 한 거 혼자 갈 수 있지  네 갑자기요 브리핑 준비...     155\n",
       "1090  중요한 전화 올 수도 있으니까 점심시간에 사무실 대기 좀 해줘요 그러면 제 점심은 ...     132\n",
       "1091  나 우울해  약 잘 챙겨먹고있어 오늘 왜 그냥 다 힘드네 죽고싶다 하  죽고싶다는 ...     133\n",
       "\n",
       "[1092 rows x 2 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화 후 각 문장의 길이를 확인한 리스트 생성\n",
    "tokenized_data_with_lengths = [(i[0], len(tok(i[0]))) for i in dataset_train]\n",
    "\n",
    "# 길이 130을 초과하는 문장만 필터링\n",
    "long_sentences = [(sentence, length) for sentence, length in tokenized_data_with_lengths if length > 130]\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df_long_sentences = pd.DataFrame(long_sentences, columns=['conversation', 'Length'])\n",
    "\n",
    "df_long_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10034d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_long_sentences.to_csv('./aftertoken.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6f27ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTDataset(Dataset):\n",
    "#     def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "#         transform = nlp.data.BERTSentenceTransform(\n",
    "#             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "#         self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        \n",
    "#         # 레이블이 없는 경우를 처리\n",
    "#         if label_idx is not None:\n",
    "#             self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "#         else:\n",
    "#             self.labels = None\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         if self.labels is not None:\n",
    "#             return self.sentences[i] + (self.labels[i], )\n",
    "#         else:\n",
    "#             return self.sentences[i]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e300a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting hyper-parameters\n",
    "max_len = 130\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.2\n",
    "num_epochs = 25\n",
    "max_grad_norm = 1\n",
    "log_interval = 5\n",
    "learning_rate =  5e-5\n",
    "dr_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "46b6fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /aiffel/aiffel/dktc/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
     ]
    }
   ],
   "source": [
    "#토큰화\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "#BERTDataset 클래스 이용, TensorDataset으로 만들어주기\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_val = BERTDataset(dataset_val, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "01a34de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치 및 데이터로더 설정\n",
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=4)\n",
    "val_dataloader = torch.utils.data.DataLoader(data_val, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba367912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size=768,\n",
    "                 num_classes=5,\n",
    "                 dr_rate=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        # attention mask는 token_ids에서 패딩(0)이 아닌 부분을 1로 설정\n",
    "        attention_mask = (token_ids != 0).float()\n",
    "        \n",
    "        # BERT 모델에서 pooler 출력 사용\n",
    "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids.long(), attention_mask=attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        # 드롭아웃 적용\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        \n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9c54e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate = dr_rate).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "28166114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "95981134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a822f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 지표 계산 함수\n",
    "def calc_metrics(X, Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    pred_labels = max_indices.cpu().numpy()\n",
    "    true_labels = Y.cpu().numpy()\n",
    "    \n",
    "    accuracy = (pred_labels == true_labels).sum() / len(true_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='weighted')\n",
    "    precision = precision_score(true_labels, pred_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, pred_labels, average='weighted')\n",
    "    \n",
    "    return accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f467445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6749c1ed314978b11635e6cad96dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 1: Loss 1.589824914932251, Accuracy 0.28125, F1 0.24150027603974972, Precision 0.23937908496732027, Recall 0.28125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 6: Loss 1.6633347272872925, Accuracy 0.25, F1 0.21405788777499302, Precision 0.227322196118152, Recall 0.25\n",
      "Epoch 1 Batch 11: Loss 1.5661797523498535, Accuracy 0.2869318181818182, F1 0.25784192639468795, Precision 0.299503511936667, Recall 0.2869318181818182\n",
      "Epoch 1 Batch 16: Loss 1.4736438989639282, Accuracy 0.296875, F1 0.2778334944164616, Precision 0.3482603174756208, Recall 0.296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 21: Loss 1.4415019750595093, Accuracy 0.3125, F1 0.2891988021240013, Precision 0.3574361652683607, Recall 0.3125\n",
      "Epoch 1 Batch 26: Loss 1.5010387897491455, Accuracy 0.3161057692307692, F1 0.2923642174049617, Precision 0.36027761926445956, Recall 0.3161057692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 31: Loss 1.4012761116027832, Accuracy 0.3185483870967742, F1 0.29195569030928953, Precision 0.3469418416893731, Recall 0.3185483870967742\n",
      "Epoch 1 Batch 36: Loss 1.4540022611618042, Accuracy 0.3272569444444444, F1 0.3033505986925835, Precision 0.3602978502933757, Recall 0.3272569444444444\n",
      "Epoch 1 Batch 41: Loss 1.349645972251892, Accuracy 0.3246951219512195, F1 0.3019253805490276, Precision 0.35543083083478844, Recall 0.3246951219512195\n",
      "Epoch 1 Batch 46: Loss 1.4964168071746826, Accuracy 0.3294836956521739, F1 0.3098739513958055, Precision 0.3689777955097196, Recall 0.3294836956521739\n",
      "Epoch 1 Batch 51: Loss 1.3372520208358765, Accuracy 0.34129901960784315, F1 0.32301572746600316, Precision 0.37897709437537397, Recall 0.34129901960784315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 56: Loss 1.3666720390319824, Accuracy 0.34486607142857145, F1 0.3245579258598874, Precision 0.3793660769160021, Recall 0.34486607142857145\n",
      "Epoch 1 Batch 61: Loss 1.3791242837905884, Accuracy 0.3519467213114754, F1 0.33062294357202837, Precision 0.38227495695208813, Recall 0.3519467213114754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 66: Loss 1.307664394378662, Accuracy 0.36079545454545453, F1 0.337596180993114, Precision 0.39081378904032077, Recall 0.36079545454545453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 71: Loss 1.3025370836257935, Accuracy 0.37235915492957744, F1 0.34897498632427043, Precision 0.39824803842384604, Recall 0.37235915492957744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 76: Loss 1.2881550788879395, Accuracy 0.3774671052631579, F1 0.3529362862785596, Precision 0.402158607874078, Recall 0.3774671052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 81: Loss 1.1786869764328003, Accuracy 0.3881172839506173, F1 0.36234977612424535, Precision 0.4103155977577364, Recall 0.3881172839506173\n",
      "Epoch 1 Batch 86: Loss 1.248471975326538, Accuracy 0.4011627906976744, F1 0.376121343234726, Precision 0.4261674989963592, Recall 0.4011627906976744\n",
      "Epoch 1 Batch 91: Loss 1.2309775352478027, Accuracy 0.4114010989010989, F1 0.38678005517180347, Precision 0.4358490117848028, Recall 0.4114010989010989\n",
      "Epoch 1 Batch 96: Loss 1.165951132774353, Accuracy 0.4251302083333333, F1 0.4014047444072932, Precision 0.4505092258998585, Recall 0.4251302083333333\n",
      "Epoch 1: Train Accuracy 0.43297558922558926, F1 0.40934499000817304, Precision 0.4572921513025754, Recall 0.43297558922558926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86081bb2ee149c2b34f13ece97e46e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy 0.6954545454545454, F1 0.6843918962074369, Precision 0.7483259396853147, Recall 0.6954545454545454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abded8db19fb4169b27250b4221ae0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 1: Loss 1.0860995054244995, Accuracy 0.78125, F1 0.777106227106227, Precision 0.8387784090909091, Recall 0.78125\n",
      "Epoch 2 Batch 6: Loss 1.116379737854004, Accuracy 0.6666666666666666, F1 0.6582590964605283, Precision 0.6933672664141414, Recall 0.6666666666666666\n",
      "Epoch 2 Batch 11: Loss 1.0438512563705444, Accuracy 0.6846590909090909, F1 0.6734966585140513, Precision 0.7264431841554168, Recall 0.6846590909090909\n",
      "Epoch 2 Batch 16: Loss 0.9288026094436646, Accuracy 0.703125, F1 0.693484195128554, Precision 0.7377899941663729, Recall 0.703125\n",
      "Epoch 2 Batch 21: Loss 0.9907744526863098, Accuracy 0.7202380952380952, F1 0.7117425172631888, Precision 0.7600688587886593, Recall 0.7202380952380952\n",
      "Epoch 2 Batch 26: Loss 0.9438207149505615, Accuracy 0.7319711538461539, F1 0.7218048007188462, Precision 0.7685136183810535, Recall 0.7319711538461539\n",
      "Epoch 2 Batch 31: Loss 0.8566385507583618, Accuracy 0.7419354838709677, F1 0.7337245254923516, Precision 0.777687702576519, Recall 0.7419354838709677\n",
      "Epoch 2 Batch 36: Loss 0.7396048307418823, Accuracy 0.7534722222222222, F1 0.7465604649186859, Precision 0.7877862159456828, Recall 0.7534722222222222\n",
      "Epoch 2 Batch 41: Loss 0.7471168041229248, Accuracy 0.7469512195121951, F1 0.7405909199501112, Precision 0.7853345294958907, Recall 0.7469512195121951\n",
      "Epoch 2 Batch 46: Loss 0.5262942910194397, Accuracy 0.7595108695652174, F1 0.753952043605073, Precision 0.7956853027265431, Recall 0.7595108695652174\n",
      "Epoch 2 Batch 51: Loss 0.7019047141075134, Accuracy 0.7634803921568627, F1 0.7584808978388563, Precision 0.7993601258090878, Recall 0.7634803921568627\n",
      "Epoch 2 Batch 56: Loss 0.687987208366394, Accuracy 0.7667410714285714, F1 0.7624217658822089, Precision 0.8011218431501389, Recall 0.7667410714285714\n",
      "Epoch 2 Batch 61: Loss 0.48529550433158875, Accuracy 0.7735655737704918, F1 0.769508513634265, Precision 0.8065145694703614, Recall 0.7735655737704918\n",
      "Epoch 2 Batch 66: Loss 0.8003191351890564, Accuracy 0.7755681818181818, F1 0.7711233191836727, Precision 0.806791715723868, Recall 0.7755681818181818\n",
      "Epoch 2 Batch 71: Loss 0.6225736737251282, Accuracy 0.7790492957746479, F1 0.7744682870944424, Precision 0.8092523663451064, Recall 0.7790492957746479\n",
      "Epoch 2 Batch 76: Loss 0.5264564156532288, Accuracy 0.78125, F1 0.7772249195255289, Precision 0.8110661681343035, Recall 0.78125\n",
      "Epoch 2 Batch 81: Loss 0.6444990038871765, Accuracy 0.7824074074074074, F1 0.7781217500491435, Precision 0.8119231607512858, Recall 0.7824074074074074\n",
      "Epoch 2 Batch 86: Loss 0.4259364604949951, Accuracy 0.7848837209302325, F1 0.7809032856697091, Precision 0.8132265480603053, Recall 0.7848837209302325\n",
      "Epoch 2 Batch 91: Loss 0.443639874458313, Accuracy 0.7860576923076923, F1 0.7822984672389445, Precision 0.8142558339287392, Recall 0.7860576923076923\n",
      "Epoch 2 Batch 96: Loss 0.4868263602256775, Accuracy 0.7874348958333334, F1 0.7838432500564796, Precision 0.8154012577230353, Recall 0.7874348958333334\n",
      "Epoch 2: Train Accuracy 0.7879840067340068, F1 0.7843584088821626, Precision 0.8160275536577621, Recall 0.7879840067340068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459e77c86d754a74bcbaa353af2f72f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Accuracy 0.8364772727272727, F1 0.8343671489597, Precision 0.8577710137085135, Recall 0.8364772727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8afc5e5bbf64ca09a7c06fa921e3562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 1: Loss 0.4175999164581299, Accuracy 0.875, F1 0.8780504948268106, Precision 0.90234375, Recall 0.875\n",
      "Epoch 3 Batch 6: Loss 0.511979877948761, Accuracy 0.8489583333333334, F1 0.8496451136761701, Precision 0.8696635251322752, Recall 0.8489583333333334\n",
      "Epoch 3 Batch 11: Loss 0.44462841749191284, Accuracy 0.84375, F1 0.8434528572591916, Precision 0.8646349616478294, Recall 0.84375\n",
      "Epoch 3 Batch 16: Loss 0.3774498701095581, Accuracy 0.84765625, F1 0.847298879520356, Precision 0.8691686399665491, Recall 0.84765625\n",
      "Epoch 3 Batch 21: Loss 0.5015528202056885, Accuracy 0.8571428571428571, F1 0.856956673031758, Precision 0.8771017791829243, Recall 0.8571428571428571\n",
      "Epoch 3 Batch 26: Loss 0.571399986743927, Accuracy 0.8509615384615384, F1 0.8496867292821968, Precision 0.8732284905344154, Recall 0.8509615384615384\n",
      "Epoch 3 Batch 31: Loss 0.4925203323364258, Accuracy 0.8578629032258065, F1 0.8566249596958316, Precision 0.8783408652345442, Recall 0.8578629032258065\n",
      "Epoch 3 Batch 36: Loss 0.2879391014575958, Accuracy 0.8689236111111112, F1 0.8678746677803582, Precision 0.8882838777791291, Recall 0.8689236111111112\n",
      "Epoch 3 Batch 41: Loss 0.46130627393722534, Accuracy 0.8612804878048781, F1 0.8601671960409426, Precision 0.881971580846222, Recall 0.8612804878048781\n",
      "Epoch 3 Batch 46: Loss 0.3127646744251251, Accuracy 0.8648097826086957, F1 0.8640361553698598, Precision 0.8854488919186807, Recall 0.8648097826086957\n",
      "Epoch 3 Batch 51: Loss 0.31254133582115173, Accuracy 0.8670343137254902, F1 0.8662635514733603, Precision 0.8869738817186912, Recall 0.8670343137254902\n",
      "Epoch 3 Batch 56: Loss 0.48079949617385864, Accuracy 0.8699776785714286, F1 0.8690292860781784, Precision 0.8896306754417965, Recall 0.8699776785714286\n",
      "Epoch 3 Batch 61: Loss 0.4804776906967163, Accuracy 0.8698770491803278, F1 0.8692033032370984, Precision 0.8901642981937702, Recall 0.8698770491803278\n",
      "Epoch 3 Batch 66: Loss 0.6470839381217957, Accuracy 0.865530303030303, F1 0.8647134032294119, Precision 0.8861224816355164, Recall 0.865530303030303\n",
      "Epoch 3 Batch 71: Loss 0.6635922193527222, Accuracy 0.8639964788732394, F1 0.8622701681474022, Precision 0.8848752820467298, Recall 0.8639964788732394\n",
      "Epoch 3 Batch 76: Loss 0.2938147485256195, Accuracy 0.8638980263157895, F1 0.862063343050233, Precision 0.8846896865496905, Recall 0.8638980263157895\n",
      "Epoch 3 Batch 81: Loss 0.36166658997535706, Accuracy 0.8587962962962963, F1 0.8561370173019588, Precision 0.8815012329088446, Recall 0.8587962962962963\n",
      "Epoch 3 Batch 86: Loss 0.3970694839954376, Accuracy 0.8601017441860465, F1 0.8574718865198658, Precision 0.8822998302189906, Recall 0.8601017441860465\n",
      "Epoch 3 Batch 91: Loss 0.43563875555992126, Accuracy 0.8581730769230769, F1 0.8555571794191028, Precision 0.8809650589436061, Recall 0.8581730769230769\n",
      "Epoch 3 Batch 96: Loss 0.31413620710372925, Accuracy 0.8597005208333334, F1 0.8571086766846, Precision 0.881696703074805, Recall 0.8597005208333334\n",
      "Epoch 3: Train Accuracy 0.8593223905723906, F1 0.8569188153901651, Precision 0.8820695717392463, Recall 0.8593223905723906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732eceb2bb947e4b3b486668f58b2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Accuracy 0.8464772727272727, F1 0.8460014184323589, Precision 0.8697996552059051, Recall 0.8464772727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d97303f8184f8e9590d22090f5c7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 1: Loss 0.13570605218410492, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 4 Batch 6: Loss 0.4250665605068207, Accuracy 0.9166666666666666, F1 0.9150843178108946, Precision 0.9237050276112776, Recall 0.9166666666666666\n",
      "Epoch 4 Batch 11: Loss 0.3516482412815094, Accuracy 0.9005681818181818, F1 0.8988269739918882, Precision 0.9136621506649916, Recall 0.9005681818181818\n",
      "Epoch 4 Batch 16: Loss 0.2990352213382721, Accuracy 0.904296875, F1 0.9028292661806598, Precision 0.9186611402919995, Recall 0.904296875\n",
      "Epoch 4 Batch 21: Loss 0.3122357428073883, Accuracy 0.9122023809523809, F1 0.9107363999240233, Precision 0.9238044693401836, Recall 0.9122023809523809\n",
      "Epoch 4 Batch 26: Loss 0.37981173396110535, Accuracy 0.9086538461538461, F1 0.9072585532419672, Precision 0.9242233094096074, Recall 0.9086538461538461\n",
      "Epoch 4 Batch 31: Loss 0.4644705653190613, Accuracy 0.907258064516129, F1 0.9057516780893852, Precision 0.9215656011774962, Recall 0.907258064516129\n",
      "Epoch 4 Batch 36: Loss 0.14639641344547272, Accuracy 0.9175347222222222, F1 0.916261141627603, Precision 0.930289128791733, Recall 0.9175347222222222\n",
      "Epoch 4 Batch 41: Loss 0.22132785618305206, Accuracy 0.9100609756097561, F1 0.9091449585662023, Precision 0.9265712141855434, Recall 0.9100609756097561\n",
      "Epoch 4 Batch 46: Loss 0.18806147575378418, Accuracy 0.9150815217391305, F1 0.9141827459673766, Precision 0.9305263482743102, Recall 0.9150815217391305\n",
      "Epoch 4 Batch 51: Loss 0.28944236040115356, Accuracy 0.9111519607843137, F1 0.9102883880366803, Precision 0.9286353318522437, Recall 0.9111519607843137\n",
      "Epoch 4 Batch 56: Loss 0.6197728514671326, Accuracy 0.9090401785714286, F1 0.9078264008857367, Precision 0.9261209416203836, Recall 0.9090401785714286\n",
      "Epoch 4 Batch 61: Loss 0.17866593599319458, Accuracy 0.9118852459016393, F1 0.9107549152185954, Precision 0.928381792956588, Recall 0.9118852459016393\n",
      "Epoch 4 Batch 66: Loss 0.369873583316803, Accuracy 0.9071969696969697, F1 0.9051729033443416, Precision 0.9224791359734541, Recall 0.9071969696969697\n",
      "Epoch 4 Batch 71: Loss 0.46562710404396057, Accuracy 0.9058098591549296, F1 0.9038979272024436, Precision 0.9215758713117866, Recall 0.9058098591549296\n",
      "Epoch 4 Batch 76: Loss 0.41061681509017944, Accuracy 0.9041940789473685, F1 0.9021672155358698, Precision 0.9198783375798177, Recall 0.9041940789473685\n",
      "Epoch 4 Batch 81: Loss 0.13370774686336517, Accuracy 0.9047067901234568, F1 0.9026501862464356, Precision 0.9201885188285649, Recall 0.9047067901234568\n",
      "Epoch 4 Batch 86: Loss 0.12616629898548126, Accuracy 0.904796511627907, F1 0.9028711385280501, Precision 0.9202118769070078, Recall 0.904796511627907\n",
      "Epoch 4 Batch 91: Loss 0.4579077661037445, Accuracy 0.9048763736263736, F1 0.9031237303977476, Precision 0.9200383436389618, Recall 0.9048763736263736\n",
      "Epoch 4 Batch 96: Loss 0.5022246837615967, Accuracy 0.9036458333333334, F1 0.9020030494673238, Precision 0.919042072655354, Recall 0.9036458333333334\n",
      "Epoch 4: Train Accuracy 0.9046717171717171, F1 0.90316127871276, Precision 0.9201049511655573, Recall 0.9046717171717171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deda329499444163a98e37590d2beeef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Accuracy 0.8577272727272727, F1 0.8583858570364291, Precision 0.876293231074481, Recall 0.8577272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff07facddd8444fc806603124abe8661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 1: Loss 0.17191356420516968, Accuracy 0.96875, F1 0.9682921245421245, Precision 0.9715909090909091, Recall 0.96875\n",
      "Epoch 5 Batch 6: Loss 0.2772243022918701, Accuracy 0.9375, F1 0.9387974213421081, Precision 0.9461724762506013, Recall 0.9375\n",
      "Epoch 5 Batch 11: Loss 0.48526331782341003, Accuracy 0.9318181818181818, F1 0.9326847045452801, Precision 0.9426543232651188, Recall 0.9318181818181818\n",
      "Epoch 5 Batch 16: Loss 0.41263318061828613, Accuracy 0.931640625, F1 0.9318310016283645, Precision 0.9420510424904956, Recall 0.931640625\n",
      "Epoch 5 Batch 21: Loss 0.22533708810806274, Accuracy 0.9375, F1 0.9375089914934469, Precision 0.9466602428656, Recall 0.9375\n",
      "Epoch 5 Batch 26: Loss 0.24123063683509827, Accuracy 0.9314903846153846, F1 0.9315629478299954, Precision 0.9426073725953533, Recall 0.9314903846153846\n",
      "Epoch 5 Batch 31: Loss 0.22761744260787964, Accuracy 0.9324596774193549, F1 0.93236056811609, Precision 0.9428326517792244, Recall 0.9324596774193549\n",
      "Epoch 5 Batch 36: Loss 0.05912892147898674, Accuracy 0.9392361111111112, F1 0.9391243584785912, Precision 0.9484601577049495, Recall 0.9392361111111112\n",
      "Epoch 5 Batch 41: Loss 0.14210891723632812, Accuracy 0.9329268292682927, F1 0.9329864098286232, Precision 0.9437649389059451, Recall 0.9329268292682927\n",
      "Epoch 5 Batch 46: Loss 0.03393540158867836, Accuracy 0.9354619565217391, F1 0.9355119124416091, Precision 0.9455870405258994, Recall 0.9354619565217391\n",
      "Epoch 5 Batch 51: Loss 0.364010751247406, Accuracy 0.928921568627451, F1 0.9291739258415685, Precision 0.9412102331219978, Recall 0.928921568627451\n",
      "Epoch 5 Batch 56: Loss 0.3039664626121521, Accuracy 0.9285714285714286, F1 0.928952887971634, Precision 0.941217508000432, Recall 0.9285714285714286\n",
      "Epoch 5 Batch 61: Loss 0.12362812459468842, Accuracy 0.9287909836065574, F1 0.9286787599441815, Precision 0.9412853653632343, Recall 0.9287909836065574\n",
      "Epoch 5 Batch 66: Loss 0.1785353422164917, Accuracy 0.9285037878787878, F1 0.9283869803251983, Precision 0.9409107136024749, Recall 0.9285037878787878\n",
      "Epoch 5 Batch 71: Loss 0.4298405051231384, Accuracy 0.9273767605633803, F1 0.9273022571409241, Precision 0.939983048543788, Recall 0.9273767605633803\n",
      "Epoch 5 Batch 76: Loss 0.6927564740180969, Accuracy 0.9259868421052632, F1 0.9260495097353454, Precision 0.9397870860864281, Recall 0.9259868421052632\n",
      "Epoch 5 Batch 81: Loss 0.09762468189001083, Accuracy 0.9263117283950617, F1 0.9264287414031444, Precision 0.9398596302531488, Recall 0.9263117283950617\n",
      "Epoch 5 Batch 86: Loss 0.05259281024336815, Accuracy 0.9258720930232558, F1 0.9260197674268551, Precision 0.9392728153629318, Recall 0.9258720930232558\n",
      "Epoch 5 Batch 91: Loss 0.26924216747283936, Accuracy 0.9244505494505495, F1 0.9245505399543932, Precision 0.9377370769111844, Recall 0.9244505494505495\n",
      "Epoch 5 Batch 96: Loss 0.11953434348106384, Accuracy 0.9254557291666666, F1 0.9255954268645912, Precision 0.9384470988719039, Recall 0.9254557291666666\n",
      "Epoch 5: Train Accuracy 0.9270833333333334, F1 0.927225927983148, Precision 0.9397892502769399, Recall 0.9270833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc269145d03446cbeee3dba0dc326b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Accuracy 0.8682954545454545, F1 0.8689184170480712, Precision 0.8866927655677656, Recall 0.8682954545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547ac01a625d4fe8846a70f9d5d9e779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 1: Loss 0.05696895718574524, Accuracy 0.96875, F1 0.968671679197995, Precision 0.9715909090909091, Recall 0.96875\n",
      "Epoch 6 Batch 6: Loss 0.05135534331202507, Accuracy 0.953125, F1 0.9528157344739289, Precision 0.957879577020202, Recall 0.953125\n",
      "Epoch 6 Batch 11: Loss 0.18501728773117065, Accuracy 0.9573863636363636, F1 0.9572526867367018, Precision 0.9626042281582055, Recall 0.9573863636363636\n",
      "Epoch 6 Batch 16: Loss 0.21909895539283752, Accuracy 0.95703125, F1 0.9569838785812137, Precision 0.9629413555194805, Recall 0.95703125\n",
      "Epoch 6 Batch 21: Loss 0.20444045960903168, Accuracy 0.9583333333333334, F1 0.9583734295927504, Precision 0.9640079042980828, Recall 0.9583333333333334\n",
      "Epoch 6 Batch 26: Loss 0.1539398729801178, Accuracy 0.9543269230769231, F1 0.9536064303308026, Precision 0.9607110604406277, Recall 0.9543269230769231\n",
      "Epoch 6 Batch 31: Loss 0.06359493732452393, Accuracy 0.9536290322580645, F1 0.9530242945931214, Precision 0.960180092085334, Recall 0.9536290322580645\n",
      "Epoch 6 Batch 36: Loss 0.020627593621611595, Accuracy 0.9548611111111112, F1 0.954342590112599, Precision 0.9617705645049394, Recall 0.9548611111111112\n",
      "Epoch 6 Batch 41: Loss 0.263755738735199, Accuracy 0.9519817073170732, F1 0.9515526753691034, Precision 0.9591401103977321, Recall 0.9519817073170732\n",
      "Epoch 6 Batch 46: Loss 0.027692394331097603, Accuracy 0.9565217391304348, F1 0.9561329472279604, Precision 0.9629543124448013, Recall 0.9565217391304348\n",
      "Epoch 6 Batch 51: Loss 0.2919483780860901, Accuracy 0.9540441176470589, F1 0.9537959502762501, Precision 0.9612773324170379, Recall 0.9540441176470589\n",
      "Epoch 6 Batch 56: Loss 0.17962302267551422, Accuracy 0.9553571428571429, F1 0.955099852564054, Precision 0.9621358614104147, Recall 0.9553571428571429\n",
      "Epoch 6 Batch 61: Loss 0.06604038923978806, Accuracy 0.9554303278688525, F1 0.9552056707251416, Precision 0.9621549343757332, Recall 0.9554303278688525\n",
      "Epoch 6 Batch 66: Loss 0.4537618160247803, Accuracy 0.9526515151515151, F1 0.9524513253013878, Precision 0.9603643765220465, Recall 0.9526515151515151\n",
      "Epoch 6 Batch 71: Loss 0.4069000482559204, Accuracy 0.948943661971831, F1 0.9487328430418606, Precision 0.9582026993448647, Recall 0.948943661971831\n",
      "Epoch 6 Batch 76: Loss 0.8602088093757629, Accuracy 0.9469572368421053, F1 0.9465104047939888, Precision 0.9571023765616691, Recall 0.9469572368421053\n",
      "Epoch 6 Batch 81: Loss 0.3541925549507141, Accuracy 0.9436728395061729, F1 0.9430647727384186, Precision 0.9545088784672117, Recall 0.9436728395061729\n",
      "Epoch 6 Batch 86: Loss 0.0948403850197792, Accuracy 0.9436773255813954, F1 0.9431379376064143, Precision 0.9541814139107015, Recall 0.9436773255813954\n",
      "Epoch 6 Batch 91: Loss 0.14371173083782196, Accuracy 0.9423076923076923, F1 0.9416859255602228, Precision 0.9529896814495028, Recall 0.9423076923076923\n",
      "Epoch 6 Batch 96: Loss 0.07750608772039413, Accuracy 0.9427083333333334, F1 0.9421320409376829, Precision 0.9530600521079037, Recall 0.9427083333333334\n",
      "Epoch 6: Train Accuracy 0.9421296296296295, F1 0.9415044099876279, Precision 0.9524596422797181, Recall 0.9421296296296295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf494bc6db34c2f8d3981ad217522e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Accuracy 0.8657954545454545, F1 0.8663952771095395, Precision 0.8831931367243868, Recall 0.8657954545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9106e494a74cde9ce00ed57403e619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 1: Loss 0.02347569726407528, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 7 Batch 6: Loss 0.019211294129490852, Accuracy 0.984375, F1 0.9844318467480232, Precision 0.9858940972222222, Recall 0.984375\n",
      "Epoch 7 Batch 11: Loss 0.05588637664914131, Accuracy 0.9772727272727273, F1 0.9772562751340339, Precision 0.9800860164141413, Recall 0.9772727272727273\n",
      "Epoch 7 Batch 16: Loss 0.05017735809087753, Accuracy 0.96875, F1 0.968640059129971, Precision 0.9724569728231837, Recall 0.96875\n",
      "Epoch 7 Batch 21: Loss 0.01625792682170868, Accuracy 0.9717261904761905, F1 0.9715921786321114, Precision 0.975169463060088, Recall 0.9717261904761905\n",
      "Epoch 7 Batch 26: Loss 0.3058890402317047, Accuracy 0.96875, F1 0.9685426242350498, Precision 0.9733021202221923, Recall 0.96875\n",
      "Epoch 7 Batch 31: Loss 0.025495583191514015, Accuracy 0.9737903225806451, F1 0.9736163945197192, Precision 0.9776082298637742, Recall 0.9737903225806451\n",
      "Epoch 7 Batch 36: Loss 0.007532731629908085, Accuracy 0.9756944444444444, F1 0.9755446730586469, Precision 0.9791773993271389, Recall 0.9756944444444444\n",
      "Epoch 7 Batch 41: Loss 0.009539566934108734, Accuracy 0.9771341463414634, F1 0.977002639758812, Precision 0.9801923506287072, Recall 0.9771341463414634\n",
      "Epoch 7 Batch 46: Loss 0.10464807599782944, Accuracy 0.9741847826086957, F1 0.9743627168195047, Precision 0.9786399449358009, Recall 0.9741847826086957\n",
      "Epoch 7 Batch 51: Loss 0.5728861093521118, Accuracy 0.9724264705882353, F1 0.9723009439012418, Precision 0.9772426587913718, Recall 0.9724264705882353\n",
      "Epoch 7 Batch 56: Loss 0.10868249833583832, Accuracy 0.9720982142857143, F1 0.9720372720579082, Precision 0.9772120465410084, Recall 0.9720982142857143\n",
      "Epoch 7 Batch 61: Loss 0.10628281533718109, Accuracy 0.9713114754098361, F1 0.9710861700747493, Precision 0.9763576324347326, Recall 0.9713114754098361\n",
      "Epoch 7 Batch 66: Loss 0.25987544655799866, Accuracy 0.96875, F1 0.9685074590632662, Precision 0.9739489617863195, Recall 0.96875\n",
      "Epoch 7 Batch 71: Loss 0.33572590351104736, Accuracy 0.9661091549295775, F1 0.9660748924061339, Precision 0.9722552700380603, Recall 0.9661091549295775\n",
      "Epoch 7 Batch 76: Loss 0.2830246090888977, Accuracy 0.9654605263157895, F1 0.9654284097085439, Precision 0.97170909260239, Recall 0.9654605263157895\n",
      "Epoch 7 Batch 81: Loss 0.017994467169046402, Accuracy 0.9621913580246914, F1 0.9622524394792509, Precision 0.969660274246501, Recall 0.9621913580246914\n",
      "Epoch 7 Batch 86: Loss 0.09287215769290924, Accuracy 0.9614825581395349, F1 0.9615657953055826, Precision 0.9688342982719799, Recall 0.9614825581395349\n",
      "Epoch 7 Batch 91: Loss 0.01967991143465042, Accuracy 0.9622252747252747, F1 0.9622964410447422, Precision 0.9692319427939275, Recall 0.9622252747252747\n",
      "Epoch 7 Batch 96: Loss 0.09224724769592285, Accuracy 0.9632161458333334, F1 0.9632580470817311, Precision 0.9699911918051566, Recall 0.9632161458333334\n",
      "Epoch 7: Train Accuracy 0.9629629629629629, F1 0.9630449802938087, Precision 0.9697957516494448, Recall 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2dd88efc96048fa9df75df5301b34e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Accuracy 0.8432954545454545, F1 0.8449311379189675, Precision 0.8733156912531908, Recall 0.8432954545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cc50616ff64635893d9b70311165ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 1: Loss 0.03566891327500343, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 8 Batch 6: Loss 0.015994038432836533, Accuracy 0.9947916666666666, F1 0.9947045707915273, Precision 0.9952256944444443, Recall 0.9947916666666666\n",
      "Epoch 8 Batch 11: Loss 0.2148241251707077, Accuracy 0.9659090909090909, F1 0.9652516790989332, Precision 0.9718107413419914, Recall 0.9659090909090909\n",
      "Epoch 8 Batch 16: Loss 0.16657871007919312, Accuracy 0.96875, F1 0.9682682603878993, Precision 0.9735235305059524, Recall 0.96875\n",
      "Epoch 8 Batch 21: Loss 0.060389552265405655, Accuracy 0.9717261904761905, F1 0.9713628803337329, Precision 0.9757599914965986, Recall 0.9717261904761905\n",
      "Epoch 8 Batch 26: Loss 0.007118487264961004, Accuracy 0.9747596153846154, F1 0.9744468608974196, Precision 0.9782714247557998, Recall 0.9747596153846154\n",
      "Epoch 8 Batch 31: Loss 0.028865857049822807, Accuracy 0.9737903225806451, F1 0.973547444276149, Precision 0.9771408754775691, Recall 0.9737903225806451\n",
      "Epoch 8 Batch 36: Loss 0.39398089051246643, Accuracy 0.9704861111111112, F1 0.9703224442261509, Precision 0.9748655520790938, Recall 0.9704861111111112\n",
      "Epoch 8 Batch 41: Loss 0.0923943966627121, Accuracy 0.9679878048780488, F1 0.9677252908211565, Precision 0.9724323652296214, Recall 0.9679878048780488\n",
      "Epoch 8 Batch 46: Loss 0.009964228607714176, Accuracy 0.970108695652174, F1 0.9698924764593798, Precision 0.9742060211829234, Recall 0.970108695652174\n",
      "Epoch 8 Batch 51: Loss 0.08129792660474777, Accuracy 0.9705882352941176, F1 0.9703508286861987, Precision 0.9746499177565353, Recall 0.9705882352941176\n",
      "Epoch 8 Batch 56: Loss 0.02638721466064453, Accuracy 0.9715401785714286, F1 0.9713268958774697, Precision 0.9756019340282733, Recall 0.9715401785714286\n",
      "Epoch 8 Batch 61: Loss 0.008711865171790123, Accuracy 0.9723360655737705, F1 0.9721513537059789, Precision 0.9761502727691252, Recall 0.9723360655737705\n",
      "Epoch 8 Batch 66: Loss 0.44005200266838074, Accuracy 0.9706439393939394, F1 0.9704294410937545, Precision 0.9747150143101848, Recall 0.9706439393939394\n",
      "Epoch 8 Batch 71: Loss 0.20903176069259644, Accuracy 0.9700704225352113, F1 0.9699514172542258, Precision 0.9745338764814999, Recall 0.9700704225352113\n",
      "Epoch 8 Batch 76: Loss 0.34171730279922485, Accuracy 0.9683388157894737, F1 0.9682232153412865, Precision 0.9730912955830718, Recall 0.9683388157894737\n",
      "Epoch 8 Batch 81: Loss 0.19554655253887177, Accuracy 0.966820987654321, F1 0.9666421607779034, Precision 0.9718142572019888, Recall 0.966820987654321\n",
      "Epoch 8 Batch 86: Loss 0.017947571352124214, Accuracy 0.9676598837209303, F1 0.967500812436101, Precision 0.9725324205429584, Recall 0.9676598837209303\n",
      "Epoch 8 Batch 91: Loss 0.01159333623945713, Accuracy 0.9680631868131868, F1 0.9679059429220183, Precision 0.9729255567768618, Recall 0.9680631868131868\n",
      "Epoch 8 Batch 96: Loss 0.0734492763876915, Accuracy 0.9694010416666666, F1 0.9692574310536927, Precision 0.9740566661709241, Recall 0.9694010416666666\n",
      "Epoch 8: Train Accuracy 0.9700126262626263, F1 0.9698800657689695, Precision 0.9745722650315455, Recall 0.9700126262626263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ec37ca29024755a6af64d5a4df786e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Accuracy 0.8670454545454546, F1 0.8663873654789795, Precision 0.881663385225885, Recall 0.8670454545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55c35431785e4ff595b9ea658193da09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 1: Loss 0.0047660828568041325, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 9 Batch 6: Loss 0.009886854328215122, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 9 Batch 11: Loss 0.21390020847320557, Accuracy 0.9857954545454546, F1 0.9856368164287566, Precision 0.9879577020202018, Recall 0.9857954545454546\n",
      "Epoch 9 Batch 16: Loss 0.00854573119431734, Accuracy 0.982421875, F1 0.9822601137261615, Precision 0.9851453993055554, Recall 0.982421875\n",
      "Epoch 9 Batch 21: Loss 0.006861966103315353, Accuracy 0.9836309523809523, F1 0.9834698443357979, Precision 0.9859983229402871, Recall 0.9836309523809523\n",
      "Epoch 9 Batch 26: Loss 0.02093375474214554, Accuracy 0.9819711538461539, F1 0.9819659338488986, Precision 0.9849850236568986, Recall 0.9819711538461539\n",
      "Epoch 9 Batch 31: Loss 0.020871244370937347, Accuracy 0.9828629032258065, F1 0.982827857216189, Precision 0.985628680235535, Recall 0.9828629032258065\n",
      "Epoch 9 Batch 36: Loss 0.004788646940141916, Accuracy 0.9817708333333334, F1 0.981758567082547, Precision 0.9847167107583774, Recall 0.9817708333333334\n",
      "Epoch 9 Batch 41: Loss 0.4326289892196655, Accuracy 0.9763719512195121, F1 0.9763182022676137, Precision 0.9795099569299264, Recall 0.9763719512195121\n",
      "Epoch 9 Batch 46: Loss 0.10444148629903793, Accuracy 0.9762228260869565, F1 0.9761400968536388, Precision 0.9794569655133515, Recall 0.9762228260869565\n",
      "Epoch 9 Batch 51: Loss 0.24093370139598846, Accuracy 0.9748774509803921, F1 0.9748160861188838, Precision 0.9783610639768728, Recall 0.9748774509803921\n",
      "Epoch 9 Batch 56: Loss 0.27535346150398254, Accuracy 0.9698660714285714, F1 0.9699375786407961, Precision 0.9755079555860806, Recall 0.9698660714285714\n",
      "Epoch 9 Batch 61: Loss 0.0061811842024326324, Accuracy 0.9713114754098361, F1 0.9713892706656385, Precision 0.9766494775716087, Recall 0.9713114754098361\n",
      "Epoch 9 Batch 66: Loss 0.4646628797054291, Accuracy 0.9682765151515151, F1 0.968238923494155, Precision 0.9742021236339419, Recall 0.9682765151515151\n",
      "Epoch 9 Batch 71: Loss 0.12942546606063843, Accuracy 0.96875, F1 0.9687233991645112, Precision 0.9744673790118332, Recall 0.96875\n",
      "Epoch 9 Batch 76: Loss 0.09644098579883575, Accuracy 0.9695723684210527, F1 0.9695363354747428, Precision 0.9750865625146051, Recall 0.9695723684210527\n",
      "Epoch 9 Batch 81: Loss 0.007712570950388908, Accuracy 0.9702932098765432, F1 0.9702470066272153, Precision 0.9756035834450186, Recall 0.9702932098765432\n",
      "Epoch 9 Batch 86: Loss 0.009367240592837334, Accuracy 0.9709302325581395, F1 0.9708920988345047, Precision 0.9760378470044168, Recall 0.9709302325581395\n",
      "Epoch 9 Batch 91: Loss 0.041074711829423904, Accuracy 0.9714972527472527, F1 0.9714587616112684, Precision 0.9764250706558399, Recall 0.9714972527472527\n",
      "Epoch 9 Batch 96: Loss 0.19394095242023468, Accuracy 0.9720052083333334, F1 0.9720022258426503, Precision 0.9768499801702927, Recall 0.9720052083333334\n",
      "Epoch 9: Train Accuracy 0.9722222222222222, F1 0.9722248415738897, Precision 0.9770227711752333, Recall 0.9722222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af627ecb0bab42e9a8c641a6af4c1889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Accuracy 0.8801136363636364, F1 0.880367225524173, Precision 0.8927804036241536, Recall 0.8801136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be03ecc9fcd94116b97cf2c30270652d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 1: Loss 0.004088785499334335, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 10 Batch 6: Loss 0.06312637776136398, Accuracy 0.9895833333333334, F1 0.9898258377425044, Precision 0.9912760416666666, Recall 0.9895833333333334\n",
      "Epoch 10 Batch 11: Loss 0.20380191504955292, Accuracy 0.9886363636363636, F1 0.9887699854611618, Precision 0.9902304292929291, Recall 0.9886363636363636\n",
      "Epoch 10 Batch 16: Loss 0.005810343660414219, Accuracy 0.984375, F1 0.9844845403439153, Precision 0.986563740079365, Recall 0.984375\n",
      "Epoch 10 Batch 21: Loss 0.0035807613749057055, Accuracy 0.9821428571428571, F1 0.9822160174611154, Precision 0.9842670433072219, Recall 0.9821428571428571\n",
      "Epoch 10 Batch 26: Loss 0.05794183164834976, Accuracy 0.9759615384615384, F1 0.9760835012330347, Precision 0.9791958475552226, Recall 0.9759615384615384\n",
      "Epoch 10 Batch 31: Loss 0.00419258838519454, Accuracy 0.9737903225806451, F1 0.9740253642515858, Precision 0.9786882836025982, Recall 0.9737903225806451\n",
      "Epoch 10 Batch 36: Loss 0.16306547820568085, Accuracy 0.9748263888888888, F1 0.9750005801565749, Precision 0.9793358406639658, Recall 0.9748263888888888\n",
      "Epoch 10 Batch 41: Loss 0.012411954812705517, Accuracy 0.9733231707317073, F1 0.9735358386653528, Precision 0.9783252634052939, Recall 0.9733231707317073\n",
      "Epoch 10 Batch 46: Loss 0.0032990099862217903, Accuracy 0.9735054347826086, F1 0.9737176377162333, Precision 0.9783434183570053, Recall 0.9735054347826086\n",
      "Epoch 10 Batch 51: Loss 0.06360052525997162, Accuracy 0.9730392156862745, F1 0.9731789333847746, Precision 0.9778908336353189, Recall 0.9730392156862745\n",
      "Epoch 10 Batch 56: Loss 0.003694395534694195, Accuracy 0.9748883928571429, F1 0.9750081952849435, Precision 0.9793497564563413, Recall 0.9748883928571429\n",
      "Epoch 10 Batch 61: Loss 0.00243538455106318, Accuracy 0.9764344262295082, F1 0.976535841978787, Precision 0.9805727955446193, Recall 0.9764344262295082\n",
      "Epoch 10 Batch 66: Loss 0.18621371686458588, Accuracy 0.9763257575757576, F1 0.9764128565301796, Precision 0.9804037029889303, Recall 0.9763257575757576\n",
      "Epoch 10 Batch 71: Loss 0.021568024531006813, Accuracy 0.9766725352112676, F1 0.9767355390304785, Precision 0.9805913423431029, Recall 0.9766725352112676\n",
      "Epoch 10 Batch 76: Loss 0.0342448465526104, Accuracy 0.9769736842105263, F1 0.9770588999432102, Precision 0.9808228230103228, Recall 0.9769736842105263\n",
      "Epoch 10 Batch 81: Loss 0.12131117284297943, Accuracy 0.9764660493827161, F1 0.9765813913543409, Precision 0.9804879405573848, Recall 0.9764660493827161\n",
      "Epoch 10 Batch 86: Loss 0.11925063282251358, Accuracy 0.9771075581395349, F1 0.9772138253703584, Precision 0.980981414620586, Recall 0.9771075581395349\n",
      "Epoch 10 Batch 91: Loss 0.007465702015906572, Accuracy 0.978021978021978, F1 0.9781300376515965, Precision 0.9817516665645097, Recall 0.978021978021978\n",
      "Epoch 10 Batch 96: Loss 0.006637809332460165, Accuracy 0.9791666666666666, F1 0.9792690981905757, Precision 0.9827021005976082, Recall 0.9791666666666666\n",
      "Epoch 10: Train Accuracy 0.9794823232323232, F1 0.9795867303016009, Precision 0.9829500798724281, Recall 0.9794823232323232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da85ea7fbf90456a9b41b4321f3f3b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Validation Accuracy 0.8801136363636364, F1 0.8815736163506146, Precision 0.896284541847042, Recall 0.8801136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10af9545a963412fa68f7f7da93d5837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 1: Loss 0.004498678259551525, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 11 Batch 6: Loss 0.003353063715621829, Accuracy 0.9947916666666666, F1 0.9949151844532279, Precision 0.9956597222222223, Recall 0.9947916666666666\n",
      "Epoch 11 Batch 11: Loss 0.08026280999183655, Accuracy 0.9943181818181818, F1 0.994338048315321, Precision 0.9950284090909091, Recall 0.9943181818181818\n",
      "Epoch 11 Batch 16: Loss 0.006101001054048538, Accuracy 0.994140625, F1 0.9941542832167832, Precision 0.9947916666666666, Recall 0.994140625\n",
      "Epoch 11 Batch 21: Loss 0.10253213346004486, Accuracy 0.9910714285714286, F1 0.9910931082806084, Precision 0.9921987734487733, Recall 0.9910714285714286\n",
      "Epoch 11 Batch 26: Loss 0.0028571072034537792, Accuracy 0.9927884615384616, F1 0.9928059720727991, Precision 0.9936990093240092, Recall 0.9927884615384616\n",
      "Epoch 11 Batch 31: Loss 0.013596617616713047, Accuracy 0.9919354838709677, F1 0.9920924851160857, Precision 0.9933712121212119, Recall 0.9919354838709677\n",
      "Epoch 11 Batch 36: Loss 0.0025205493438988924, Accuracy 0.9921875, F1 0.9923327599288676, Precision 0.9935323284932659, Recall 0.9921875\n",
      "Epoch 11 Batch 41: Loss 0.002692251466214657, Accuracy 0.993140243902439, F1 0.9932677892058349, Precision 0.9943210689209163, Recall 0.993140243902439\n",
      "Epoch 11 Batch 46: Loss 0.0019143075915053487, Accuracy 0.9925271739130435, F1 0.9926559522148625, Precision 0.993791001043039, Recall 0.9925271739130435\n",
      "Epoch 11 Batch 51: Loss 0.09377333521842957, Accuracy 0.9926470588235294, F1 0.9927632118016407, Precision 0.9938745163469427, Recall 0.9926470588235294\n",
      "Epoch 11 Batch 56: Loss 0.00215169251896441, Accuracy 0.9927455357142857, F1 0.9928790463734772, Precision 0.9940029077445371, Recall 0.9927455357142857\n",
      "Epoch 11 Batch 61: Loss 0.0017380508361384273, Accuracy 0.9933401639344263, F1 0.9934627310969627, Precision 0.9944944726835094, Recall 0.9933401639344263\n",
      "Epoch 11 Batch 66: Loss 0.0020431065931916237, Accuracy 0.9928977272727273, F1 0.9929941740273275, Precision 0.9940601959067866, Recall 0.9928977272727273\n",
      "Epoch 11 Batch 71: Loss 0.11673881113529205, Accuracy 0.9920774647887324, F1 0.9921530759811193, Precision 0.9932808437649985, Recall 0.9920774647887324\n",
      "Epoch 11 Batch 76: Loss 0.0074455044232308865, Accuracy 0.9925986842105263, F1 0.9926693209823615, Precision 0.9937228935173013, Recall 0.9925986842105263\n",
      "Epoch 11 Batch 81: Loss 0.0025849812664091587, Accuracy 0.9930555555555556, F1 0.993121832032833, Precision 0.9941103692261097, Recall 0.9930555555555556\n",
      "Epoch 11 Batch 86: Loss 0.0529390424489975, Accuracy 0.9927325581395349, F1 0.9927872883878003, Precision 0.9938118416225247, Recall 0.9927325581395349\n",
      "Epoch 11 Batch 91: Loss 0.0017256647115573287, Accuracy 0.9931318681318682, F1 0.9931835912236354, Precision 0.9941518503245838, Recall 0.9931318681318682\n",
      "Epoch 11 Batch 96: Loss 0.0017244248883798718, Accuracy 0.9931640625, F1 0.9932065998122667, Precision 0.9941541721082735, Recall 0.9931640625\n",
      "Epoch 11: Train Accuracy 0.9933712121212122, F1 0.9934124604240161, Precision 0.9943313184080228, Recall 0.9933712121212122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199d314a501146e7b37aa1a99fad2995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Validation Accuracy 0.8495454545454545, F1 0.8505570788510072, Precision 0.8707186667499167, Recall 0.8495454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565758f9c2cc482d811dba3c226a1736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 1: Loss 0.0016886790981516242, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 12 Batch 6: Loss 0.002253662096336484, Accuracy 0.9895833333333334, F1 0.9894531249999999, Precision 0.9914772727272728, Recall 0.9895833333333334\n",
      "Epoch 12 Batch 11: Loss 0.08221054077148438, Accuracy 0.9914772727272727, F1 0.9914221654570919, Precision 0.9928259871441689, Recall 0.9914772727272727\n",
      "Epoch 12 Batch 16: Loss 0.0014617096167057753, Accuracy 0.990234375, F1 0.9903112137805582, Precision 0.991928915268759, Recall 0.990234375\n",
      "Epoch 12 Batch 21: Loss 0.2217465341091156, Accuracy 0.9880952380952381, F1 0.9881402537895163, Precision 0.990179967188896, Recall 0.9880952380952381\n",
      "Epoch 12 Batch 26: Loss 0.001568147912621498, Accuracy 0.9867788461538461, F1 0.9868043573433485, Precision 0.9889467737123988, Recall 0.9867788461538461\n",
      "Epoch 12 Batch 31: Loss 0.001758308382704854, Accuracy 0.9868951612903226, F1 0.9868524707552918, Precision 0.9888917729895553, Recall 0.9868951612903226\n",
      "Epoch 12 Batch 36: Loss 0.0016517759067937732, Accuracy 0.9878472222222222, F1 0.9877977421654491, Precision 0.9896454408824202, Recall 0.9878472222222222\n",
      "Epoch 12 Batch 41: Loss 0.007338626775890589, Accuracy 0.9878048780487805, F1 0.9877498336793018, Precision 0.9895552956528567, Recall 0.9878048780487805\n",
      "Epoch 12 Batch 46: Loss 0.0013258117251098156, Accuracy 0.9891304347826086, F1 0.989081373496769, Precision 0.9906905896036331, Recall 0.9891304347826086\n",
      "Epoch 12 Batch 51: Loss 0.00162361073307693, Accuracy 0.9901960784313726, F1 0.9901518270755171, Precision 0.9916032768973946, Recall 0.9901960784313726\n",
      "Epoch 12 Batch 56: Loss 0.0014770639827474952, Accuracy 0.9910714285714286, F1 0.9910311282294888, Precision 0.99235298431727, Recall 0.9910714285714286\n",
      "Epoch 12 Batch 61: Loss 0.0014383663656190038, Accuracy 0.9912909836065574, F1 0.9912454197639433, Precision 0.9925101850562916, Recall 0.9912909836065574\n",
      "Epoch 12 Batch 66: Loss 0.0014822171069681644, Accuracy 0.9919507575757576, F1 0.9919086455394022, Precision 0.9930775952792997, Recall 0.9919507575757576\n",
      "Epoch 12 Batch 71: Loss 0.030115481466054916, Accuracy 0.9920774647887324, F1 0.9920416954284722, Precision 0.9931738522627607, Recall 0.9920774647887324\n",
      "Epoch 12 Batch 76: Loss 0.009560181759297848, Accuracy 0.9917763157894737, F1 0.9917614493402317, Precision 0.9929033685612633, Recall 0.9917763157894737\n",
      "Epoch 12 Batch 81: Loss 0.0012314054183661938, Accuracy 0.9918981481481481, F1 0.9918812392597116, Precision 0.9929877799669466, Recall 0.9918981481481481\n",
      "Epoch 12 Batch 86: Loss 0.0011572297662496567, Accuracy 0.9920058139534884, F1 0.9919812706199682, Precision 0.9930623760928995, Recall 0.9920058139534884\n",
      "Epoch 12 Batch 91: Loss 0.0018237355398014188, Accuracy 0.992445054945055, F1 0.9924218601463436, Precision 0.9934435642196632, Recall 0.992445054945055\n",
      "Epoch 12 Batch 96: Loss 0.002664472907781601, Accuracy 0.9928385416666666, F1 0.9928165549303882, Precision 0.9937850452498891, Recall 0.9928385416666666\n",
      "Epoch 12: Train Accuracy 0.9924242424242424, F1 0.99240676393612, Precision 0.9934338914452551, Recall 0.9924242424242424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc6fb6329d34a108f2b11c21deeb564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Validation Accuracy 0.8607954545454546, F1 0.8625257409361066, Precision 0.8781551295926296, Recall 0.8607954545454546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94db8f8dc0464239a804b6b097ee0e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch 1: Loss 0.002477988600730896, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 13 Batch 6: Loss 0.0027829918544739485, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 13 Batch 11: Loss 0.0019935413729399443, Accuracy 0.9971590909090909, F1 0.9971790271132377, Precision 0.9975142045454546, Recall 0.9971590909090909\n",
      "Epoch 13 Batch 16: Loss 0.001285164151340723, Accuracy 0.994140625, F1 0.9941780053827751, Precision 0.994873046875, Recall 0.994140625\n",
      "Epoch 13 Batch 21: Loss 0.0012450219364836812, Accuracy 0.9955357142857143, F1 0.9955641945773525, Precision 0.99609375, Recall 0.9955357142857143\n",
      "Epoch 13 Batch 26: Loss 0.0011761484201997519, Accuracy 0.9939903846153846, F1 0.9941044427064164, Precision 0.9951279189560439, Recall 0.9939903846153846\n",
      "Epoch 13 Batch 31: Loss 0.0011663883924484253, Accuracy 0.9949596774193549, F1 0.9950553390440912, Precision 0.9959137384792627, Recall 0.9949596774193549\n",
      "Epoch 13 Batch 36: Loss 0.010988502763211727, Accuracy 0.9956597222222222, F1 0.9957420975101896, Precision 0.9964812748015872, Recall 0.9956597222222222\n",
      "Epoch 13 Batch 41: Loss 0.0022618037182837725, Accuracy 0.9961890243902439, F1 0.996261353911386, Precision 0.996910387630662, Recall 0.9961890243902439\n",
      "Epoch 13 Batch 46: Loss 0.0010447920067235827, Accuracy 0.9966032608695652, F1 0.9966677284862354, Precision 0.9972462150621118, Recall 0.9966032608695652\n",
      "Epoch 13 Batch 51: Loss 0.0010648644529283047, Accuracy 0.9963235294117647, F1 0.9963898466084998, Precision 0.9969800420168067, Recall 0.9963235294117647\n",
      "Epoch 13 Batch 56: Loss 0.0012276257621124387, Accuracy 0.9966517857142857, F1 0.996712181732741, Precision 0.9972496811224489, Recall 0.9966517857142857\n",
      "Epoch 13 Batch 61: Loss 0.0009152431739494205, Accuracy 0.9969262295081968, F1 0.996981675033336, Precision 0.9974751170960187, Recall 0.9969262295081968\n",
      "Epoch 13 Batch 66: Loss 0.06740694493055344, Accuracy 0.9966856060606061, F1 0.9967256222774867, Precision 0.9972323683261184, Recall 0.9966856060606061\n",
      "Epoch 13 Batch 71: Loss 0.0010300881695002317, Accuracy 0.996919014084507, F1 0.9969562122579454, Precision 0.997427271965124, Recall 0.996919014084507\n",
      "Epoch 13 Batch 76: Loss 0.0008573001250624657, Accuracy 0.9967105263157895, F1 0.9967386807028386, Precision 0.9972310333124479, Recall 0.9967105263157895\n",
      "Epoch 13 Batch 81: Loss 0.0009158258908428252, Accuracy 0.9969135802469136, F1 0.9969399967088362, Precision 0.9974019571820498, Recall 0.9969135802469136\n",
      "Epoch 13 Batch 86: Loss 0.000900674203876406, Accuracy 0.997093023255814, F1 0.9971179038769271, Precision 0.9975530061830935, Recall 0.997093023255814\n",
      "Epoch 13 Batch 91: Loss 0.0008919208776205778, Accuracy 0.9972527472527473, F1 0.9972762608067662, Precision 0.9976874563928136, Recall 0.9972527472527473\n",
      "Epoch 13 Batch 96: Loss 0.0008373204618692398, Accuracy 0.9973958333333334, F1 0.9974181222230806, Precision 0.9978079013723545, Recall 0.9973958333333334\n",
      "Epoch 13: Train Accuracy 0.9974747474747475, F1 0.9974963609435933, Precision 0.9978743286034953, Recall 0.9974747474747475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255b3657759d4ae1b0976aa8bd1a8201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Validation Accuracy 0.8788636363636364, F1 0.8806110394269636, Precision 0.8970923798423798, Recall 0.8788636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9636d8a8662b4dbe9934700a13b5cad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 1: Loss 0.02729014866054058, Accuracy 0.96875, F1 0.9693480861244019, Precision 0.9739583333333334, Recall 0.96875\n",
      "Epoch 14 Batch 6: Loss 0.001035521738231182, Accuracy 0.984375, F1 0.984411421911422, Precision 0.9863425925925927, Recall 0.984375\n",
      "Epoch 14 Batch 11: Loss 0.000817229738458991, Accuracy 0.9914772727272727, F1 0.991497139224412, Precision 0.9925505050505051, Recall 0.9914772727272727\n",
      "Epoch 14 Batch 16: Loss 0.0009592381538823247, Accuracy 0.994140625, F1 0.9941542832167832, Precision 0.9948784722222223, Recall 0.994140625\n",
      "Epoch 14 Batch 21: Loss 0.001482224091887474, Accuracy 0.9955357142857143, F1 0.9955461205461207, Precision 0.9960978835978835, Recall 0.9955357142857143\n",
      "Epoch 14 Batch 26: Loss 0.0008338656625710428, Accuracy 0.9963942307692307, F1 0.9964026358257129, Precision 0.9968482905982905, Recall 0.9963942307692307\n",
      "Epoch 14 Batch 31: Loss 0.0008067113813012838, Accuracy 0.9969758064516129, F1 0.9969828558538236, Precision 0.9973566308243728, Recall 0.9969758064516129\n",
      "Epoch 14 Batch 36: Loss 0.021716192364692688, Accuracy 0.9973958333333334, F1 0.9974019036519037, Precision 0.9977237654320987, Recall 0.9973958333333334\n",
      "Epoch 14 Batch 41: Loss 0.0019405829953029752, Accuracy 0.9969512195121951, F1 0.9969473108192621, Precision 0.9973344342818428, Recall 0.9969512195121951\n",
      "Epoch 14 Batch 46: Loss 0.0007863419014029205, Accuracy 0.9966032608695652, F1 0.9966227501494515, Precision 0.9970806914251208, Recall 0.9966032608695652\n",
      "Epoch 14 Batch 51: Loss 0.005304461345076561, Accuracy 0.9969362745098039, F1 0.9969538530759758, Precision 0.9973668981481482, Recall 0.9969362745098039\n",
      "Epoch 14 Batch 56: Loss 0.0007850535912439227, Accuracy 0.9966517857142857, F1 0.9966706564872325, Precision 0.9971236802012472, Recall 0.9966517857142857\n",
      "Epoch 14 Batch 61: Loss 0.0007486284011974931, Accuracy 0.9969262295081968, F1 0.9969435534964758, Precision 0.9973594441191778, Recall 0.9969262295081968\n",
      "Epoch 14 Batch 66: Loss 0.0009232900338247418, Accuracy 0.9966856060606061, F1 0.9967106795365064, Precision 0.9971649155242904, Recall 0.9966856060606061\n",
      "Epoch 14 Batch 71: Loss 0.0007514369208365679, Accuracy 0.996919014084507, F1 0.9969423218226678, Precision 0.997364569360608, Recall 0.996919014084507\n",
      "Epoch 14 Batch 76: Loss 0.000775472610257566, Accuracy 0.9971217105263158, F1 0.9971434848606503, Precision 0.9975379529553049, Recall 0.9971217105263158\n",
      "Epoch 14 Batch 81: Loss 0.0006953698466531932, Accuracy 0.9972993827160493, F1 0.9973198129556718, Precision 0.9976899311679404, Recall 0.9972993827160493\n",
      "Epoch 14 Batch 86: Loss 0.0006643056985922158, Accuracy 0.9967296511627907, F1 0.9967372167319511, Precision 0.9971620927925433, Recall 0.9967296511627907\n",
      "Epoch 14 Batch 91: Loss 0.0006751071196049452, Accuracy 0.9969093406593407, F1 0.996916490537888, Precision 0.9973180217599861, Recall 0.9969093406593407\n",
      "Epoch 14 Batch 96: Loss 0.000689125619828701, Accuracy 0.9970703125, F1 0.9970770899890397, Precision 0.9974577081266535, Recall 0.9970703125\n",
      "Epoch 14: Train Accuracy 0.9971590909090909, F1 0.9971656630196748, Precision 0.9975347472743306, Recall 0.9971590909090909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5c15949bf407a9808e839e2e53558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Validation Accuracy 0.8489772727272726, F1 0.8509765954197499, Precision 0.8766583832833834, Recall 0.8489772727272726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aca0e748fa145548d78c91e2b2c5e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 1: Loss 0.0006244492833502591, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 15 Batch 6: Loss 0.0008194997208192945, Accuracy 0.9895833333333334, F1 0.9891493055555555, Precision 0.9911858974358975, Recall 0.9895833333333334\n",
      "Epoch 15 Batch 11: Loss 0.0006219166098162532, Accuracy 0.9914772727272727, F1 0.9912604665071769, Precision 0.9927065122377623, Recall 0.9914772727272727\n",
      "Epoch 15 Batch 16: Loss 0.0007170430617406964, Accuracy 0.994140625, F1 0.9939915707236842, Precision 0.9949857271634616, Recall 0.994140625\n",
      "Epoch 15 Batch 21: Loss 0.0008417822537012398, Accuracy 0.9955357142857143, F1 0.995422149122807, Precision 0.9961796016483517, Recall 0.9955357142857143\n",
      "Epoch 15 Batch 26: Loss 0.0007195378420874476, Accuracy 0.9963942307692307, F1 0.9963025050607287, Precision 0.9969142936390534, Recall 0.9963942307692307\n",
      "Epoch 15 Batch 31: Loss 0.0007755588740110397, Accuracy 0.9969758064516129, F1 0.9968988752122241, Precision 0.9974119882133996, Recall 0.9969758064516129\n",
      "Epoch 15 Batch 36: Loss 0.0005942340940237045, Accuracy 0.9973958333333334, F1 0.9973295869883041, Precision 0.9977714342948718, Recall 0.9973958333333334\n",
      "Epoch 15 Batch 41: Loss 0.0006965601351112127, Accuracy 0.9977134146341463, F1 0.9976552471116816, Precision 0.9980432106003753, Recall 0.9977134146341463\n",
      "Epoch 15 Batch 46: Loss 0.000617686309851706, Accuracy 0.9979619565217391, F1 0.997910111556064, Precision 0.9982559051003345, Recall 0.9979619565217391\n",
      "Epoch 15 Batch 51: Loss 0.017208868637681007, Accuracy 0.9981617647058824, F1 0.9981150025799793, Precision 0.9984268947963801, Recall 0.9981617647058824\n",
      "Epoch 15 Batch 56: Loss 0.0007751339580863714, Accuracy 0.9983258928571429, F1 0.9982833059210526, Precision 0.998567350618132, Recall 0.9983258928571429\n",
      "Epoch 15 Batch 61: Loss 0.0006083260523155332, Accuracy 0.9979508196721312, F1 0.9979228121066451, Precision 0.9982578683270282, Recall 0.9979508196721312\n",
      "Epoch 15 Batch 66: Loss 0.0006510532111860812, Accuracy 0.9976325757575758, F1 0.9975985962192603, Precision 0.9979840038433789, Recall 0.9976325757575758\n",
      "Epoch 15 Batch 71: Loss 0.0034547285176813602, Accuracy 0.9977992957746479, F1 0.9977677091615659, Precision 0.9981259754037043, Recall 0.9977992957746479\n",
      "Epoch 15 Batch 76: Loss 0.0006800488336011767, Accuracy 0.9979440789473685, F1 0.9979145704009366, Precision 0.9982492664955659, Recall 0.9979440789473685\n",
      "Epoch 15 Batch 81: Loss 0.0007827977533452213, Accuracy 0.998070987654321, F1 0.998043300623101, Precision 0.9983573364649754, Recall 0.998070987654321\n",
      "Epoch 15 Batch 86: Loss 0.0006739916279911995, Accuracy 0.9981831395348837, F1 0.9981570622147811, Precision 0.9984528401588721, Recall 0.9981831395348837\n",
      "Epoch 15 Batch 91: Loss 0.0005725970841012895, Accuracy 0.998282967032967, F1 0.9982583225326502, Precision 0.9985378489413518, Recall 0.998282967032967\n",
      "Epoch 15 Batch 96: Loss 0.0018448707414790988, Accuracy 0.9983723958333334, F1 0.9983490349007414, Precision 0.998614002642323, Recall 0.9983723958333334\n",
      "Epoch 15: Train Accuracy 0.9984217171717171, F1 0.9983990641461735, Precision 0.9986560025622526, Recall 0.9984217171717171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995e1e0ed96e49d180f55ad153ff03ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Validation Accuracy 0.8745454545454545, F1 0.8749064663119145, Precision 0.8860141594516593, Recall 0.8745454545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20900e2658b743e7a51b9bcf50b07666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Batch 1: Loss 0.000629427086096257, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 6: Loss 0.0005886885919608176, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 11: Loss 0.000562475121114403, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 16: Loss 0.0005642540054395795, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 21: Loss 0.000616841425653547, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 26: Loss 0.0005838174838572741, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 31: Loss 0.0005438397638499737, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 36: Loss 0.0005269115208648145, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 41: Loss 0.0006387392058968544, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 46: Loss 0.0005527052562683821, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 51: Loss 0.00048226796207018197, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 56: Loss 0.000601071456912905, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 61: Loss 0.0005426033167168498, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 66: Loss 0.0006204068777151406, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 71: Loss 0.0005438635125756264, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 76: Loss 0.0005420479574240744, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 81: Loss 0.0005180796724744141, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 86: Loss 0.0005106069729663432, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 91: Loss 0.000487716548377648, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16 Batch 96: Loss 0.000471385515993461, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 16: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cfbce7438244c2a43ff6c7d89537fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Validation Accuracy 0.8782954545454545, F1 0.8791952055197723, Precision 0.8932078026140524, Recall 0.8782954545454545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d56b7194848079d23b18dd3f288ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Batch 1: Loss 0.0004896401660516858, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 17 Batch 6: Loss 0.0004688215267378837, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 17 Batch 11: Loss 0.0004608867457136512, Accuracy 0.9971590909090909, F1 0.9971174658674659, Precision 0.9974173553719009, Recall 0.9971590909090909\n",
      "Epoch 17 Batch 16: Loss 0.0005164393223822117, Accuracy 0.998046875, F1 0.9980182577838828, Precision 0.9982244318181819, Recall 0.998046875\n",
      "Epoch 17 Batch 21: Loss 0.0004956576158292592, Accuracy 0.9985119047619048, F1 0.9984901011686725, Precision 0.9986471861471862, Recall 0.9985119047619048\n",
      "Epoch 17 Batch 26: Loss 0.001076833694241941, Accuracy 0.9987980769230769, F1 0.9987804663285432, Precision 0.9989073426573427, Recall 0.9987980769230769\n",
      "Epoch 17 Batch 31: Loss 0.001744359964504838, Accuracy 0.998991935483871, F1 0.9989771653078104, Precision 0.99908357771261, Recall 0.998991935483871\n",
      "Epoch 17 Batch 36: Loss 0.0004385695210658014, Accuracy 0.9991319444444444, F1 0.9991192256817256, Precision 0.9992108585858586, Recall 0.9991319444444444\n",
      "Epoch 17 Batch 41: Loss 0.00046048802323639393, Accuracy 0.9992378048780488, F1 0.9992266371839542, Precision 0.9993070953436807, Recall 0.9992378048780488\n",
      "Epoch 17 Batch 46: Loss 0.0004229499609209597, Accuracy 0.9993206521739131, F1 0.9993106983596114, Precision 0.9993824110671936, Recall 0.9993206521739131\n",
      "Epoch 17 Batch 51: Loss 0.0004657601530198008, Accuracy 0.9993872549019608, F1 0.9993782769518064, Precision 0.9994429590017825, Recall 0.9993872549019608\n",
      "Epoch 17 Batch 56: Loss 0.000495859538204968, Accuracy 0.9994419642857143, F1 0.9994337879382522, Precision 0.9994926948051948, Recall 0.9994419642857143\n",
      "Epoch 17 Batch 61: Loss 0.00047373841516673565, Accuracy 0.9994877049180327, F1 0.9994801987629857, Precision 0.9995342771982115, Recall 0.9994877049180327\n",
      "Epoch 17 Batch 66: Loss 0.00044454896124079823, Accuracy 0.9995265151515151, F1 0.9995195776445778, Precision 0.9995695592286501, Recall 0.9995265151515151\n",
      "Epoch 17 Batch 71: Loss 0.0004875635786447674, Accuracy 0.9995598591549296, F1 0.9995534102048188, Precision 0.9995998719590269, Recall 0.9995598591549296\n",
      "Epoch 17 Batch 76: Loss 0.0004742254095617682, Accuracy 0.9995888157894737, F1 0.9995827911123965, Precision 0.9996261961722488, Recall 0.9995888157894737\n",
      "Epoch 17 Batch 81: Loss 0.0004527347336988896, Accuracy 0.9996141975308642, F1 0.9996085447474338, Precision 0.9996492704826038, Recall 0.9996141975308642\n",
      "Epoch 17 Batch 86: Loss 0.00043038366129621863, Accuracy 0.9996366279069767, F1 0.9996313037737458, Precision 0.9996696617336152, Recall 0.9996366279069767\n",
      "Epoch 17 Batch 91: Loss 0.0004347867798060179, Accuracy 0.9996565934065934, F1 0.9996515618081553, Precision 0.9996878121878121, Recall 0.9996565934065934\n",
      "Epoch 17 Batch 96: Loss 0.00044100978993810713, Accuracy 0.9996744791666666, F1 0.9996697096306472, Precision 0.9997040719696969, Recall 0.9996744791666666\n",
      "Epoch 17: Train Accuracy 0.9996843434343434, F1 0.9996797184297185, Precision 0.9997130394857667, Recall 0.9996843434343434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8921d5a82ff4c55a074e9615ef5b3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Validation Accuracy 0.8801136363636364, F1 0.8804978642337505, Precision 0.8923566017316016, Recall 0.8801136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c3473d9680452e9aa1c163275a0ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch 1: Loss 0.0004139407828915864, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 6: Loss 0.000442788063082844, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 11: Loss 0.0004259456181898713, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 16: Loss 0.00045067816972732544, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 21: Loss 0.00044479992357082665, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 26: Loss 0.00044859107583761215, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 31: Loss 0.0004288777709007263, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 36: Loss 0.0003844821476377547, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 41: Loss 0.00042341413791291416, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 46: Loss 0.0004527439014054835, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 51: Loss 0.00036614882992580533, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 56: Loss 0.0003804609877988696, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 61: Loss 0.0004019765474367887, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 66: Loss 0.00040889543015509844, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 71: Loss 0.00044631530181504786, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 76: Loss 0.00041233887895941734, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 81: Loss 0.0004223249270580709, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 86: Loss 0.0004147760337218642, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 91: Loss 0.00042243857751600444, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18 Batch 96: Loss 0.0004180247487965971, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 18: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b823e696a6e4c8099b89159db2d4659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Validation Accuracy 0.8826136363636364, F1 0.8829713334459969, Precision 0.8944961219336218, Recall 0.8826136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4c979439bb4fb3a3c881d1ffe04869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Batch 1: Loss 0.0004097222408745438, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 6: Loss 0.0003893169923685491, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 11: Loss 0.0003738325322046876, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 16: Loss 0.00036285430542193353, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 21: Loss 0.0004103122337255627, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 26: Loss 0.00039417794323526323, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 31: Loss 0.0003852069203276187, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 36: Loss 0.0003473426913842559, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 41: Loss 0.0004158473457209766, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 46: Loss 0.00036501025897450745, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 51: Loss 0.0003463868924882263, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 56: Loss 0.0003970602701883763, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 61: Loss 0.00036355378688313067, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 66: Loss 0.0003888904757332057, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 71: Loss 0.00038980203680694103, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 76: Loss 0.00038964158738963306, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 81: Loss 0.0004156262439209968, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 86: Loss 0.0003740571846719831, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 91: Loss 0.00038963367114774883, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19 Batch 96: Loss 0.0004086332628503442, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 19: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32c143bab784f1ebdc68222e9b5e685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Validation Accuracy 0.8851136363636364, F1 0.8854867319967216, Precision 0.896562094155844, Recall 0.8851136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508f93d9f6e6431daf8758f62a8a812e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Batch 1: Loss 0.0003812018549069762, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 6: Loss 0.00035182651481591165, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 11: Loss 0.0003675210173241794, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 16: Loss 0.0003393021470401436, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 21: Loss 0.00039440832915715873, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 26: Loss 0.0004375154385343194, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 31: Loss 0.00037520116893574595, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 36: Loss 0.0003577590105123818, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 41: Loss 0.00039231512346304953, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 46: Loss 0.00034019138547591865, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 51: Loss 0.0003468851209618151, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 56: Loss 0.0003952624392695725, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 61: Loss 0.0003663802635855973, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 66: Loss 0.0003631067229434848, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 71: Loss 0.0003716682840604335, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 76: Loss 0.0003683357499539852, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 81: Loss 0.0003580186457838863, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 86: Loss 0.000353994284523651, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 91: Loss 0.0003686776908580214, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20 Batch 96: Loss 0.0003664795949589461, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 20: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6a0a7e948a4da6accff25a86a5fb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Validation Accuracy 0.8851136363636364, F1 0.8854867319967216, Precision 0.896562094155844, Recall 0.8851136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd890cccf1047d2b34039c4b64229bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Batch 1: Loss 0.00036575162084773183, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 6: Loss 0.0003663964744191617, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 11: Loss 0.0003814806113950908, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 16: Loss 0.0003262542886659503, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 21: Loss 0.00034552376018837094, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 26: Loss 0.000341538165230304, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 31: Loss 0.0003823445877060294, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 36: Loss 0.0003250954032409936, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 41: Loss 0.0003930218517780304, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 46: Loss 0.0003540912293829024, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 51: Loss 0.000319832208333537, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 56: Loss 0.0003617639886215329, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 61: Loss 0.00034387654159218073, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 66: Loss 0.0003476065176073462, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 71: Loss 0.00033733173040673137, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 76: Loss 0.0003664161777123809, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 81: Loss 0.0003553085552994162, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 86: Loss 0.0003416576364543289, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 91: Loss 0.0003244833496864885, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21 Batch 96: Loss 0.00036390841705724597, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 21: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a76581bc44b2b83f528859923cfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Validation Accuracy 0.8863636363636364, F1 0.8867817308575099, Precision 0.8979484577922077, Recall 0.8863636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26a83a5f492471b83c31d221abeaf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch 1: Loss 0.0003389438788872212, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 6: Loss 0.0003312721091788262, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 11: Loss 0.0003143076319247484, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 16: Loss 0.0003775100049097091, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 21: Loss 0.00038110598688945174, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 26: Loss 0.0003608916013035923, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 31: Loss 0.0008354345336556435, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 36: Loss 0.0003020676667802036, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 41: Loss 0.00032286031637340784, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 46: Loss 0.0003102005284745246, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 51: Loss 0.0003225869149900973, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 56: Loss 0.0003274128830526024, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 61: Loss 0.000313944328809157, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 66: Loss 0.0003341220726724714, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 71: Loss 0.0003357818932272494, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 76: Loss 0.0003673670580610633, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 81: Loss 0.00034926572698168457, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 86: Loss 0.00031429974478669465, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 91: Loss 0.00033697838080115616, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22 Batch 96: Loss 0.0003606939862947911, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 22: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5373ab18bb61489f8151ce9a1da24e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Validation Accuracy 0.8851136363636364, F1 0.8854443182700972, Precision 0.8963413149350649, Recall 0.8851136363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26a6e05434495ca0b08ebe179ffea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Batch 1: Loss 0.0003480626328382641, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 6: Loss 0.00036855845246464014, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 11: Loss 0.0009042279561981559, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 16: Loss 0.0003270501038059592, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 21: Loss 0.0003544530482031405, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 26: Loss 0.00035689937067218125, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 31: Loss 0.00034627164131961763, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 36: Loss 0.000305442837998271, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 41: Loss 0.00034642411628738046, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 46: Loss 0.00031778699485585093, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 51: Loss 0.00033799230004660785, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 56: Loss 0.0003721249522641301, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 61: Loss 0.00032096137874759734, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 66: Loss 0.00034550853888504207, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 71: Loss 0.0003292238397989422, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 76: Loss 0.0003534922143444419, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 81: Loss 0.000350977381458506, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 86: Loss 0.00030857391539029777, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 91: Loss 0.00032184305018745363, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23 Batch 96: Loss 0.0007438608445227146, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 23: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0aeaad656b44418b988090464f2a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Validation Accuracy 0.8863636363636364, F1 0.8867817308575099, Precision 0.8979484577922077, Recall 0.8863636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32857efec8c4645b5a2bde1608f77c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Batch 1: Loss 0.00031843234319239855, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 6: Loss 0.00031446179491467774, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 11: Loss 0.0003106816438958049, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 16: Loss 0.0003256040799897164, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 21: Loss 0.0003627222904469818, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 26: Loss 0.00035469993599690497, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 31: Loss 0.0003935776185244322, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 36: Loss 0.00031979309278540313, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 41: Loss 0.0003033001848962158, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 46: Loss 0.00032758031738922, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 51: Loss 0.00028838677098974586, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 56: Loss 0.00031470644171349704, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 61: Loss 0.0003242113161832094, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 66: Loss 0.00033948838245123625, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 71: Loss 0.0003472462994977832, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 76: Loss 0.0003227192210033536, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 81: Loss 0.00036783734685741365, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 86: Loss 0.00033170884125865996, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 91: Loss 0.0003535100840963423, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24 Batch 96: Loss 0.00032746169017627835, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 24: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0394db5de34ca79e25ec7f466f2079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Validation Accuracy 0.8863636363636364, F1 0.8867817308575099, Precision 0.8979484577922077, Recall 0.8863636363636364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcf3710b57b3408e85264fb65ebceb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Batch 1: Loss 0.0003316677757538855, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 6: Loss 0.00035466247936710715, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 11: Loss 0.00033607034129090607, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 16: Loss 0.0003151815908495337, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 21: Loss 0.00037073122803121805, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 26: Loss 0.0003300454409327358, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 31: Loss 0.0003345369768794626, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 36: Loss 0.00030848829192109406, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 41: Loss 0.00034257848164997995, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 46: Loss 0.00031580845825374126, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 51: Loss 0.0003171298303641379, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 56: Loss 0.00038197555113583803, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 61: Loss 0.00032735036802478135, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 66: Loss 0.0003385113668628037, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 71: Loss 0.00033961026929318905, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 76: Loss 0.0003289768937975168, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 81: Loss 0.000364541367162019, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 86: Loss 0.00034027767833322287, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 91: Loss 0.0003143211069982499, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25 Batch 96: Loss 0.0003454094403423369, Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n",
      "Epoch 25: Train Accuracy 1.0, F1 1.0, Precision 1.0, Recall 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3821735571.py:62: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f77bdc9cac4c49bac5b519623f297e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Validation Accuracy 0.8863636363636364, F1 0.8867817308575099, Precision 0.8979484577922077, Recall 0.8863636363636364\n"
     ]
    }
   ],
   "source": [
    "# 로스와 성능 지표 저장 리스트\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs, val_accs = [], []\n",
    "train_f1s, val_f1s = [], []\n",
    "train_precisions, val_precisions = [], []\n",
    "train_recalls = []  \n",
    "val_recalls = []    \n",
    "\n",
    "for e in range(num_epochs):\n",
    "    # 각 에포크 시작 시 성능 지표 초기화\n",
    "    train_acc = 0.0\n",
    "    train_f1, train_precision, train_recall = 0.0, 0.0, 0.0  # 성능 지표 초기화\n",
    "    test_acc, val_f1, val_precision, val_recall = 0.0, 0.0, 0.0, 0.0  # Validation 성능 지표 초기화\n",
    "    train_loss = 0.0  # 훈련 로스 초기화\n",
    "    val_loss = 0.0  # Validation 로스 초기화\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "        \n",
    "        out = model(token_ids, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        \n",
    "        # 성능 지표 계산\n",
    "        batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "        train_acc += batch_acc\n",
    "        train_f1 += batch_f1\n",
    "        train_precision += batch_precision\n",
    "        train_recall += batch_recall\n",
    "        train_loss += loss.item()  # 배치의 로스를 누적\n",
    "        \n",
    "        if batch_id % log_interval == 0:\n",
    "            print(f\"Epoch {e+1} Batch {batch_id+1}: Loss {loss.data.cpu().numpy()}, Accuracy {train_acc/(batch_id+1)}, F1 {train_f1/(batch_id+1)}, Precision {train_precision/(batch_id+1)}, Recall {train_recall/(batch_id+1)}\")\n",
    "    \n",
    "    # 에포크가 끝난 후 평균 성능 지표 계산\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_f1 /= len(train_dataloader)\n",
    "    train_precision /= len(train_dataloader)\n",
    "    train_recall /= len(train_dataloader)\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    train_f1s.append(train_f1)\n",
    "    train_precisions.append(train_precision)\n",
    "    train_recalls.append(train_recall)\n",
    "\n",
    "    print(f\"Epoch {e+1}: Train Accuracy {train_acc}, F1 {train_f1}, Precision {train_precision}, Recall {train_recall}\")\n",
    "    \n",
    "    # Validation 성능 계산\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_id, (token_ids, _, segment_ids, label) in enumerate(tqdm_notebook(val_dataloader)):\n",
    "            token_ids = token_ids.long().to(device)\n",
    "            segment_ids = segment_ids.long().to(device)\n",
    "            label = label.long().to(device)\n",
    "            \n",
    "            out = model(token_ids, segment_ids)\n",
    "            loss = loss_fn(out, label)\n",
    "            val_loss += loss.item()  # 배치의 validation 로스를 누적\n",
    "            batch_acc, batch_f1, batch_precision, batch_recall = calc_metrics(out, label)\n",
    "            test_acc += batch_acc\n",
    "            val_f1 += batch_f1\n",
    "            val_precision += batch_precision\n",
    "            val_recall += batch_recall\n",
    "        \n",
    "        # 에포크가 끝난 후 평균 validation 성능 지표 계산\n",
    "        test_acc /= len(val_dataloader)\n",
    "        val_f1 /= len(val_dataloader)\n",
    "        val_precision /= len(val_dataloader)\n",
    "        val_recall /= len(val_dataloader)\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(test_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "        val_precisions.append(val_precision)\n",
    "        val_recalls.append(val_recall)\n",
    "\n",
    "        print(f\"Epoch {e+1}: Validation Accuracy {test_acc}, F1 {val_f1}, Precision {val_precision}, Recall {val_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e47d053a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xticks(range(len(train_losses)))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d2820e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_003</th>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "t_003  이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "...                                                  ...\n",
       "t_495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...\n",
       "t_496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...\n",
       "t_497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...\n",
       "t_498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...\n",
       "t_499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_test = os.getenv('HOME')+'/aiffel/dktc/data/test.json'\n",
    "\n",
    "test = pd.read_json(filepath_test)\n",
    "\n",
    "test.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8d2aea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 열에 있는 'text' 데이터를 추출하여 하나의 리스트로 만듭니다.\n",
    "test_list = []\n",
    "\n",
    "for column in test.columns:\n",
    "    conversation = test[column]['text']\n",
    "    test_list.append([conversation])  # 각 대화를 리스트에 추가\n",
    "\n",
    "# 텍스트 클렌징 함수 (train에서 사용한 것과 동일하게 적용)\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣a-zA-Z\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# 전처리 적용\n",
    "test_list = [[clean_text(conversation[0])] for conversation in test_list]\n",
    "\n",
    "# 토큰화 및 데이터셋 변환 (BERTDataset 클래스 및 토크나이저는 이미 정의되어 있다고 가정)\n",
    "data_test = BERTDataset(test_list, 0, None, tok, max_len, True, False)\n",
    "\n",
    "# 테스트 데이터 로더 생성\n",
    "test_dataloader = DataLoader(data_test, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2b20448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3595121026.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777f8cd5d8994ef3801adeafee0fcf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 예측\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_id, (token_ids, valid_length, segment_ids) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        \n",
    "        # 모델 예측\n",
    "        out = model(token_ids, segment_ids)\n",
    "        pred = torch.argmax(out, dim=1).cpu().numpy()  # 가장 높은 확률의 클래스를 예측\n",
    "        predictions.extend(pred)\n",
    "\n",
    "# 숫자 레이블을 클래스명으로 변환하는 매핑\n",
    "label_mapping = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\"\n",
    "}\n",
    "\n",
    "# 예측된 숫자 레이블을 클래스명으로 변환\n",
    "predicted_classes = [label_mapping[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "29969ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name  class\n",
       "0     t_000    NaN\n",
       "1     t_001    NaN\n",
       "2     t_002    NaN\n",
       "3     t_004    NaN\n",
       "4     t_005    NaN"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath_sub = os.getenv('HOME')+'/aiffel/dktc/data/submission.csv'\n",
    "\n",
    "sub = pd.read_csv(filepath_sub)\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "33c51aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과를 DataFrame으로 변환\n",
    "submission = pd.DataFrame({\n",
    "    'idx': test.columns,  # 각 대화에 해당하는 열 이름\n",
    "    'target': predicted_classes  # 예측된 클래스명\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f8763cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일로 저장\n",
    "submission.to_csv('./LJ3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4cb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
